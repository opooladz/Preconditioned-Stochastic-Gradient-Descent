{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "\"\"\"Created in May, 2018\n",
        "Pytorch functions for preconditioned SGD\n",
        "@author: XILIN LI, lixilinx@gmail.com\n",
        "@author: OMEAD POOLADZANDI, omeadbpooladzandi@gmail.com\n",
        "\n",
        "Updated in Dec, 2020: \n",
        "Wrapped Kronecker product preconditioner for easy use: the code will select the proper Kronecker product  \n",
        "preconditioner based on the formats of input left and right preconditioners.\n",
        "Add torch.jit.script decorator by default\n",
        "\n",
        "Updated in June, 2022: \n",
        "Added UVd and XMat preconditioner using functional and optim class form.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "###############################################################################\n",
        "@torch.jit.script\n",
        "def update_precond_dense(Q, dxs, dgs, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, List[Tensor], List[Tensor], float, float) -> Tensor\n",
        "    \"\"\"\n",
        "    update dense preconditioner P = Q^T*Q\n",
        "    Q: Cholesky factor of preconditioner with positive diagonal entries \n",
        "    dxs: list of perturbations of parameters\n",
        "    dgs: list of perturbations of gradients\n",
        "    step: update step size normalized to range [0, 1] \n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    dx = torch.cat([torch.reshape(x, [-1, 1]) for x in dxs])\n",
        "    dg = torch.cat([torch.reshape(g, [-1, 1]) for g in dgs])\n",
        "    \n",
        "    a = Q.mm(dg)\n",
        "    #b = torch.triangular_solve(dx, Q, upper=True, transpose=True)[0]\n",
        "    b = torch.linalg.solve_triangular(Q.t(), dx, upper=False)\n",
        "\n",
        "    grad = torch.triu(a.mm(a.t()) - b.mm(b.t()))\n",
        "    step0 = step/(grad.abs().max() + _tiny)        \n",
        "        \n",
        "    return Q - step0*grad.mm(Q)\n",
        "\n",
        "@torch.jit.script\n",
        "def precond_grad_dense(Q, grads):\n",
        "    # type: (Tensor, List[Tensor]) -> List[Tensor]\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using dense preconditioner\n",
        "    Q: Cholesky factor of preconditioner\n",
        "    grads: list of gradients\n",
        "    \"\"\"\n",
        "    grad = [torch.reshape(g, [-1, 1]) for g in grads]\n",
        "    lens = [g.shape[0] for g in grad]\n",
        "    grad = torch.cat(grad)\n",
        "    grad = Q.t().mm(Q.mm(grad))\n",
        "    \n",
        "    pre_grads = []\n",
        "    idx = 0\n",
        "    for i in range(len(grads)):\n",
        "        pre_grads.append(torch.reshape(grad[idx : idx + lens[i]], grads[i].shape))\n",
        "        idx = idx + lens[i]\n",
        "        \n",
        "    return pre_grads\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def update_precond_kron(Ql, Qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    \"\"\"\n",
        "    Update Kronecker product preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql)\n",
        "    Either Ql or Qr can be sparse, and the code can choose the right update rule.\n",
        "    dX: perturbation of (matrix) parameter\n",
        "    dG: perturbation of (matrix) gradient\n",
        "    step: update step size\n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    m, n = Ql.shape\n",
        "    p, q = Qr.shape\n",
        "    if m==n: # left is dense\n",
        "        if p==q: #(dense, dense) format\n",
        "            return _update_precond_dense_dense(Ql, Qr, dX, dG, step, _tiny)\n",
        "        elif p==2: # (dense, normalization) format\n",
        "            return _update_precond_norm_dense(Qr, Ql, dX.t(), dG.t(), step, _tiny)[::-1]\n",
        "        elif p==1: # (dense, scaling) format\n",
        "            return _update_precond_dense_scale(Ql, Qr, dX, dG, step, _tiny)\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    elif m==2: # left is normalization\n",
        "        if p==q: # (normalization, dense) format\n",
        "            return _update_precond_norm_dense(Ql, Qr, dX, dG, step, _tiny)\n",
        "        elif p==1: # (normalization, scaling) format\n",
        "            return _update_precond_norm_scale(Ql, Qr, dX, dG, step, _tiny)\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    elif m==1: # left is scaling\n",
        "        if p==q: # (scaling, dense) format\n",
        "            return _update_precond_dense_scale(Qr, Ql, dX.t(), dG.t(), step, _tiny)[::-1]\n",
        "        elif p==2: # (scaling, normalization) format\n",
        "            return _update_precond_norm_scale(Qr, Ql, dX.t(), dG.t(), step, _tiny)[::-1]\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    else:\n",
        "        raise Exception('Unknown Kronecker product preconditioner')\n",
        " \n",
        "       \n",
        "def precond_grad_kron(Ql, Qr, Grad):\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using Kronecker product preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql)\n",
        "    Either Ql or Qr can be sparse, and the code can choose the right way to precondition the gradient\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    m, n = Ql.shape\n",
        "    p, q = Qr.shape\n",
        "    if m==n: # left is dense\n",
        "        if p==q: #(dense, dense) format\n",
        "            return _precond_grad_dense_dense(Ql, Qr, Grad)\n",
        "        elif p==2: # (dense, normalization) format\n",
        "            return _precond_grad_norm_dense(Qr, Ql, Grad.t()).t()\n",
        "        elif p==1: # (dense, scaling) format\n",
        "            return _precond_grad_dense_scale(Ql, Qr, Grad)\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    elif m==2: # left is normalization\n",
        "        if p==q: # (normalization, dense) format\n",
        "            return _precond_grad_norm_dense(Ql, Qr, Grad)\n",
        "        elif p==1: # (normalization, scaling) format\n",
        "            return _precond_grad_norm_scale(Ql, Qr, Grad)\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    elif m==1: # left is scaling\n",
        "        if p==q: # (scaling, dense) format\n",
        "            return _precond_grad_dense_scale(Qr, Ql, Grad.t()).t()\n",
        "        elif p==2: # (scaling, normalization) format\n",
        "            return _precond_grad_norm_scale(Qr, Ql, Grad.t()).t()\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    else:\n",
        "        raise Exception('Unknown Kronecker product preconditioner')\n",
        "        \n",
        "\n",
        "###############################################################################\n",
        "@torch.jit.script\n",
        "def _update_precond_dense_dense(Ql, Qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    update Kronecker product preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql)\n",
        "    Ql: (left side) Cholesky factor of preconditioner with positive diagonal entries\n",
        "    Qr: (right side) Cholesky factor of preconditioner with positive diagonal entries\n",
        "    dX: perturbation of (matrix) parameter\n",
        "    dG: perturbation of (matrix) gradient\n",
        "    step: update step size normalized to range [0, 1] \n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    max_l = torch.max(torch.diag(Ql))\n",
        "    max_r = torch.max(torch.diag(Qr))\n",
        "    \n",
        "    rho = torch.sqrt(max_l/max_r)\n",
        "    Ql /= rho\n",
        "    Qr *= rho\n",
        "    \n",
        "    #A = Ql.mm( dG.mm( Qr.t() ) )\n",
        "    #Bt = torch.triangular_solve((torch.triangular_solve(dX.t(), Qr, upper=True, transpose=True))[0].t(), \n",
        "    #                 Ql, upper=True, transpose=True)[0]\n",
        "    A = torch.linalg.multi_dot([Ql, dG, Qr.t()])\n",
        "    Bt = torch.linalg.solve_triangular(Ql.t(), torch.linalg.solve_triangular(Qr, dX, upper=True, left=False), upper=False)\n",
        "    \n",
        "    grad1 = torch.triu(A.mm(A.t()) - Bt.mm(Bt.t()))\n",
        "    grad2 = torch.triu(A.t().mm(A) - Bt.t().mm(Bt))\n",
        "    \n",
        "    step1 = step/(torch.max(torch.abs(grad1)) + _tiny)\n",
        "    step2 = step/(torch.max(torch.abs(grad2)) + _tiny)\n",
        "        \n",
        "    return Ql - step1*grad1.mm(Ql), Qr - step2*grad2.mm(Qr)\n",
        "    \n",
        "@torch.jit.script\n",
        "def _precond_grad_dense_dense(Ql, Qr, Grad):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using Kronecker product preconditioner\n",
        "    Ql: (left side) Cholesky factor of preconditioner\n",
        "    Qr: (right side) Cholesky factor of preconditioner\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    #return torch.chain_matmul(Ql.t(), Ql, Grad, Qr.t(), Qr)\n",
        "    return torch.linalg.multi_dot([Ql.t(), Ql, Grad, Qr.t(), Qr])\n",
        "    \n",
        "\n",
        "###############################################################################\n",
        "# (normalization, dense) format Kronecker product preconditioner\n",
        "@torch.jit.script\n",
        "def _update_precond_norm_dense(ql, Qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    update (normalization, dense) Kronecker product preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql), where\n",
        "    dX and dG have shape (M, N)\n",
        "    ql has shape (2, M)\n",
        "    Qr has shape (N, N)\n",
        "    ql[0] is the diagonal part of Ql\n",
        "    ql[1,0:-1] is the last column of Ql, excluding the last entry\n",
        "    dX is perturbation of (matrix) parameter\n",
        "    dG is perturbation of (matrix) gradient\n",
        "    step: update step size normalized to range [0, 1] \n",
        "    _tiny: an offset to avoid division by zero  \n",
        "    \"\"\"\n",
        "    # make sure that Ql and Qr have similar dynamic range\n",
        "    max_l = torch.max(ql[0])\n",
        "    max_r = torch.max(torch.diag(Qr))  \n",
        "    rho = torch.sqrt(max_l/max_r)\n",
        "    ql /= rho\n",
        "    Qr *= rho\n",
        "    \n",
        "    # refer to https://arxiv.org/abs/1512.04202 for details\n",
        "    A = ql[0:1].t()*dG + ql[1:].t().mm( dG[-1:] ) # Ql*dG \n",
        "    A = A.mm(Qr.t())\n",
        "    \n",
        "    Bt = dX/ql[0:1].t()\n",
        "    Bt[-1:] -= (ql[1:]/(ql[0:1]*ql[0,-1])).mm(dX)\n",
        "    #Bt = torch.triangular_solve(Bt.t(), Qr, upper=True, transpose=True)[0].t()\n",
        "    Bt = torch.linalg.solve_triangular(Qr, Bt, upper=True, left=False)\n",
        "    \n",
        "    grad1_diag = torch.sum(A*A, dim=1) - torch.sum(Bt*Bt, dim=1)\n",
        "    grad1_bias = A[:-1].mm(A[-1:].t()) - Bt[:-1].mm(Bt[-1:].t()) \n",
        "    grad1_bias = torch.cat([torch.squeeze(grad1_bias), grad1_bias.new_zeros(1)])  \n",
        "\n",
        "    step1 = step/(torch.max(torch.max(torch.abs(grad1_diag)), \n",
        "                            torch.max(torch.abs(grad1_bias))) + _tiny)\n",
        "    new_ql0 = ql[0] - step1*grad1_diag*ql[0]\n",
        "    new_ql1 = ql[1] - step1*(grad1_diag*ql[1] + ql[0,-1]*grad1_bias)\n",
        "    \n",
        "    grad2 = torch.triu(A.t().mm(A) - Bt.t().mm(Bt))\n",
        "    step2 = step/(torch.max(torch.abs(grad2)) + _tiny)\n",
        "    \n",
        "    return torch.stack((new_ql0, new_ql1)), Qr - step2*grad2.mm(Qr)\n",
        "\n",
        "@torch.jit.script\n",
        "def _precond_grad_norm_dense(ql, Qr, Grad):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using (normalization, dense) Kronecker product preconditioner \n",
        "    Suppose Grad has shape (M, N)\n",
        "    ql[0] is the diagonal part of Ql\n",
        "    ql[1, 0:-1] is the last column of Ql, excluding the last entry\n",
        "    Qr: shape (N, N), Cholesky factor of right preconditioner\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    preG = ql[0:1].t()*Grad + ql[1:].t().mm(Grad[-1:]) # Ql*Grad \n",
        "    #preG = torch.chain_matmul(preG, Qr.t(), Qr)\n",
        "    preG = torch.linalg.multi_dot([preG, Qr.t(), Qr])\n",
        "    add_last_row = ql[1:].mm(preG) # use it to modify the last row\n",
        "    preG *= ql[0:1].t()\n",
        "    preG[-1:] += add_last_row\n",
        "    \n",
        "    return preG\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# (normalization, scaling) Kronecker product preconditioner \n",
        "# the left one is a normalization preconditioner; the right one is a scaling preconditioner\n",
        "@torch.jit.script\n",
        "def _update_precond_norm_scale(ql, qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    update (normalization, scaling) preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql), where\n",
        "    dX and dG have shape (M, N)\n",
        "    ql has shape (2, M)\n",
        "    qr has shape (1, N)\n",
        "    ql[0] is the diagonal part of Ql\n",
        "    ql[1, 0:-1] is the last column of Ql, excluding the last entry\n",
        "    qr is the diagonal part of Qr\n",
        "    dX is perturbation of (matrix) parameter\n",
        "    dG is perturbation of (matrix) gradient\n",
        "    step: update step size\n",
        "    _tiny: an offset to avoid division by zero  \n",
        "    \"\"\"\n",
        "    # make sure that Ql and Qr have similar dynamic range\n",
        "    max_l = torch.max(ql[0])\n",
        "    max_r = torch.max(qr) # qr always is positive\n",
        "    rho = torch.sqrt(max_l/max_r)\n",
        "    ql /= rho\n",
        "    qr *= rho\n",
        "    \n",
        "    # refer to https://arxiv.org/abs/1512.04202 for details\n",
        "    A = ql[0:1].t()*dG + ql[1:].t().mm( dG[-1:] ) # Ql*dG \n",
        "    A *= qr # Ql*dG*Qr \n",
        "    \n",
        "    Bt = dX/ql[0:1].t()\n",
        "    Bt[-1:] -= (ql[1:]/(ql[0:1]*ql[0,-1])).mm(dX)\n",
        "    Bt /= qr # Ql^(-T)*dX*Qr^(-1) \n",
        "    \n",
        "    grad1_diag = torch.sum(A*A, dim=1) - torch.sum(Bt*Bt, dim=1)\n",
        "    grad1_bias = A[:-1].mm(A[-1:].t()) - Bt[:-1].mm(Bt[-1:].t()) \n",
        "    grad1_bias = torch.cat([torch.squeeze(grad1_bias), grad1_bias.new_zeros(1)])  \n",
        "\n",
        "    step1 = step/(torch.max(torch.max(torch.abs(grad1_diag)), \n",
        "                            torch.max(torch.abs(grad1_bias))) + _tiny)\n",
        "    new_ql0 = ql[0] - step1*grad1_diag*ql[0]\n",
        "    new_ql1 = ql[1] - step1*(grad1_diag*ql[1] + ql[0,-1]*grad1_bias)\n",
        "    \n",
        "    grad2 = torch.sum(A*A, dim=0, keepdim=True) - torch.sum(Bt*Bt, dim=0, keepdim=True)\n",
        "    step2 = step/(torch.max(torch.abs(grad2)) + _tiny)\n",
        "    \n",
        "    return torch.stack((new_ql0, new_ql1)), qr - step2*grad2*qr\n",
        "\n",
        "@torch.jit.script\n",
        "def _precond_grad_norm_scale(ql, qr, Grad):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using (normalization, scaling) Kronecker product preconditioner\n",
        "    Suppose Grad has shape (M, N)\n",
        "    ql has shape (2, M) \n",
        "    qr has shape (1, N) \n",
        "    ql[0] is the diagonal part of Ql\n",
        "    ql[1, 0:-1] is the last column of Ql, excluding the last entry\n",
        "    qr is the diagonal part of Qr\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    preG = ql[0:1].t()*Grad + ql[1:].t().mm(Grad[-1:]) # Ql*Grad \n",
        "    preG *= (qr*qr) # Ql*Grad*Qr^T*Qr\n",
        "    add_last_row = ql[1:].mm(preG) # use it to modify the last row\n",
        "    preG *= ql[0:1].t()\n",
        "    preG[-1:] += add_last_row\n",
        "    \n",
        "    return preG\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "@torch.jit.script\n",
        "def _update_precond_dense_scale(Ql, qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    update (dense, scaling) preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql), where\n",
        "    dX and dG have shape (M, N)\n",
        "    Ql has shape (M, M)\n",
        "    qr has shape (1, N)\n",
        "    qr is the diagonal part of Qr\n",
        "    dX is perturbation of (matrix) parameter\n",
        "    dG is perturbation of (matrix) gradient\n",
        "    step: update step size\n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    max_l = torch.max(torch.diag(Ql))\n",
        "    max_r = torch.max(qr)\n",
        "    \n",
        "    rho = torch.sqrt(max_l/max_r)\n",
        "    Ql /= rho\n",
        "    qr *= rho\n",
        "    \n",
        "    A = Ql.mm( dG*qr )\n",
        "    #Bt = torch.triangular_solve(dX/qr, Ql, upper=True, transpose=True)[0]\n",
        "    Bt = torch.linalg.solve_triangular(Ql.t(), dX/qr, upper=False)\n",
        "    \n",
        "    grad1 = torch.triu(A.mm(A.t()) - Bt.mm(Bt.t()))\n",
        "    grad2 = torch.sum(A*A, dim=0, keepdim=True) - torch.sum(Bt*Bt, dim=0, keepdim=True)\n",
        "    \n",
        "    step1 = step/(torch.max(torch.abs(grad1)) + _tiny)\n",
        "    step2 = step/(torch.max(torch.abs(grad2)) + _tiny)\n",
        "        \n",
        "    return Ql - step1*grad1.mm(Ql), qr - step2*grad2*qr\n",
        "    \n",
        "@torch.jit.script\n",
        "def _precond_grad_dense_scale(Ql, qr, Grad):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using (dense, scaling) Kronecker product preconditioner\n",
        "    Suppose Grad has shape (M, N)\n",
        "    Ql: shape (M, M), (left side) Cholesky factor of preconditioner\n",
        "    qr: shape (1, N), defines a diagonal matrix for output feature scaling\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    #return torch.chain_matmul(Ql.t(), Ql, Grad*(qr*qr))\n",
        "    return torch.linalg.multi_dot([Ql.t(), Ql, Grad*(qr*qr)])\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################   \n",
        "@torch.jit.script                     \n",
        "def update_precond_splu(L12, l3, U12, u3, dxs, dgs, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor,Tensor,Tensor,Tensor, List[Tensor],List[Tensor], float,float) -> Tuple[Tensor,Tensor,Tensor,Tensor]\n",
        "    \"\"\"\n",
        "    update sparse LU preconditioner P = Q^T*Q, where \n",
        "    Q = L*U,\n",
        "    L12 = [L1; L2]\n",
        "    U12 = [U1, U2]\n",
        "    L = [L1, 0; L2, diag(l3)]\n",
        "    U = [U1, U2; 0, diag(u3)]\n",
        "    l3 and u3 are column vectors\n",
        "    dxs: a list of random perturbation on parameters\n",
        "    dgs: a list of resultant perturbation on gradients\n",
        "    step: update step size normalized to range [0, 1] \n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    # make sure that L and U have similar dynamic range\n",
        "    max_l = torch.max(torch.max(torch.diag(L12)), torch.max(l3))\n",
        "    max_u = torch.max(torch.max(torch.diag(U12)), torch.max(u3))\n",
        "    rho = torch.sqrt(max_l/max_u)\n",
        "    L12 /= rho\n",
        "    l3 /= rho\n",
        "    U12 *= rho\n",
        "    u3 *= rho\n",
        "    \n",
        "    # extract the blocks\n",
        "    r = U12.shape[0]\n",
        "    L1 = L12[:r]\n",
        "    L2 = L12[r:]\n",
        "    U1 = U12[:, :r]\n",
        "    U2 = U12[:, r:]\n",
        "    \n",
        "    dx = torch.cat([torch.reshape(x, [-1, 1]) for x in dxs]) # a tall column vector\n",
        "    dg = torch.cat([torch.reshape(g, [-1, 1]) for g in dgs]) # a tall column vector\n",
        "    \n",
        "    # U*dg\n",
        "    Ug1 = U1.mm(dg[:r]) + U2.mm(dg[r:])\n",
        "    Ug2 = u3*dg[r:]\n",
        "    # Q*dg\n",
        "    Qg1 = L1.mm(Ug1)\n",
        "    Qg2 = L2.mm(Ug1) + l3*Ug2\n",
        "    # inv(U^T)*dx\n",
        "    #iUtx1 = torch.triangular_solve(dx[:r], U1, upper=True, transpose=True)[0]\n",
        "    iUtx1 = torch.linalg.solve_triangular(U1.t(), dx[:r], upper=False)\n",
        "    iUtx2 = (dx[r:] - U2.t().mm(iUtx1))/u3\n",
        "    # inv(Q^T)*dx\n",
        "    iQtx2 = iUtx2/l3\n",
        "    #iQtx1 = torch.triangular_solve(iUtx1 - L2.t().mm(iQtx2), L1, upper=False, transpose=True)[0]\n",
        "    iQtx1 = torch.linalg.solve_triangular(L1.t(), iUtx1 - L2.t().mm(iQtx2), upper=True)\n",
        "    # L^T*Q*dg\n",
        "    LtQg1 = L1.t().mm(Qg1) + L2.t().mm(Qg2)\n",
        "    LtQg2 = l3*Qg2\n",
        "    # P*dg\n",
        "    Pg1 = U1.t().mm(LtQg1)\n",
        "    Pg2 = U2.t().mm(LtQg1) + u3*LtQg2\n",
        "    # inv(L)*inv(Q^T)*dx\n",
        "    #iLiQtx1 = torch.triangular_solve(iQtx1, L1, upper=False)[0]\n",
        "    iLiQtx1 = torch.linalg.solve_triangular(L1, iQtx1, upper=False)\n",
        "    iLiQtx2 = (iQtx2 - L2.mm(iLiQtx1))/l3\n",
        "    # inv(P)*dx\n",
        "    iPx2 = iLiQtx2/u3\n",
        "    #iPx1 = torch.triangular_solve(iLiQtx1 - U2.mm(iPx2), U1, upper=True)[0]\n",
        "    iPx1 = torch.linalg.solve_triangular(U1, iLiQtx1 - U2.mm(iPx2), upper=True)\n",
        "    \n",
        "    # update L\n",
        "    grad1 = Qg1.mm(Qg1.t()) - iQtx1.mm(iQtx1.t())\n",
        "    grad1 = torch.tril(grad1)\n",
        "    grad2 = Qg2.mm(Qg1.t()) - iQtx2.mm(iQtx1.t())\n",
        "    grad3 = Qg2*Qg2 - iQtx2*iQtx2\n",
        "    max_abs_grad = torch.max(torch.abs(grad1))\n",
        "    max_abs_grad = torch.max(max_abs_grad, torch.max(torch.abs(grad2)))\n",
        "    max_abs_grad = torch.max(max_abs_grad, torch.max(torch.abs(grad3)))\n",
        "    step0 = step/(max_abs_grad + _tiny)\n",
        "    newL1 = L1 - step0*grad1.mm(L1)\n",
        "    newL2 = L2 - step0*grad2.mm(L1) - step0*grad3*L2\n",
        "    newl3 = l3 - step0*grad3*l3\n",
        "\n",
        "    # update U\n",
        "    grad1 = Pg1.mm(dg[:r].t()) - dx[:r].mm(iPx1.t())\n",
        "    grad1 = torch.triu(grad1)\n",
        "    grad2 = Pg1.mm(dg[r:].t()) - dx[:r].mm(iPx2.t())\n",
        "    grad3 = Pg2*dg[r:] - dx[r:]*iPx2\n",
        "    max_abs_grad = torch.max(torch.abs(grad1))\n",
        "    max_abs_grad = torch.max(max_abs_grad, torch.max(torch.abs(grad2)))\n",
        "    max_abs_grad = torch.max(max_abs_grad, torch.max(torch.abs(grad3)))\n",
        "    step0 = step/(max_abs_grad + _tiny)\n",
        "    newU1 = U1 - U1.mm(step0*grad1)\n",
        "    newU2 = U2 - U1.mm(step0*grad2) - step0*grad3.t()*U2\n",
        "    newu3 = u3 - step0*grad3*u3\n",
        "\n",
        "    return torch.cat([newL1, newL2], dim=0), newl3, torch.cat([newU1, newU2], dim=1), newu3\n",
        "\n",
        "@torch.jit.script\n",
        "def precond_grad_splu(L12, l3, U12, u3, grads):\n",
        "    # type: (Tensor,Tensor,Tensor,Tensor, List[Tensor]) -> List[Tensor]\n",
        "    \"\"\"\n",
        "    return preconditioned gradient with sparse LU preconditioner\n",
        "    where P = Q^T*Q, \n",
        "    Q = L*U,\n",
        "    L12 = [L1; L2]\n",
        "    U12 = [U1, U2]\n",
        "    L = [L1, 0; L2, diag(l3)]\n",
        "    U = [U1, U2; 0, diag(u3)]\n",
        "    l3 and u3 are column vectors\n",
        "    grads: a list of gradients to be preconditioned\n",
        "    \"\"\"\n",
        "    grad = [torch.reshape(g, [-1, 1]) for g in grads] # a list of column vector\n",
        "    lens = [g.shape[0] for g in grad] # length of each column vector\n",
        "    grad = torch.cat(grad)  # a tall column vector\n",
        "    \n",
        "    r = U12.shape[0]\n",
        "    L1 = L12[:r]\n",
        "    L2 = L12[r:]\n",
        "    U1 = U12[:, :r]\n",
        "    U2 = U12[:, r:]    \n",
        "    \n",
        "    # U*g\n",
        "    Ug1 = U1.mm(grad[:r]) + U2.mm(grad[r:])\n",
        "    Ug2 = u3*grad[r:]\n",
        "    # Q*g\n",
        "    Qg1 = L1.mm(Ug1)\n",
        "    Qg2 = L2.mm(Ug1) + l3*Ug2\n",
        "    # L^T*Q*g\n",
        "    LtQg1 = L1.t().mm(Qg1) + L2.t().mm(Qg2)\n",
        "    LtQg2 = l3*Qg2\n",
        "    # P*g\n",
        "    pre_grad = torch.cat([U1.t().mm(LtQg1),\n",
        "                          U2.t().mm(LtQg1) + u3*LtQg2])\n",
        "    \n",
        "    pre_grads = [] # restore pre_grad to its original shapes\n",
        "    idx = 0\n",
        "    for i in range(len(grads)):\n",
        "        pre_grads.append(torch.reshape(pre_grad[idx : idx + lens[i]], grads[i].shape))\n",
        "        idx = idx + lens[i]\n",
        "    \n",
        "    return pre_grads\n",
        "\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "#\n",
        "# The low-rank approximation (UVd) preconditioner is defined by\n",
        "#\n",
        "#   Q = (I + U*V')*diag(d)\n",
        "#\n",
        "# which, after reparameterization, is equivalent to form\n",
        "#\n",
        "#   diag(d) + U*V'\n",
        "# \n",
        "# It relates to the LM-BFGS and conjugate gradient methods. \n",
        "#\n",
        "# The JIT decorator can be enabled if helps. \n",
        "# \n",
        "\n",
        "#@torch.jit.script\n",
        "def IpUVtmatvec(U, V, x):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    Returns (I + U*V')*x. All variables are either matrices or column vectors. \n",
        "    \"\"\"\n",
        "    return x + U.mm(V.t().mm(x))\n",
        "\n",
        "# def IpUVtsolve(U, V, x):\n",
        "#     \"\"\"\n",
        "#     Returns inv(I + U*V')*x. All variables are either matrices or column vectors.\n",
        "#     \"\"\"\n",
        "#     VtU = V.t().mm(U)\n",
        "#     I = torch.eye(VtU.size(dim=0), dtype=VtU.dtype, device=VtU.device)\n",
        "#     return x - U.mm(torch.linalg.solve(I + VtU, V.t().mm(x))) # torch.solve is slow\n",
        "\n",
        "# def norm_UVt(U, V):\n",
        "#     \"\"\"\n",
        "#     Returns ||U*V'||_fro = sqrt(tr(U'*U*V'*V)) = sqrt(sum((U'*U)*(V'*V))) \n",
        "#     \"\"\"\n",
        "#     return torch.sqrt(torch.abs(torch.sum( (U.t().mm(U))*(V.t().mm(V)) )))\n",
        "\n",
        "#@torch.jit.script\n",
        "def update_precond_UVd_math_(U, V, d, v, h, step, tiny):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, float, float) -> None\n",
        "    \"\"\"\n",
        "    Update preconditioner Q = (I + U*V')*diag(d) with (vector, Hessian-vector product) = (v, h).\n",
        "    State variables U, V and d are updated inplace. \n",
        "                               \n",
        "    U, V, d, v, and h are either matrices or column vectors.  \n",
        "    \"\"\"\n",
        "    # balance the numerical dynamic ranges of U and V; optional \n",
        "    if torch.rand([]) < 0.01:\n",
        "        normU = torch.linalg.vector_norm(U)\n",
        "        normV = torch.linalg.vector_norm(V)\n",
        "        rho = torch.sqrt(normU/normV)\n",
        "        U.div_(rho)\n",
        "        V.mul_(rho)\n",
        "\n",
        "    Qh = IpUVtmatvec(U, V, d*h)\n",
        "    Ph = d*IpUVtmatvec(V, U, Qh)\n",
        "    \n",
        "    # invQtv = IpUVtsolve(V, U, v/d)\n",
        "    # invPv = IpUVtsolve(U, V, invQtv)/d\n",
        "    VtU = V.t().mm(U)\n",
        "    I = torch.eye(VtU.size(dim=0), dtype=VtU.dtype, device=VtU.device)\n",
        "    IpVtU = I + VtU\n",
        "    invQtv = v/d\n",
        "    # torch's linalg.solve is slow for small matrix\n",
        "    invQtv = invQtv - V.mm(torch.linalg.solve(IpVtU.t(), U.t().mm(invQtv)))  \n",
        "    invPv  = invQtv - U.mm(torch.linalg.solve(IpVtU,     V.t().mm(invQtv)))\n",
        "    invPv = invPv/d\n",
        "\n",
        "    nablaD = Ph*h - v*invPv\n",
        "    mu = step/(torch.max(torch.abs(nablaD)) + tiny)\n",
        "    #d = d - mu*d*nablaD\n",
        "    d.sub_(mu*d*nablaD)\n",
        "    \n",
        "    # update either U or V, not both at the same time\n",
        "    a, b = Qh, invQtv\n",
        "    if torch.rand([]) < 0.5:\n",
        "        # nablaU = Qh.mm(Qh.t().mm(V)) - invQtv.mm(invQtv.t().mm(V))\n",
        "        # mu = step/(norm_UVt(nablaU, V) + _tiny)\n",
        "        # U = U - mu*(nablaU + nablaU.mm(V.t().mm(U)))\n",
        "        atV = a.t().mm(V)\n",
        "        atVVt = atV.mm(V.t())\n",
        "        btV = b.t().mm(V)\n",
        "        btVVt = btV.mm(V.t())\n",
        "        norm = torch.sqrt(torch.abs( (a.t().mm(a))*(atVVt.mm(atVVt.t())) # abs to avoid sqrt(-0.0) \n",
        "                                    +(b.t().mm(b))*(btVVt.mm(btVVt.t())) \n",
        "                                  -2*(a.t().mm(b))*(atVVt.mm(btVVt.t())) ))\n",
        "        mu = step/(norm + tiny)\n",
        "        # U = U - mu*( a.mm(atV.mm(IpVtU)) \n",
        "        #             -b.mm(btV.mm(IpVtU)) )\n",
        "        U.sub_(mu*( a.mm(atV.mm(IpVtU)) \n",
        "                   -b.mm(btV.mm(IpVtU)) ))\n",
        "    else:\n",
        "        # nablaV = Qh.mm(Qh.t().mm(U)) - invQtv.mm(invQtv.t().mm(U))\n",
        "        # mu = step/(norm_UVt(U, nablaV) + _tiny)\n",
        "        # V = V - mu*(nablaV + V.mm(U.t().mm(nablaV)))\n",
        "        atU = a.t().mm(U)\n",
        "        btU = b.t().mm(U)\n",
        "        UUta = U.mm(atU.t())\n",
        "        UUtb = U.mm(btU.t())\n",
        "        norm = torch.sqrt(torch.abs( (UUta.t().mm(UUta))*(a.t().mm(a)) # abs to avoid sqrt(-0.0)\n",
        "                                    +(UUtb.t().mm(UUtb))*(b.t().mm(b))\n",
        "                                  -2*(UUta.t().mm(UUtb))*(a.t().mm(b)) ))\n",
        "        mu = step/(norm + tiny)\n",
        "        # V = V - mu*( (a + V.mm(atU.t())).mm(atU) \n",
        "        #             -(b + V.mm(btU.t())).mm(btU) )\n",
        "        V.sub_(mu*( (a + V.mm(atU.t())).mm(atU) \n",
        "                   -(b + V.mm(btU.t())).mm(btU) ))\n",
        "\n",
        "    # return [U, V, d]\n",
        "\n",
        "#@torch.jit.script\n",
        "def precond_grad_UVd_math(U, V, d, g):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    Preconditioning gradient g with Q = (I + U*V')*diag(d).\n",
        "                                         \n",
        "    All variables here are either matrices or column vectors. \n",
        "    \"\"\"\n",
        "    g = IpUVtmatvec(U, V, d*g)\n",
        "    g = d*IpUVtmatvec(V, U, g)\n",
        "    return g\n",
        "\n",
        "## Functional form of UVd Precond\n",
        "\n",
        "def update_precond_UVd(UVd, vs, hs, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, List[Tensor], List[Tensor], float, float) -> Tensor\n",
        "    \"\"\"\n",
        "    update UVd preconditioner Q = (I + U*V')*diag(d) with\n",
        "    vs: a list of vectors;\n",
        "    hs: a list of associated Hessian-vector products;\n",
        "    step: step size, setting to larger values, say 0.1, if updating is sparse;\n",
        "    _tiny: an offset to avoid divided by zero. \n",
        "    \n",
        "    It is a wrapped version of function update_precond_UVd_math for easy use. \n",
        "    Also, U, V, and d are transposed (row-major order as Python convention), and \n",
        "    packaged into one tensor. \n",
        "    \"\"\" \n",
        "    sizes = [len(UVd)//2]*2 + [1]\n",
        "    U, V, d = torch.split(UVd.t(), sizes, dim=1)\n",
        "\n",
        "    v = torch.cat([torch.flatten(v) for v in vs])\n",
        "    h = torch.cat([torch.flatten(h) for h in hs])\n",
        "    update_precond_UVd_math_(U, V, d, v[:,None], h[:,None], step=step, tiny=_tiny)\n",
        "    return torch.cat([U, V, d], 1).t()\n",
        "\n",
        "\n",
        "#@torch.jit.script\n",
        "def precond_grad_UVd(UVd, grads):\n",
        "    # type: (Tensor, List[Tensor]) -> List[Tensor]\n",
        "    \"\"\"\n",
        "    return preconditioned gradient with UVd preconditioner Q = (I + U*V')*diag(d),\n",
        "    and a list of gradients, grads.\n",
        "    \n",
        "    It is a wrapped version of function precond_grad_UVd_math for easy use.\n",
        "    Also, U, V, and d are transposed (row-major order as Python convention), and \n",
        "    packaged into one tensor.\n",
        "    \"\"\"\n",
        "    sizes = [len(UVd)//2]*2 + [1]\n",
        "    U, V, d = torch.split(UVd.t(), sizes, dim=1)\n",
        "\n",
        "    # record the sizes and shapes, and then flatten gradients\n",
        "    sizes = [torch.numel(g) for g in grads]\n",
        "    shapes = [g.shape for g in grads]\n",
        "    cumsizes = torch.cumsum(torch.tensor(sizes), 0)\n",
        "    \n",
        "    grad = torch.cat([torch.flatten(g) for g in grads])\n",
        "\n",
        "    # precondition gradients\n",
        "    pre_grad = precond_grad_UVd_math(U, V, d, grad[:,None])\n",
        "\n",
        "    # restore gradients to their original shapes\n",
        "    return [torch.reshape(pre_grad[j-i:j], s) for (i, j, s) in zip(sizes, cumsizes, shapes)]\n",
        "\n",
        "class UVd(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the low-rank approximation (UVd) preconditioner, Q = (I + U*V')*diag(d), as a class.\n",
        "\n",
        "    Args for initialization:\n",
        "        params_with_grad: a list of parameters or variables requiring gradients;\n",
        "        rank_of_approximation: rank of approximation, i.e., rank of U or V;\n",
        "        preconditioner_init_scale: initial scale of Q, or roughly, Q = preconditioner_init_scale*eye();\n",
        "        lr_params: normalized learning rate for parameters in range [0, 1];\n",
        "        lr_preconditioner: normalized learning rate for preconditioner in range [0, 1];\n",
        "        momentum: momentum factor in range [0,1);\n",
        "        grad_clip_max_norm: maximum allowable gradient norm after clipping, None for no clipping;\n",
        "        preconditioner_update_probability: probability on updating Q, 1 for updating at every step, and 0 for never;\n",
        "        exact_hessian_vector_product: True for exact Hessian-vector product via 2nd derivative,\n",
        "                                    and False for approximate one via finite-difference formulae.\n",
        "\n",
        "    Notes:\n",
        "        Note 1: The Hessian-vector product can be approximated using the finite-difference formulae by setting \n",
        "        exact_hessian_vector_product = False when the 2nd derivatives is not available.\n",
        "        In this case, make sure that the closure produces the same outputs given the same inputs, \n",
        "        except for numerical errors due to non-deterministic behaviors.\n",
        "        Random numbers, if any, used inside the closure should be generated starting from the same state, where the rng state can be\n",
        "        read and set by, e.g., `torch.cuda.get_rng_state' and `torch.cuda.set_rng_state', respectively.\n",
        "        \n",
        "        Note 2: Momentum here is the moving average of gradient so that its setting is decoupled from the learning rate.\n",
        "        This is necessary as the learning rate in PSGD is normalized. \n",
        "\n",
        "        Note 3: `torch.linalg.solve' is called twice in function `update_precond_UVd_math_'.\n",
        "        Certain solver could be orders of magnitude faster than others, especially for small matrices (see the pdf file).\n",
        "        Considering replace it with faster ones if the default solver is too slow.\n",
        "\n",
        "        Note 4: Currently, no support of sparse and mixed-precision gradients. \n",
        "        Half precision is supported except that torch.linalg.solve (v1.12) requires casting float16 to float32.    \n",
        "        \n",
        "        Note 5: lr_params, lr_preconditioner, momentum, grad_clip_max_norm, preconditioner_update_probability, and \n",
        "        exact_hessian_vector_product (bool) all can be reset on the fly. \n",
        "    \"\"\"\n",
        "    def __init__(self,  params_with_grad, rank_of_approximation:int=10, preconditioner_init_scale=1.0,\n",
        "                        lr_params=0.01, lr_preconditioner=0.01, momentum=0.0,\n",
        "                        grad_clip_max_norm=None, preconditioner_update_probability=1.0,\n",
        "                        exact_hessian_vector_product:bool=True, directional = 1):\n",
        "        # mutable members\n",
        "        self.lr_params = lr_params\n",
        "        self.lr_preconditioner = lr_preconditioner\n",
        "        self.momentum = momentum if (0<momentum<1) else 0.0\n",
        "        self.grad_clip_max_norm = grad_clip_max_norm\n",
        "        self.preconditioner_update_probability = preconditioner_update_probability\n",
        "        self.exact_hessian_vector_product = exact_hessian_vector_product\n",
        "        self.directional = directional\n",
        "        # protected members\n",
        "        params_with_grad = [params_with_grad,] if isinstance(params_with_grad, torch.Tensor) else params_with_grad\n",
        "        self._params_with_grad = [param for param in params_with_grad if param.requires_grad] # double check requires_grad flag\n",
        "        dtype, device = self._params_with_grad[0].dtype, self._params_with_grad[0].device\n",
        "        self._tiny = torch.finfo(dtype).tiny\n",
        "        self._delta_param_scale = torch.finfo(dtype).eps**0.5\n",
        "        self._param_sizes = [torch.numel(param) for param in self._params_with_grad]\n",
        "        self._param_cumsizes = torch.cumsum(torch.tensor(self._param_sizes), 0)\n",
        "        num_params = self._param_cumsizes[-1]\n",
        "        self._U = torch.randn(num_params, rank_of_approximation, dtype=dtype, device=device) / (num_params*rank_of_approximation)**0.5\n",
        "        self._V = torch.randn(num_params, rank_of_approximation, dtype=dtype, device=device) / (num_params*rank_of_approximation)**0.5\n",
        "        self._d = torch.ones( num_params, 1, dtype=dtype, device=device) * preconditioner_init_scale\n",
        "        self._m = None # momentum buffer \n",
        "        defaults = dict(lr=lr_params)\n",
        "        super(UVd, self).__init__(self._params_with_grad, defaults)            \n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure):\n",
        "        \"\"\"\n",
        "        Performs a single step of PSGD with low-rank approximation (UVd) preconditioner, i.e., \n",
        "        updating the trainable parameters once, and returning what closure returns.\n",
        "\n",
        "        Args:\n",
        "            closure (callable): a closure that evaluates the function of self._params_with_grad,\n",
        "                                and returns the loss, or an iterable with the first one being loss.\n",
        "                                Random numbers, if any, used inside the closure should be generated starting \n",
        "                                from the same rng state if self.exact_hessian_vector_product = False; otherwise doesn't matter. \n",
        "        \"\"\"\n",
        "        if torch.rand([]) < self.preconditioner_update_probability:\n",
        "            # evaluates gradients, Hessian-vector product, and updates the preconditioner\n",
        "            if self.exact_hessian_vector_product:\n",
        "                # exact Hessian-vector product\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad, create_graph=True)\n",
        "                    vs = [torch.randn_like(param) for param in self._params_with_grad]\n",
        "                    Hvs = torch.autograd.grad(grads, self._params_with_grad, vs)\n",
        "            else:\n",
        "                # approximate Hessian-vector product via finite-difference formulae. Use it with cautions.\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "                if self.directional == 1:\n",
        "                  vs = [self._delta_param_scale * torch.randn_like(param) for param in self._params_with_grad]\n",
        "                else:\n",
        "                  vs = [self._delta_param_scale * (1 + torch.abs(param)) * torch.randn_like(param) for param in self._params_with_grad]\n",
        "                [param.add_(v) for (param, v) in zip(self._params_with_grad, vs)]\n",
        "                with torch.enable_grad():\n",
        "                    perturbed_returns = closure()\n",
        "                    perturbed_loss = perturbed_returns if isinstance(perturbed_returns, torch.Tensor) else perturbed_returns[0]\n",
        "                    perturbed_grads = torch.autograd.grad(perturbed_loss, self._params_with_grad)\n",
        "                Hvs = [perturbed_g - g for (perturbed_g, g) in zip(perturbed_grads, grads)]\n",
        "            # update preconditioner\n",
        "            v = torch.cat([torch.flatten(v) for v in vs])\n",
        "            h = torch.cat([torch.flatten(h) for h in Hvs])\n",
        "            if self.exact_hessian_vector_product:\n",
        "                update_precond_UVd_math_(self._U, self._V, self._d,\n",
        "                                         v[:,None], h[:,None], step=self.lr_preconditioner, tiny=self._tiny)\n",
        "            else: # compensate the levels of v and h; helpful to reduce numerical errors in half-precision training\n",
        "                update_precond_UVd_math_(self._U, self._V, self._d,\n",
        "                                         v[:,None]/self._delta_param_scale, h[:,None]/self._delta_param_scale, step=self.lr_preconditioner, tiny=self._tiny)\n",
        "        else:\n",
        "            # only evaluates the gradients\n",
        "            with torch.enable_grad():\n",
        "                closure_returns = closure()\n",
        "                loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "            vs = None # no vs and Hvs\n",
        "\n",
        "        # preconditioned gradients; momentum is optional\n",
        "        grad = torch.cat([torch.flatten(g) for g in grads])\n",
        "        if self.momentum > 0:\n",
        "            if self._m is None:\n",
        "                self._m = (1 - self.momentum)*grad\n",
        "            else:\n",
        "                self._m.mul_(self.momentum).add_((1 - self.momentum)*grad)\n",
        "            pre_grad = precond_grad_UVd_math(self._U, self._V, self._d, self._m[:, None])\n",
        "        else:\n",
        "            self._m = None # clean the buffer when momentum is set to zero \n",
        "            pre_grad = precond_grad_UVd_math(self._U, self._V, self._d, grad[:, None])\n",
        "            \n",
        "        # gradient clipping is optional\n",
        "        if self.grad_clip_max_norm is None:\n",
        "            lr = self.lr_params\n",
        "        else:\n",
        "            grad_norm = torch.linalg.vector_norm(pre_grad) + self._tiny\n",
        "            lr = self.lr_params * min(self.grad_clip_max_norm/grad_norm, 1.0)\n",
        "            \n",
        "        # update the parameters\n",
        "        if self.exact_hessian_vector_product or (vs is None):\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param))\n",
        "             for (param, i, j) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes)]\n",
        "        else: # in this case, do not forget to remove the perturbation on parameters\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param) + v)\n",
        "             for (param, i, j, v) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes, vs)]\n",
        "        # return whatever closure returns\n",
        "        return closure_returns\n",
        "\n",
        "################## end of UVd preconditioner #################################\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# An Xmat (X-matrix) preconditioner is defined by\n",
        "#\n",
        "#   Q = diag(a) + adiag(b)\n",
        "#\n",
        "# where adiag means anti-diagonal.\n",
        "# It's slightly more complicated than a diagonal preconditioner, but performs better.\n",
        "#\n",
        "\n",
        "#@torch.jit.script\n",
        "def update_precond_Xmat_math_(a, b, v, h, step, tiny):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> None\n",
        "    \"\"\"\n",
        "    Update preconditioner Q = diag(a) + adiag(b) with (vector, Hessian-vector product) = (v, h).\n",
        "    State variables a and b are updated inplace.\n",
        "    \"\"\"\n",
        "    Qh = a*h + b*torch.flip(h, [0])\n",
        "    aflip, bflip = torch.flip(a, [0]), torch.flip(b, [0])\n",
        "    invQtv = (aflip*v - bflip*torch.flip(v, [0]))/(a*aflip - b*bflip)\n",
        "    nablaA = Qh*Qh - invQtv*invQtv\n",
        "    nablaB = Qh*torch.flip(Qh, [0]) - invQtv*torch.flip(invQtv, [0])\n",
        "    q, r = divmod(len(nablaB), 2)\n",
        "    if r == 1:\n",
        "        nablaB[q] = 0\n",
        "\n",
        "    mu = step/(torch.maximum(torch.max(torch.abs(nablaA)), torch.max(torch.abs(nablaB))) + tiny)\n",
        "    a.sub_(mu*(nablaA*a + nablaB*bflip))\n",
        "    b.sub_(mu*(nablaA*b + nablaB*aflip))\n",
        "\n",
        "#@torch.jit.script\n",
        "def precond_grad_Xmat_math(a, b, g):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    Preconditioning gradient g with Q = diag(a) + adiag(b).\n",
        "    \"\"\"\n",
        "    ab = a * b\n",
        "    return (a*a + torch.flip(b*b, [0]))*g + (ab + torch.flip(ab, [0]))*torch.flip(g, [0])\n",
        "\n",
        "\n",
        "def update_precond_XMat(a,b, vs, hs, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, List[Tensor], List[Tensor], float, float) -> Tensor\n",
        "    \"\"\"\n",
        "    update XMat preconditioner Q = diag(a) + adiag(b) with\n",
        "    vs: a list of vectors;\n",
        "    hs: a list of associated Hessian-vector products;\n",
        "    step: step size, setting to larger values, say 0.1, if updating is sparse;\n",
        "    _tiny: an offset to avoid divided by zero. \n",
        "    \n",
        "    It is a wrapped version of function update_precond_XMat_math for easy use. \n",
        "    \"\"\" \n",
        "\n",
        "    v = torch.cat([torch.flatten(v) for v in vs])\n",
        "    h = torch.cat([torch.flatten(h) for h in hs])\n",
        "    update_precond_Xmat_math_(a, b, v, h, step=step, tiny=_tiny)       \n",
        "    return a, b\n",
        "\n",
        "## Functional form of XMat Precond\n",
        "#@torch.jit.script\n",
        "def precond_grad_XMat(a,b, grads):\n",
        "\n",
        "    \"\"\"\n",
        "    return preconditioned gradient with UVd preconditioner Q = (I + U*V')*diag(d),\n",
        "    and a list of gradients, grads.\n",
        "    \n",
        "    It is a wrapped version of function precond_grad_UVd_math for easy use.\n",
        "    Also, U, V, and d are transposed (row-major order as Python convention), and \n",
        "    packaged into one tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # record the sizes and shapes, and then flatten gradients\n",
        "    # sizes = [torch.numel(g) for g in grads]\n",
        "    # shapes = [g.shape for g in grads]\n",
        "    # cumsizes = torch.cumsum(torch.tensor(sizes), 0)\n",
        "    \n",
        "    grad = torch.cat([torch.flatten(g) for g in grads])\n",
        "\n",
        "    # precondition gradients\n",
        "    pre_grad =  precond_grad_Xmat_math(a, b, grad)\n",
        "    # restore gradients to their original shapes\n",
        "    # return [torch.reshape(pre_grad[j-i:j], s) for (i, j, s) in zip(sizes, cumsizes, shapes)]\n",
        "    return torch.reshape(pre_grad,grads.shape)\n",
        "\n",
        "\n",
        "class XMat(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the Xmat preconditioner, Q = diag(a) + adiag(b), as a class.\n",
        "    Args for initialization:\n",
        "        params_with_grad: a list of parameters or variables requiring gradients;\n",
        "        preconditioner_init_scale: initial scale of Q, i.e., Q = preconditioner_init_scale*eye();\n",
        "        lr_params: normalized learning rate for parameters in range [0, 1];\n",
        "        lr_preconditioner: normalized learning rate for preconditioner in range [0, 1];\n",
        "        momentum: momentum factor in range [0,1);\n",
        "        grad_clip_max_norm: maximum allowable gradient norm after clipping, None for no clipping;\n",
        "        preconditioner_update_probability: probability on updating Q, 1 for updating at every step, and 0 for never, i.e., SGD;\n",
        "        exact_hessian_vector_product: True for exact Hessian-vector product via 2nd derivative,\n",
        "                                    and False for approximate one via finite-difference formulae.\n",
        "    Notes:\n",
        "        Note 1: The Hessian-vector product can be approximated using the finite-difference formulae by setting\n",
        "        exact_hessian_vector_product = False when the 2nd derivatives is not available.\n",
        "        In this case, make sure that the closure produces the same outputs given the same inputs,\n",
        "        except for numerical errors due to non-deterministic behaviors.\n",
        "        Random numbers, if any, used inside the closure should be generated starting from the same state, where the rng state can be\n",
        "        read and set by, e.g., `torch.cuda.get_rng_state' and `torch.cuda.set_rng_state', respectively.\n",
        "        \n",
        "        Note 2: Momentum here is the moving average of gradient so that its setting is decoupled from the learning rate.\n",
        "        This is necessary as the learning rate in PSGD is normalized.\n",
        "\n",
        "        Note 3: Currently, no support of sparse and mixed-precision gradients.\n",
        "\n",
        "        Note 4: lr_params, lr_preconditioner, momentum, grad_clip_max_norm, preconditioner_update_probability, and\n",
        "        exact_hessian_vector_product (bool) all can be reset on the fly.\n",
        "    \"\"\"\n",
        "    def __init__(self, params_with_grad, preconditioner_init_scale=1.0,\n",
        "                 lr_params=0.01, lr_preconditioner=0.01, momentum=0.0, \n",
        "                 grad_clip_max_norm=None, preconditioner_update_probability=1.0,\n",
        "                 exact_hessian_vector_product: bool = True):\n",
        "        # mutable members\n",
        "        self.lr_params = lr_params\n",
        "        self.lr_preconditioner = lr_preconditioner\n",
        "        self.momentum = momentum if (0<momentum<1) else 0.0\n",
        "        self.grad_clip_max_norm = grad_clip_max_norm\n",
        "        self.preconditioner_update_probability = preconditioner_update_probability\n",
        "        self.exact_hessian_vector_product = exact_hessian_vector_product\n",
        "        # protected members\n",
        "        params_with_grad = [params_with_grad, ] if isinstance(params_with_grad, torch.Tensor) else params_with_grad\n",
        "        self._params_with_grad = [param for param in params_with_grad if param.requires_grad]  # double check requires_grad flag\n",
        "        dtype, device = self._params_with_grad[0].dtype, self._params_with_grad[0].device\n",
        "        self._tiny = torch.finfo(dtype).tiny\n",
        "        self._delta_param_scale = torch.finfo(dtype).eps ** 0.5\n",
        "        self._param_sizes = [torch.numel(param) for param in self._params_with_grad]\n",
        "        self._param_cumsizes = torch.cumsum(torch.tensor(self._param_sizes), 0)\n",
        "        num_params = self._param_cumsizes[-1]\n",
        "        self._a = torch.ones(num_params, dtype=dtype, device=device)*preconditioner_init_scale\n",
        "        self._b = torch.zeros(num_params, dtype=dtype, device=device)\n",
        "        self._m = None # buffer for momentum \n",
        "        defaults = dict(lr=lr_params)\n",
        "        super(XMat, self).__init__(self._params_with_grad, defaults)        \n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure):\n",
        "        \"\"\"\n",
        "        Performs a single step of PSGD with Xmat preconditioner, i.e.,\n",
        "        updating the trainable parameters once, and returning what closure returns.\n",
        "        Args:\n",
        "            closure (callable): a closure that evaluates the function of self._params_with_grad,\n",
        "                                and returns the loss, or an iterable with the first one being loss.\n",
        "                                Random numbers, if any, used inside the closure should be generated starting\n",
        "                                from the same rng state if self.exact_hessian_vector_product = False; otherwise doesn't matter.\n",
        "        \"\"\"\n",
        "        if torch.rand([]) < self.preconditioner_update_probability:\n",
        "            # evaluates gradients, Hessian-vector product, and updates the preconditioner\n",
        "            if self.exact_hessian_vector_product:\n",
        "                # exact Hessian-vector product\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad, create_graph=True)\n",
        "                    vs = [torch.randn_like(param) for param in self._params_with_grad]\n",
        "                    Hvs = torch.autograd.grad(grads, self._params_with_grad, vs)\n",
        "            else:\n",
        "                # approximate Hessian-vector product via finite-difference formulae. Use it with cautions.\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "                vs = [self._delta_param_scale * torch.randn_like(param) for param in self._params_with_grad]\n",
        "                [param.add_(v) for (param, v) in zip(self._params_with_grad, vs)]\n",
        "                with torch.enable_grad():\n",
        "                    perturbed_returns = closure()\n",
        "                    perturbed_loss = perturbed_returns if isinstance(perturbed_returns, torch.Tensor) else perturbed_returns[0]\n",
        "                    perturbed_grads = torch.autograd.grad(perturbed_loss, self._params_with_grad)\n",
        "                Hvs = [perturbed_g - g for (perturbed_g, g) in zip(perturbed_grads, grads)]\n",
        "            # update preconditioner\n",
        "            v = torch.cat([torch.flatten(v) for v in vs])\n",
        "            h = torch.cat([torch.flatten(h) for h in Hvs])\n",
        "            if self.exact_hessian_vector_product:\n",
        "                update_precond_Xmat_math_(self._a, self._b,\n",
        "                                         v, h, step=self.lr_preconditioner, tiny=self._tiny)\n",
        "            else:  # compensate the levels of v and h; helpful to reduce numerical errors in half-precision training\n",
        "                update_precond_Xmat_math_(self._a, self._b,\n",
        "                                         v/self._delta_param_scale, h/self._delta_param_scale,\n",
        "                                         step=self.lr_preconditioner, tiny=self._tiny)\n",
        "        else:\n",
        "            # only evaluates the gradients\n",
        "            with torch.enable_grad():\n",
        "                closure_returns = closure()\n",
        "                loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "            vs = None  # no vs and Hvs\n",
        "\n",
        "        # preconditioned gradients; momentum is optional        \n",
        "        grad = torch.cat([torch.flatten(g) for g in grads])\n",
        "        if self.momentum > 0:\n",
        "            if self._m is None:\n",
        "                self._m = (1 - self.momentum)*grad\n",
        "            else:\n",
        "                self._m.mul_(self.momentum).add_((1 - self.momentum)*grad)\n",
        "            pre_grad = precond_grad_Xmat_math(self._a, self._b, self._m)\n",
        "        else:\n",
        "            self._m = None # clean the buffer when momentum is set to zero again \n",
        "            pre_grad = precond_grad_Xmat_math(self._a, self._b, grad)\n",
        "        \n",
        "        # gradient clipping is optional\n",
        "        if self.grad_clip_max_norm is None:\n",
        "            lr = self.lr_params\n",
        "        else:\n",
        "            grad_norm = torch.linalg.vector_norm(pre_grad) + self._tiny\n",
        "            lr = self.lr_params * min(self.grad_clip_max_norm / grad_norm, 1.0)\n",
        "\n",
        "        # update the parameters\n",
        "        if self.exact_hessian_vector_product or (vs is None):\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param))\n",
        "             for (param, i, j) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes)]\n",
        "        else:  # in this case, do not forget to remove the perturbation on parameters\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param) + v)\n",
        "             for (param, i, j, v) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes, vs)]\n",
        "        # return whatever closure returns\n",
        "        return closure_returns\n",
        "\n",
        "################## end of Xmat preconditioner #################################\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# The classic NewtonRaphson type preconditioner.\n",
        "# Clearly, it is applicable only to small scale problems \n",
        "#\n",
        "\n",
        "# @torch.jit.script\n",
        "def update_precond_newton_math_(Q, v, h, step, tiny):\n",
        "    # type: (Tensor, Tensor, Tensor, float, float) -> None\n",
        "    \"\"\"\n",
        "    Update the classic NewtonRaphson type preconditioner P = Q'*Q with (v, h).\n",
        "    \"\"\"\n",
        "    a = Q.mm(h)\n",
        "    b = torch.linalg.solve_triangular(Q.t(), v, upper=False)\n",
        "    grad = torch.triu(a.mm(a.t()) - b.mm(b.t()))\n",
        "    mu = step/(grad.abs().max() + tiny)      \n",
        "    Q.sub_(mu*grad.mm(Q))\n",
        "\n",
        "class Newton:\n",
        "    \"\"\"\n",
        "    Implements the classic NewtonRaphson type preconditioner for SGD as a class.\n",
        "    Args for initialization:\n",
        "        params_with_grad: a list of parameters or variables requiring gradients;\n",
        "        preconditioner_init_scale: initial scale of Q, i.e., Q = preconditioner_init_scale*eye();\n",
        "        lr_params: normalized learning rate for parameters in range [0, 1];\n",
        "        lr_preconditioner: normalized learning rate for preconditioner in range [0, 1];\n",
        "        momentum: momentum factor in range [0,1);\n",
        "        grad_clip_max_norm: maximum allowable gradient norm after clipping, None for no clipping;\n",
        "        preconditioner_update_probability: probability on updating Q, 1 for updating at every step, and 0 for never, i.e., SGD;\n",
        "        exact_hessian_vector_product: True for exact Hessian-vector product via 2nd derivative,\n",
        "                                    and False for approximate one via finite-difference formulae.\n",
        "    Notes:\n",
        "        Note 1: The Hessian-vector product can be approximated using the finite-difference formulae by setting\n",
        "        exact_hessian_vector_product = False when the 2nd derivatives is not available.\n",
        "        In this case, make sure that the closure produces the same outputs given the same inputs,\n",
        "        except for numerical errors due to non-deterministic behaviors.\n",
        "        Random numbers, if any, used inside the closure should be generated starting from the same state, where the rng state can be\n",
        "        read and set by, e.g., `torch.cuda.get_rng_state' and `torch.cuda.set_rng_state', respectively.\n",
        "        \n",
        "        Note 2: Momentum here is the moving average of gradient so that its setting is decoupled from the learning rate.\n",
        "        This is necessary as the learning rate in PSGD is normalized.\n",
        "        Note 3: Currently, no support of sparse and mixed-precision gradients.\n",
        "        Note 4: lr_params, lr_preconditioner, momentum, grad_clip_max_norm, preconditioner_update_probability, and\n",
        "        exact_hessian_vector_product (bool) all can be reset on the fly.\n",
        "    \"\"\"\n",
        "    def __init__(self, params_with_grad, preconditioner_init_scale=1.0,\n",
        "                 lr_params=0.01, lr_preconditioner=0.01, momentum=0.0, \n",
        "                 grad_clip_max_norm=None, preconditioner_update_probability=1.0,\n",
        "                 exact_hessian_vector_product: bool = True):\n",
        "        # mutable members\n",
        "        self.lr_params = lr_params\n",
        "        self.lr_preconditioner = lr_preconditioner\n",
        "        self.momentum = momentum if (0<momentum<1) else 0.0\n",
        "        self.grad_clip_max_norm = grad_clip_max_norm\n",
        "        self.preconditioner_update_probability = preconditioner_update_probability\n",
        "        self.exact_hessian_vector_product = exact_hessian_vector_product\n",
        "        # protected members\n",
        "        params_with_grad = [params_with_grad, ] if isinstance(params_with_grad, torch.Tensor) else params_with_grad\n",
        "        self._params_with_grad = [param for param in params_with_grad if param.requires_grad]  # double check requires_grad flag\n",
        "        dtype, device = self._params_with_grad[0].dtype, self._params_with_grad[0].device\n",
        "        self._tiny = torch.finfo(dtype).tiny\n",
        "        self._delta_param_scale = torch.finfo(dtype).eps ** 0.5\n",
        "        self._param_sizes = [torch.numel(param) for param in self._params_with_grad]\n",
        "        self._param_cumsizes = torch.cumsum(torch.tensor(self._param_sizes), 0)\n",
        "        num_params = self._param_cumsizes[-1]\n",
        "        self._Q = torch.eye(num_params, dtype=dtype, device=device)*preconditioner_init_scale\n",
        "        self._m = None # buffer for momentum \n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure):\n",
        "        \"\"\"\n",
        "        Performs a single step of PSGD with NewtonRaphson preconditioner, i.e.,\n",
        "        updating the trainable parameters once, and returning what closure returns.\n",
        "        Args:\n",
        "            closure (callable): a closure that evaluates the function of self._params_with_grad,\n",
        "                                and returns the loss, or an iterable with the first one being loss.\n",
        "                                Random numbers, if any, used inside the closure should be generated starting\n",
        "                                from the same rng state if self.exact_hessian_vector_product = False; otherwise doesn't matter.\n",
        "        \"\"\"\n",
        "        if torch.rand([]) < self.preconditioner_update_probability:\n",
        "            # evaluates gradients, Hessian-vector product, and updates the preconditioner\n",
        "            if self.exact_hessian_vector_product:\n",
        "                # exact Hessian-vector product\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad, create_graph=True)\n",
        "                    vs = [torch.randn_like(param) for param in self._params_with_grad]\n",
        "                    Hvs = torch.autograd.grad(grads, self._params_with_grad, vs)\n",
        "            else:\n",
        "                # approximate Hessian-vector product via finite-difference formulae. Use it with cautions.\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "                vs = [self._delta_param_scale * torch.randn_like(param) for param in self._params_with_grad]\n",
        "                [param.add_(v) for (param, v) in zip(self._params_with_grad, vs)]\n",
        "                with torch.enable_grad():\n",
        "                    perturbed_returns = closure()\n",
        "                    perturbed_loss = perturbed_returns if isinstance(perturbed_returns, torch.Tensor) else perturbed_returns[0]\n",
        "                    perturbed_grads = torch.autograd.grad(perturbed_loss, self._params_with_grad)\n",
        "                Hvs = [perturbed_g - g for (perturbed_g, g) in zip(perturbed_grads, grads)]\n",
        "            # update preconditioner\n",
        "            v = torch.cat([torch.flatten(v) for v in vs])\n",
        "            h = torch.cat([torch.flatten(h) for h in Hvs])\n",
        "            if self.exact_hessian_vector_product:\n",
        "                update_precond_newton_math_(self._Q,\n",
        "                                            v[:,None], h[:,None], step=self.lr_preconditioner, tiny=self._tiny)\n",
        "            else:  # compensate the levels of v and h; helpful to reduce numerical errors in half-precision training\n",
        "                update_precond_newton_math_(self._Q,\n",
        "                                            v[:,None]/self._delta_param_scale, h[:,None]/self._delta_param_scale,\n",
        "                                            step=self.lr_preconditioner, tiny=self._tiny)\n",
        "        else:\n",
        "            # only evaluates the gradients\n",
        "            with torch.enable_grad():\n",
        "                closure_returns = closure()\n",
        "                loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "            vs = None  # no vs and Hvs\n",
        "\n",
        "        # preconditioned gradients; momentum is optional        \n",
        "        grad = torch.cat([torch.flatten(g) for g in grads])\n",
        "        if self.momentum > 0:\n",
        "            if self._m is None:\n",
        "                self._m = (1 - self.momentum)*grad\n",
        "            else:\n",
        "                self._m.mul_(self.momentum).add_((1 - self.momentum)*grad)\n",
        "            pre_grad = self._Q.t() @ (self._Q @ self._m)\n",
        "        else:\n",
        "            self._m = None # clean the buffer when momentum is set to zero again \n",
        "            pre_grad = self._Q.t() @ (self._Q @ grad)\n",
        "        \n",
        "        # gradient clipping is optional\n",
        "        if self.grad_clip_max_norm is None:\n",
        "            lr = self.lr_params\n",
        "        else:\n",
        "            grad_norm = torch.linalg.vector_norm(pre_grad) + self._tiny\n",
        "            lr = self.lr_params * min(self.grad_clip_max_norm / grad_norm, 1.0)\n",
        "\n",
        "        # update the parameters\n",
        "        if self.exact_hessian_vector_product or (vs is None):\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param))\n",
        "             for (param, i, j) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes)]\n",
        "        else:  # in this case, do not forget to remove the perturbation on parameters\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param) + v)\n",
        "             for (param, i, j, v) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes, vs)]\n",
        "        # return whatever closure returns\n",
        "        return closure_returns\n",
        "\n",
        "################## end of NewtonRaphson preconditioner #################################"
      ],
      "metadata": {
        "id": "-Ssga-nW9fNk",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models"
      ],
      "metadata": {
        "id": "8uj_9DcLrZfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Data"
      ],
      "metadata": {
        "id": "qeq7aDhdr0fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RNN network with the classic delayed XOR problem. Accelerated directional derivative \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "# import preconditioned_stochastic_gradient_descent as psgd\n",
        "\n",
        "device = torch.device('cpu')\n",
        "batch_size, seq_len = 128, 32          # increasing sequence_length or decreasing dimension_hidden_layer will make learning harder;\n",
        "dim_in, dim_hidden, dim_out = 2, 30, 1  # current setting can solve seq len 80 ~ 90 reliably without the help of momentum \n",
        "\n",
        "def generate_train_data():\n",
        "    x = np.zeros([batch_size, seq_len, dim_in], dtype=np.float32)\n",
        "    y = np.zeros([batch_size, dim_out], dtype=np.float32)\n",
        "    for i in range(batch_size):\n",
        "        x[i, :, 0] = np.random.choice([-1.0, 1.0], seq_len)\n",
        "\n",
        "        i1 = int(np.floor(np.random.rand() * 0.1 * seq_len))\n",
        "        i2 = int(np.floor(np.random.rand() * 0.4 * seq_len + 0.1 * seq_len))\n",
        "        x[i, i1, 1] = 1.0\n",
        "        x[i, i2, 1] = 1.0\n",
        "        if x[i, i1, 0] == x[i, i2, 0]:  # XOR\n",
        "            y[i] = -1.0  # lable 0\n",
        "        else:\n",
        "            y[i] = 1.0  # lable 1\n",
        "\n",
        "    # tranpose x to format (sequence_length, batch_size, dimension_of_input)\n",
        "    return [torch.tensor(np.transpose(x, [1, 0, 2])).to(device),\n",
        "            torch.tensor(y).to(device)]\n",
        "\n",
        "# generate a random orthogonal matrix for recurrent matrix initialization\n",
        "def get_rand_orth(dim):\n",
        "    temp = np.random.normal(size=[dim, dim])\n",
        "    q, _ = np.linalg.qr(temp)\n",
        "    return torch.tensor(q, dtype=torch.float32).to(device)\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.W1x = torch.nn.Parameter(0.1 * torch.randn(dim_in, dim_hidden))\n",
        "        self.W1h = torch.nn.Parameter(get_rand_orth(dim_hidden))\n",
        "        self.b1 = torch.nn.Parameter(torch.zeros(dim_hidden))\n",
        "        self.W2 = torch.nn.Parameter(0.1 * torch.randn(dim_hidden, dim_out))\n",
        "        self.b2 = torch.nn.Parameter(torch.zeros([]))\n",
        "\n",
        "    def forward(self, xs):\n",
        "        h = torch.zeros(batch_size, dim_hidden, device=device)\n",
        "        for x in torch.unbind(xs):\n",
        "            h = torch.tanh(x @ self.W1x + h @ self.W1h + self.b1)\n",
        "        return h @ self.W2 + self.b2\n",
        "\n",
        "model = Model().to(device)\n",
        "# initialize the PSGD optimizer with the low-rank approximation preconditioner \n",
        "opt = UVd(model.parameters(),\n",
        "               # rank_of_approximation=10, preconditioner_init_scale=1.0,\n",
        "               # lr_params=0.01, lr_preconditioner=0.01, momentum=0.9,\n",
        "               grad_clip_max_norm=1.0, # preconditioner_update_probability=1.0,\n",
        "               exact_hessian_vector_product=False,\n",
        "               momentum=0.9,\n",
        "               directional = 0\n",
        "               )\n",
        "\n",
        "\n",
        "def train_loss(xy_pair):  # logistic loss\n",
        "    return -torch.mean(torch.log(torch.sigmoid(xy_pair[1] * model(xy_pair[0]))))\n",
        "\n",
        "Losses = []\n",
        "for num_iter in range(10000):\n",
        "    train_data = generate_train_data()\n",
        "\n",
        "    rng_state = torch.get_rng_state()\n",
        "    # rng_cuda_state = torch.cuda.get_rng_state()\n",
        "    def closure(): \n",
        "        # If exact_hessian_vector_product=False and rng is used inside closure, \n",
        "        # make sure rng starts from the same state; otherwise, doesn't matter. \n",
        "        torch.set_rng_state(rng_state)\n",
        "        # torch.cuda.set_rng_state(rng_cuda_state)\n",
        "        return train_loss(train_data) # return a loss\n",
        "\n",
        "    loss = opt.step(closure)\n",
        "    Losses.append(loss.item())\n",
        "    print('Iteration: {}; loss: {}'.format(num_iter, Losses[-1]))\n",
        "\n",
        "plt.plot(Losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SoxKl2JUHxsi",
        "outputId": "b0441b73-a362-4621-e93b-2990136f4b7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration: 5000; loss: 0.6909657120704651\n",
            "Iteration: 5001; loss: 0.694749653339386\n",
            "Iteration: 5002; loss: 0.694681704044342\n",
            "Iteration: 5003; loss: 0.6933072209358215\n",
            "Iteration: 5004; loss: 0.6931990385055542\n",
            "Iteration: 5005; loss: 0.6923579573631287\n",
            "Iteration: 5006; loss: 0.6942448616027832\n",
            "Iteration: 5007; loss: 0.6938759684562683\n",
            "Iteration: 5008; loss: 0.6934541463851929\n",
            "Iteration: 5009; loss: 0.6936913132667542\n",
            "Iteration: 5010; loss: 0.692625880241394\n",
            "Iteration: 5011; loss: 0.6932264566421509\n",
            "Iteration: 5012; loss: 0.6942523121833801\n",
            "Iteration: 5013; loss: 0.6939437389373779\n",
            "Iteration: 5014; loss: 0.6948838829994202\n",
            "Iteration: 5015; loss: 0.6919194459915161\n",
            "Iteration: 5016; loss: 0.6919542551040649\n",
            "Iteration: 5017; loss: 0.6943112015724182\n",
            "Iteration: 5018; loss: 0.692568302154541\n",
            "Iteration: 5019; loss: 0.6946664452552795\n",
            "Iteration: 5020; loss: 0.6941471099853516\n",
            "Iteration: 5021; loss: 0.6920502185821533\n",
            "Iteration: 5022; loss: 0.6931614875793457\n",
            "Iteration: 5023; loss: 0.6931151747703552\n",
            "Iteration: 5024; loss: 0.6943032741546631\n",
            "Iteration: 5025; loss: 0.6922754049301147\n",
            "Iteration: 5026; loss: 0.6962149739265442\n",
            "Iteration: 5027; loss: 0.6934458017349243\n",
            "Iteration: 5028; loss: 0.69367516040802\n",
            "Iteration: 5029; loss: 0.693231999874115\n",
            "Iteration: 5030; loss: 0.6932099461555481\n",
            "Iteration: 5031; loss: 0.6937274932861328\n",
            "Iteration: 5032; loss: 0.6928130984306335\n",
            "Iteration: 5033; loss: 0.691974401473999\n",
            "Iteration: 5034; loss: 0.6942198872566223\n",
            "Iteration: 5035; loss: 0.6931112408638\n",
            "Iteration: 5036; loss: 0.6944103240966797\n",
            "Iteration: 5037; loss: 0.6941030621528625\n",
            "Iteration: 5038; loss: 0.693444550037384\n",
            "Iteration: 5039; loss: 0.6916587352752686\n",
            "Iteration: 5040; loss: 0.6891933679580688\n",
            "Iteration: 5041; loss: 0.6929091215133667\n",
            "Iteration: 5042; loss: 0.6930041909217834\n",
            "Iteration: 5043; loss: 0.6931943893432617\n",
            "Iteration: 5044; loss: 0.6941534876823425\n",
            "Iteration: 5045; loss: 0.6936966180801392\n",
            "Iteration: 5046; loss: 0.694716215133667\n",
            "Iteration: 5047; loss: 0.6929919719696045\n",
            "Iteration: 5048; loss: 0.6937283277511597\n",
            "Iteration: 5049; loss: 0.6911563277244568\n",
            "Iteration: 5050; loss: 0.6941801905632019\n",
            "Iteration: 5051; loss: 0.6946541666984558\n",
            "Iteration: 5052; loss: 0.6925324201583862\n",
            "Iteration: 5053; loss: 0.695486843585968\n",
            "Iteration: 5054; loss: 0.6892589330673218\n",
            "Iteration: 5055; loss: 0.6944273710250854\n",
            "Iteration: 5056; loss: 0.6945971250534058\n",
            "Iteration: 5057; loss: 0.695207417011261\n",
            "Iteration: 5058; loss: 0.6958748698234558\n",
            "Iteration: 5059; loss: 0.6967043876647949\n",
            "Iteration: 5060; loss: 0.6938107013702393\n",
            "Iteration: 5061; loss: 0.6950697302818298\n",
            "Iteration: 5062; loss: 0.6943040490150452\n",
            "Iteration: 5063; loss: 0.6928492784500122\n",
            "Iteration: 5064; loss: 0.6931861639022827\n",
            "Iteration: 5065; loss: 0.6930305361747742\n",
            "Iteration: 5066; loss: 0.696419358253479\n",
            "Iteration: 5067; loss: 0.6949183940887451\n",
            "Iteration: 5068; loss: 0.6914600133895874\n",
            "Iteration: 5069; loss: 0.6930067539215088\n",
            "Iteration: 5070; loss: 0.6924392580986023\n",
            "Iteration: 5071; loss: 0.6952064633369446\n",
            "Iteration: 5072; loss: 0.6915527582168579\n",
            "Iteration: 5073; loss: 0.6911189556121826\n",
            "Iteration: 5074; loss: 0.6933726668357849\n",
            "Iteration: 5075; loss: 0.6930565237998962\n",
            "Iteration: 5076; loss: 0.6931036114692688\n",
            "Iteration: 5077; loss: 0.6931135058403015\n",
            "Iteration: 5078; loss: 0.6934595704078674\n",
            "Iteration: 5079; loss: 0.691726565361023\n",
            "Iteration: 5080; loss: 0.6934882998466492\n",
            "Iteration: 5081; loss: 0.6935874819755554\n",
            "Iteration: 5082; loss: 0.6931646466255188\n",
            "Iteration: 5083; loss: 0.6938726305961609\n",
            "Iteration: 5084; loss: 0.6928731203079224\n",
            "Iteration: 5085; loss: 0.6935518980026245\n",
            "Iteration: 5086; loss: 0.6922398805618286\n",
            "Iteration: 5087; loss: 0.6930633783340454\n",
            "Iteration: 5088; loss: 0.6925955414772034\n",
            "Iteration: 5089; loss: 0.6957842707633972\n",
            "Iteration: 5090; loss: 0.6935617923736572\n",
            "Iteration: 5091; loss: 0.6935383677482605\n",
            "Iteration: 5092; loss: 0.6929771900177002\n",
            "Iteration: 5093; loss: 0.6933085322380066\n",
            "Iteration: 5094; loss: 0.6941426396369934\n",
            "Iteration: 5095; loss: 0.6942472457885742\n",
            "Iteration: 5096; loss: 0.694891095161438\n",
            "Iteration: 5097; loss: 0.6915150880813599\n",
            "Iteration: 5098; loss: 0.6948228478431702\n",
            "Iteration: 5099; loss: 0.6929668188095093\n",
            "Iteration: 5100; loss: 0.6921670436859131\n",
            "Iteration: 5101; loss: 0.6925143003463745\n",
            "Iteration: 5102; loss: 0.6920772790908813\n",
            "Iteration: 5103; loss: 0.6931127309799194\n",
            "Iteration: 5104; loss: 0.6954187750816345\n",
            "Iteration: 5105; loss: 0.6925808787345886\n",
            "Iteration: 5106; loss: 0.6912628412246704\n",
            "Iteration: 5107; loss: 0.6923269033432007\n",
            "Iteration: 5108; loss: 0.6930471658706665\n",
            "Iteration: 5109; loss: 0.6928378939628601\n",
            "Iteration: 5110; loss: 0.6910569667816162\n",
            "Iteration: 5111; loss: 0.6960569024085999\n",
            "Iteration: 5112; loss: 0.6937398314476013\n",
            "Iteration: 5113; loss: 0.6932993531227112\n",
            "Iteration: 5114; loss: 0.6923421621322632\n",
            "Iteration: 5115; loss: 0.6928420662879944\n",
            "Iteration: 5116; loss: 0.6936516761779785\n",
            "Iteration: 5117; loss: 0.6925053596496582\n",
            "Iteration: 5118; loss: 0.6923868656158447\n",
            "Iteration: 5119; loss: 0.6931045651435852\n",
            "Iteration: 5120; loss: 0.690758228302002\n",
            "Iteration: 5121; loss: 0.6939076781272888\n",
            "Iteration: 5122; loss: 0.6936206221580505\n",
            "Iteration: 5123; loss: 0.6956035494804382\n",
            "Iteration: 5124; loss: 0.695777416229248\n",
            "Iteration: 5125; loss: 0.6916086673736572\n",
            "Iteration: 5126; loss: 0.6927092671394348\n",
            "Iteration: 5127; loss: 0.692229151725769\n",
            "Iteration: 5128; loss: 0.6932161450386047\n",
            "Iteration: 5129; loss: 0.6931243538856506\n",
            "Iteration: 5130; loss: 0.6926717758178711\n",
            "Iteration: 5131; loss: 0.6935881972312927\n",
            "Iteration: 5132; loss: 0.6925956606864929\n",
            "Iteration: 5133; loss: 0.6914812922477722\n",
            "Iteration: 5134; loss: 0.6936777830123901\n",
            "Iteration: 5135; loss: 0.6920388340950012\n",
            "Iteration: 5136; loss: 0.6932088732719421\n",
            "Iteration: 5137; loss: 0.6936144828796387\n",
            "Iteration: 5138; loss: 0.6929779052734375\n",
            "Iteration: 5139; loss: 0.6960028409957886\n",
            "Iteration: 5140; loss: 0.6941019296646118\n",
            "Iteration: 5141; loss: 0.6937015056610107\n",
            "Iteration: 5142; loss: 0.6934806704521179\n",
            "Iteration: 5143; loss: 0.692963182926178\n",
            "Iteration: 5144; loss: 0.6931742429733276\n",
            "Iteration: 5145; loss: 0.6900853514671326\n",
            "Iteration: 5146; loss: 0.692893922328949\n",
            "Iteration: 5147; loss: 0.69464111328125\n",
            "Iteration: 5148; loss: 0.6944546699523926\n",
            "Iteration: 5149; loss: 0.6902421116828918\n",
            "Iteration: 5150; loss: 0.6913118958473206\n",
            "Iteration: 5151; loss: 0.6943309307098389\n",
            "Iteration: 5152; loss: 0.6953749060630798\n",
            "Iteration: 5153; loss: 0.6921409964561462\n",
            "Iteration: 5154; loss: 0.6922764182090759\n",
            "Iteration: 5155; loss: 0.6920766830444336\n",
            "Iteration: 5156; loss: 0.6967753767967224\n",
            "Iteration: 5157; loss: 0.6931034326553345\n",
            "Iteration: 5158; loss: 0.6944289207458496\n",
            "Iteration: 5159; loss: 0.6916840672492981\n",
            "Iteration: 5160; loss: 0.6922118663787842\n",
            "Iteration: 5161; loss: 0.6940861344337463\n",
            "Iteration: 5162; loss: 0.6942991018295288\n",
            "Iteration: 5163; loss: 0.6936194896697998\n",
            "Iteration: 5164; loss: 0.6927547454833984\n",
            "Iteration: 5165; loss: 0.6907376646995544\n",
            "Iteration: 5166; loss: 0.6949106454849243\n",
            "Iteration: 5167; loss: 0.6958352327346802\n",
            "Iteration: 5168; loss: 0.6930297017097473\n",
            "Iteration: 5169; loss: 0.6956702470779419\n",
            "Iteration: 5170; loss: 0.6949630379676819\n",
            "Iteration: 5171; loss: 0.692231297492981\n",
            "Iteration: 5172; loss: 0.6942600011825562\n",
            "Iteration: 5173; loss: 0.6943506002426147\n",
            "Iteration: 5174; loss: 0.6955393552780151\n",
            "Iteration: 5175; loss: 0.6935261487960815\n",
            "Iteration: 5176; loss: 0.6942036747932434\n",
            "Iteration: 5177; loss: 0.6928286552429199\n",
            "Iteration: 5178; loss: 0.6926289796829224\n",
            "Iteration: 5179; loss: 0.6927115321159363\n",
            "Iteration: 5180; loss: 0.6940652132034302\n",
            "Iteration: 5181; loss: 0.6934712529182434\n",
            "Iteration: 5182; loss: 0.6930769681930542\n",
            "Iteration: 5183; loss: 0.6910303235054016\n",
            "Iteration: 5184; loss: 0.6935968399047852\n",
            "Iteration: 5185; loss: 0.6933362483978271\n",
            "Iteration: 5186; loss: 0.6920254230499268\n",
            "Iteration: 5187; loss: 0.6923249363899231\n",
            "Iteration: 5188; loss: 0.6936494708061218\n",
            "Iteration: 5189; loss: 0.6939736008644104\n",
            "Iteration: 5190; loss: 0.6941397190093994\n",
            "Iteration: 5191; loss: 0.6919504404067993\n",
            "Iteration: 5192; loss: 0.6951982975006104\n",
            "Iteration: 5193; loss: 0.6925894021987915\n",
            "Iteration: 5194; loss: 0.6956814527511597\n",
            "Iteration: 5195; loss: 0.692380428314209\n",
            "Iteration: 5196; loss: 0.6935927867889404\n",
            "Iteration: 5197; loss: 0.6928248405456543\n",
            "Iteration: 5198; loss: 0.693199872970581\n",
            "Iteration: 5199; loss: 0.6952661871910095\n",
            "Iteration: 5200; loss: 0.6942868232727051\n",
            "Iteration: 5201; loss: 0.6944969296455383\n",
            "Iteration: 5202; loss: 0.6959867477416992\n",
            "Iteration: 5203; loss: 0.6918066740036011\n",
            "Iteration: 5204; loss: 0.6944910883903503\n",
            "Iteration: 5205; loss: 0.6945574283599854\n",
            "Iteration: 5206; loss: 0.6918247938156128\n",
            "Iteration: 5207; loss: 0.6945666670799255\n",
            "Iteration: 5208; loss: 0.6924092173576355\n",
            "Iteration: 5209; loss: 0.6925953030586243\n",
            "Iteration: 5210; loss: 0.6925684213638306\n",
            "Iteration: 5211; loss: 0.694029688835144\n",
            "Iteration: 5212; loss: 0.6953459978103638\n",
            "Iteration: 5213; loss: 0.6931787133216858\n",
            "Iteration: 5214; loss: 0.6921889781951904\n",
            "Iteration: 5215; loss: 0.6937755346298218\n",
            "Iteration: 5216; loss: 0.6927703022956848\n",
            "Iteration: 5217; loss: 0.6917322874069214\n",
            "Iteration: 5218; loss: 0.6928237676620483\n",
            "Iteration: 5219; loss: 0.694061279296875\n",
            "Iteration: 5220; loss: 0.6912233233451843\n",
            "Iteration: 5221; loss: 0.6941966414451599\n",
            "Iteration: 5222; loss: 0.6945555210113525\n",
            "Iteration: 5223; loss: 0.694705605506897\n",
            "Iteration: 5224; loss: 0.6927878856658936\n",
            "Iteration: 5225; loss: 0.6922863125801086\n",
            "Iteration: 5226; loss: 0.6920362114906311\n",
            "Iteration: 5227; loss: 0.6937454342842102\n",
            "Iteration: 5228; loss: 0.6939229369163513\n",
            "Iteration: 5229; loss: 0.6907649040222168\n",
            "Iteration: 5230; loss: 0.69062739610672\n",
            "Iteration: 5231; loss: 0.6926223039627075\n",
            "Iteration: 5232; loss: 0.6941868662834167\n",
            "Iteration: 5233; loss: 0.6918442845344543\n",
            "Iteration: 5234; loss: 0.6902220249176025\n",
            "Iteration: 5235; loss: 0.6925145983695984\n",
            "Iteration: 5236; loss: 0.6929727792739868\n",
            "Iteration: 5237; loss: 0.6921891570091248\n",
            "Iteration: 5238; loss: 0.6925238966941833\n",
            "Iteration: 5239; loss: 0.6955118775367737\n",
            "Iteration: 5240; loss: 0.6945948600769043\n",
            "Iteration: 5241; loss: 0.6940491199493408\n",
            "Iteration: 5242; loss: 0.691389799118042\n",
            "Iteration: 5243; loss: 0.6917998790740967\n",
            "Iteration: 5244; loss: 0.6908842921257019\n",
            "Iteration: 5245; loss: 0.6934127807617188\n",
            "Iteration: 5246; loss: 0.6945079565048218\n",
            "Iteration: 5247; loss: 0.6947232484817505\n",
            "Iteration: 5248; loss: 0.6929803490638733\n",
            "Iteration: 5249; loss: 0.695716381072998\n",
            "Iteration: 5250; loss: 0.6920576095581055\n",
            "Iteration: 5251; loss: 0.6924117207527161\n",
            "Iteration: 5252; loss: 0.6949325203895569\n",
            "Iteration: 5253; loss: 0.6942036151885986\n",
            "Iteration: 5254; loss: 0.6945139169692993\n",
            "Iteration: 5255; loss: 0.6933760046958923\n",
            "Iteration: 5256; loss: 0.6937947273254395\n",
            "Iteration: 5257; loss: 0.6939110159873962\n",
            "Iteration: 5258; loss: 0.6907133460044861\n",
            "Iteration: 5259; loss: 0.6930246949195862\n",
            "Iteration: 5260; loss: 0.6933895945549011\n",
            "Iteration: 5261; loss: 0.6934399604797363\n",
            "Iteration: 5262; loss: 0.6956627368927002\n",
            "Iteration: 5263; loss: 0.6937715411186218\n",
            "Iteration: 5264; loss: 0.6919894814491272\n",
            "Iteration: 5265; loss: 0.6912651062011719\n",
            "Iteration: 5266; loss: 0.6958984732627869\n",
            "Iteration: 5267; loss: 0.6924280524253845\n",
            "Iteration: 5268; loss: 0.6924647688865662\n",
            "Iteration: 5269; loss: 0.6923316121101379\n",
            "Iteration: 5270; loss: 0.6949819922447205\n",
            "Iteration: 5271; loss: 0.6933836340904236\n",
            "Iteration: 5272; loss: 0.6941325664520264\n",
            "Iteration: 5273; loss: 0.6942667365074158\n",
            "Iteration: 5274; loss: 0.6934537291526794\n",
            "Iteration: 5275; loss: 0.6922938823699951\n",
            "Iteration: 5276; loss: 0.6925579905509949\n",
            "Iteration: 5277; loss: 0.6920459270477295\n",
            "Iteration: 5278; loss: 0.6944747567176819\n",
            "Iteration: 5279; loss: 0.692745566368103\n",
            "Iteration: 5280; loss: 0.6923970580101013\n",
            "Iteration: 5281; loss: 0.6958803534507751\n",
            "Iteration: 5282; loss: 0.6948174238204956\n",
            "Iteration: 5283; loss: 0.6923218965530396\n",
            "Iteration: 5284; loss: 0.6913806200027466\n",
            "Iteration: 5285; loss: 0.6912272572517395\n",
            "Iteration: 5286; loss: 0.692074179649353\n",
            "Iteration: 5287; loss: 0.6934912800788879\n",
            "Iteration: 5288; loss: 0.69257652759552\n",
            "Iteration: 5289; loss: 0.6929672956466675\n",
            "Iteration: 5290; loss: 0.6955351829528809\n",
            "Iteration: 5291; loss: 0.6932153105735779\n",
            "Iteration: 5292; loss: 0.6933282613754272\n",
            "Iteration: 5293; loss: 0.6941577196121216\n",
            "Iteration: 5294; loss: 0.6935290694236755\n",
            "Iteration: 5295; loss: 0.6932418346405029\n",
            "Iteration: 5296; loss: 0.6917884349822998\n",
            "Iteration: 5297; loss: 0.6899513006210327\n",
            "Iteration: 5298; loss: 0.6935336589813232\n",
            "Iteration: 5299; loss: 0.6916660666465759\n",
            "Iteration: 5300; loss: 0.6936866641044617\n",
            "Iteration: 5301; loss: 0.6924775838851929\n",
            "Iteration: 5302; loss: 0.6917174458503723\n",
            "Iteration: 5303; loss: 0.6959455609321594\n",
            "Iteration: 5304; loss: 0.6916450262069702\n",
            "Iteration: 5305; loss: 0.6910567283630371\n",
            "Iteration: 5306; loss: 0.6921259164810181\n",
            "Iteration: 5307; loss: 0.6933294534683228\n",
            "Iteration: 5308; loss: 0.6912542581558228\n",
            "Iteration: 5309; loss: 0.6917072534561157\n",
            "Iteration: 5310; loss: 0.6938235759735107\n",
            "Iteration: 5311; loss: 0.6941400170326233\n",
            "Iteration: 5312; loss: 0.6924501657485962\n",
            "Iteration: 5313; loss: 0.6921768188476562\n",
            "Iteration: 5314; loss: 0.6935698986053467\n",
            "Iteration: 5315; loss: 0.6947716474533081\n",
            "Iteration: 5316; loss: 0.6930912137031555\n",
            "Iteration: 5317; loss: 0.6936626434326172\n",
            "Iteration: 5318; loss: 0.6917597651481628\n",
            "Iteration: 5319; loss: 0.690996527671814\n",
            "Iteration: 5320; loss: 0.6929264664649963\n",
            "Iteration: 5321; loss: 0.6944800019264221\n",
            "Iteration: 5322; loss: 0.6931629180908203\n",
            "Iteration: 5323; loss: 0.695626437664032\n",
            "Iteration: 5324; loss: 0.693663477897644\n",
            "Iteration: 5325; loss: 0.6909796595573425\n",
            "Iteration: 5326; loss: 0.6958127021789551\n",
            "Iteration: 5327; loss: 0.6947095394134521\n",
            "Iteration: 5328; loss: 0.6954746246337891\n",
            "Iteration: 5329; loss: 0.691478431224823\n",
            "Iteration: 5330; loss: 0.6917938590049744\n",
            "Iteration: 5331; loss: 0.6945347785949707\n",
            "Iteration: 5332; loss: 0.690908670425415\n",
            "Iteration: 5333; loss: 0.6967814564704895\n",
            "Iteration: 5334; loss: 0.6941611766815186\n",
            "Iteration: 5335; loss: 0.6921476125717163\n",
            "Iteration: 5336; loss: 0.6915993690490723\n",
            "Iteration: 5337; loss: 0.6945158839225769\n",
            "Iteration: 5338; loss: 0.6945798993110657\n",
            "Iteration: 5339; loss: 0.6921823620796204\n",
            "Iteration: 5340; loss: 0.6919129490852356\n",
            "Iteration: 5341; loss: 0.6940004825592041\n",
            "Iteration: 5342; loss: 0.6968380808830261\n",
            "Iteration: 5343; loss: 0.692061722278595\n",
            "Iteration: 5344; loss: 0.6944424510002136\n",
            "Iteration: 5345; loss: 0.6907243132591248\n",
            "Iteration: 5346; loss: 0.6958497762680054\n",
            "Iteration: 5347; loss: 0.6950715184211731\n",
            "Iteration: 5348; loss: 0.695034921169281\n",
            "Iteration: 5349; loss: 0.6934544444084167\n",
            "Iteration: 5350; loss: 0.6932128667831421\n",
            "Iteration: 5351; loss: 0.6932259202003479\n",
            "Iteration: 5352; loss: 0.6937925815582275\n",
            "Iteration: 5353; loss: 0.6915256381034851\n",
            "Iteration: 5354; loss: 0.6929031014442444\n",
            "Iteration: 5355; loss: 0.6951751708984375\n",
            "Iteration: 5356; loss: 0.6933448314666748\n",
            "Iteration: 5357; loss: 0.6919247508049011\n",
            "Iteration: 5358; loss: 0.6934024095535278\n",
            "Iteration: 5359; loss: 0.6914830207824707\n",
            "Iteration: 5360; loss: 0.6932774782180786\n",
            "Iteration: 5361; loss: 0.6916103363037109\n",
            "Iteration: 5362; loss: 0.6931973099708557\n",
            "Iteration: 5363; loss: 0.6913528442382812\n",
            "Iteration: 5364; loss: 0.6943116784095764\n",
            "Iteration: 5365; loss: 0.6920592188835144\n",
            "Iteration: 5366; loss: 0.6932061910629272\n",
            "Iteration: 5367; loss: 0.6931434273719788\n",
            "Iteration: 5368; loss: 0.6922435760498047\n",
            "Iteration: 5369; loss: 0.6915525794029236\n",
            "Iteration: 5370; loss: 0.694779634475708\n",
            "Iteration: 5371; loss: 0.6941750645637512\n",
            "Iteration: 5372; loss: 0.6951932907104492\n",
            "Iteration: 5373; loss: 0.6933404207229614\n",
            "Iteration: 5374; loss: 0.6918842792510986\n",
            "Iteration: 5375; loss: 0.6937568187713623\n",
            "Iteration: 5376; loss: 0.6933737993240356\n",
            "Iteration: 5377; loss: 0.6911715269088745\n",
            "Iteration: 5378; loss: 0.6904677748680115\n",
            "Iteration: 5379; loss: 0.6923575401306152\n",
            "Iteration: 5380; loss: 0.6923922896385193\n",
            "Iteration: 5381; loss: 0.6912169456481934\n",
            "Iteration: 5382; loss: 0.692863941192627\n",
            "Iteration: 5383; loss: 0.6923362612724304\n",
            "Iteration: 5384; loss: 0.6926375031471252\n",
            "Iteration: 5385; loss: 0.6937934160232544\n",
            "Iteration: 5386; loss: 0.691589891910553\n",
            "Iteration: 5387; loss: 0.6900603771209717\n",
            "Iteration: 5388; loss: 0.690767765045166\n",
            "Iteration: 5389; loss: 0.6941287517547607\n",
            "Iteration: 5390; loss: 0.6931576132774353\n",
            "Iteration: 5391; loss: 0.6920473575592041\n",
            "Iteration: 5392; loss: 0.69303297996521\n",
            "Iteration: 5393; loss: 0.6925961971282959\n",
            "Iteration: 5394; loss: 0.6931359767913818\n",
            "Iteration: 5395; loss: 0.6921999454498291\n",
            "Iteration: 5396; loss: 0.6926776170730591\n",
            "Iteration: 5397; loss: 0.6942040324211121\n",
            "Iteration: 5398; loss: 0.693211019039154\n",
            "Iteration: 5399; loss: 0.6906450986862183\n",
            "Iteration: 5400; loss: 0.6943301558494568\n",
            "Iteration: 5401; loss: 0.69306480884552\n",
            "Iteration: 5402; loss: 0.6961616277694702\n",
            "Iteration: 5403; loss: 0.6929224133491516\n",
            "Iteration: 5404; loss: 0.6883672475814819\n",
            "Iteration: 5405; loss: 0.6936569213867188\n",
            "Iteration: 5406; loss: 0.6932573318481445\n",
            "Iteration: 5407; loss: 0.6935142278671265\n",
            "Iteration: 5408; loss: 0.6927725076675415\n",
            "Iteration: 5409; loss: 0.6959130167961121\n",
            "Iteration: 5410; loss: 0.6933104991912842\n",
            "Iteration: 5411; loss: 0.6965617537498474\n",
            "Iteration: 5412; loss: 0.6943283081054688\n",
            "Iteration: 5413; loss: 0.6910742521286011\n",
            "Iteration: 5414; loss: 0.6923917531967163\n",
            "Iteration: 5415; loss: 0.6930906176567078\n",
            "Iteration: 5416; loss: 0.6918342113494873\n",
            "Iteration: 5417; loss: 0.6917964220046997\n",
            "Iteration: 5418; loss: 0.6897642612457275\n",
            "Iteration: 5419; loss: 0.6964989304542542\n",
            "Iteration: 5420; loss: 0.6913115382194519\n",
            "Iteration: 5421; loss: 0.6959713697433472\n",
            "Iteration: 5422; loss: 0.6930496692657471\n",
            "Iteration: 5423; loss: 0.6938270926475525\n",
            "Iteration: 5424; loss: 0.6924903392791748\n",
            "Iteration: 5425; loss: 0.6922438144683838\n",
            "Iteration: 5426; loss: 0.6921552419662476\n",
            "Iteration: 5427; loss: 0.693570613861084\n",
            "Iteration: 5428; loss: 0.691769540309906\n",
            "Iteration: 5429; loss: 0.6933190822601318\n",
            "Iteration: 5430; loss: 0.6943724751472473\n",
            "Iteration: 5431; loss: 0.6931197643280029\n",
            "Iteration: 5432; loss: 0.6921957731246948\n",
            "Iteration: 5433; loss: 0.691864550113678\n",
            "Iteration: 5434; loss: 0.6969778537750244\n",
            "Iteration: 5435; loss: 0.6931968927383423\n",
            "Iteration: 5436; loss: 0.693967342376709\n",
            "Iteration: 5437; loss: 0.6920011043548584\n",
            "Iteration: 5438; loss: 0.6912693977355957\n",
            "Iteration: 5439; loss: 0.6910478472709656\n",
            "Iteration: 5440; loss: 0.6942375898361206\n",
            "Iteration: 5441; loss: 0.6947635412216187\n",
            "Iteration: 5442; loss: 0.6928274035453796\n",
            "Iteration: 5443; loss: 0.6926028728485107\n",
            "Iteration: 5444; loss: 0.6936546564102173\n",
            "Iteration: 5445; loss: 0.6930830478668213\n",
            "Iteration: 5446; loss: 0.6933236122131348\n",
            "Iteration: 5447; loss: 0.6916295886039734\n",
            "Iteration: 5448; loss: 0.6891520619392395\n",
            "Iteration: 5449; loss: 0.693947970867157\n",
            "Iteration: 5450; loss: 0.6921921372413635\n",
            "Iteration: 5451; loss: 0.6929463148117065\n",
            "Iteration: 5452; loss: 0.6932138204574585\n",
            "Iteration: 5453; loss: 0.691704511642456\n",
            "Iteration: 5454; loss: 0.6900433897972107\n",
            "Iteration: 5455; loss: 0.6914941072463989\n",
            "Iteration: 5456; loss: 0.6906713247299194\n",
            "Iteration: 5457; loss: 0.693882942199707\n",
            "Iteration: 5458; loss: 0.692461371421814\n",
            "Iteration: 5459; loss: 0.6928151845932007\n",
            "Iteration: 5460; loss: 0.6926109790802002\n",
            "Iteration: 5461; loss: 0.6928079128265381\n",
            "Iteration: 5462; loss: 0.6902340054512024\n",
            "Iteration: 5463; loss: 0.6937127113342285\n",
            "Iteration: 5464; loss: 0.6963881254196167\n",
            "Iteration: 5465; loss: 0.6929507851600647\n",
            "Iteration: 5466; loss: 0.6923968195915222\n",
            "Iteration: 5467; loss: 0.6905515193939209\n",
            "Iteration: 5468; loss: 0.6876538991928101\n",
            "Iteration: 5469; loss: 0.6931641101837158\n",
            "Iteration: 5470; loss: 0.691558301448822\n",
            "Iteration: 5471; loss: 0.6901203989982605\n",
            "Iteration: 5472; loss: 0.690325140953064\n",
            "Iteration: 5473; loss: 0.6952970623970032\n",
            "Iteration: 5474; loss: 0.6953558921813965\n",
            "Iteration: 5475; loss: 0.6973864436149597\n",
            "Iteration: 5476; loss: 0.69371497631073\n",
            "Iteration: 5477; loss: 0.6926100850105286\n",
            "Iteration: 5478; loss: 0.6947536468505859\n",
            "Iteration: 5479; loss: 0.6957518458366394\n",
            "Iteration: 5480; loss: 0.6913701891899109\n",
            "Iteration: 5481; loss: 0.6921725273132324\n",
            "Iteration: 5482; loss: 0.691455066204071\n",
            "Iteration: 5483; loss: 0.6916413903236389\n",
            "Iteration: 5484; loss: 0.6947762370109558\n",
            "Iteration: 5485; loss: 0.6921869516372681\n",
            "Iteration: 5486; loss: 0.6937056183815002\n",
            "Iteration: 5487; loss: 0.6908486485481262\n",
            "Iteration: 5488; loss: 0.6931326389312744\n",
            "Iteration: 5489; loss: 0.690865159034729\n",
            "Iteration: 5490; loss: 0.6937917470932007\n",
            "Iteration: 5491; loss: 0.6950754523277283\n",
            "Iteration: 5492; loss: 0.6934271454811096\n",
            "Iteration: 5493; loss: 0.6962735652923584\n",
            "Iteration: 5494; loss: 0.6913423538208008\n",
            "Iteration: 5495; loss: 0.69533771276474\n",
            "Iteration: 5496; loss: 0.6922329068183899\n",
            "Iteration: 5497; loss: 0.6936253905296326\n",
            "Iteration: 5498; loss: 0.695135235786438\n",
            "Iteration: 5499; loss: 0.6901848316192627\n",
            "Iteration: 5500; loss: 0.6938377022743225\n",
            "Iteration: 5501; loss: 0.6932705044746399\n",
            "Iteration: 5502; loss: 0.6920497417449951\n",
            "Iteration: 5503; loss: 0.6927235722541809\n",
            "Iteration: 5504; loss: 0.6985024809837341\n",
            "Iteration: 5505; loss: 0.6894682049751282\n",
            "Iteration: 5506; loss: 0.6914377808570862\n",
            "Iteration: 5507; loss: 0.6913750171661377\n",
            "Iteration: 5508; loss: 0.6914083361625671\n",
            "Iteration: 5509; loss: 0.6952700614929199\n",
            "Iteration: 5510; loss: 0.6975701451301575\n",
            "Iteration: 5511; loss: 0.69428551197052\n",
            "Iteration: 5512; loss: 0.6933507323265076\n",
            "Iteration: 5513; loss: 0.6902393698692322\n",
            "Iteration: 5514; loss: 0.6940455436706543\n",
            "Iteration: 5515; loss: 0.6913019418716431\n",
            "Iteration: 5516; loss: 0.6968141794204712\n",
            "Iteration: 5517; loss: 0.6938307285308838\n",
            "Iteration: 5518; loss: 0.6907322406768799\n",
            "Iteration: 5519; loss: 0.6910778284072876\n",
            "Iteration: 5520; loss: 0.6902052164077759\n",
            "Iteration: 5521; loss: 0.6953938007354736\n",
            "Iteration: 5522; loss: 0.6948604583740234\n",
            "Iteration: 5523; loss: 0.6923149228096008\n",
            "Iteration: 5524; loss: 0.6928804516792297\n",
            "Iteration: 5525; loss: 0.6900327205657959\n",
            "Iteration: 5526; loss: 0.691531777381897\n",
            "Iteration: 5527; loss: 0.6929744482040405\n",
            "Iteration: 5528; loss: 0.6925791501998901\n",
            "Iteration: 5529; loss: 0.6941573619842529\n",
            "Iteration: 5530; loss: 0.6938075423240662\n",
            "Iteration: 5531; loss: 0.6941096186637878\n",
            "Iteration: 5532; loss: 0.6948826313018799\n",
            "Iteration: 5533; loss: 0.6924933195114136\n",
            "Iteration: 5534; loss: 0.698746919631958\n",
            "Iteration: 5535; loss: 0.6951202154159546\n",
            "Iteration: 5536; loss: 0.6920408010482788\n",
            "Iteration: 5537; loss: 0.6927039623260498\n",
            "Iteration: 5538; loss: 0.6921831369400024\n",
            "Iteration: 5539; loss: 0.6930692791938782\n",
            "Iteration: 5540; loss: 0.6930726766586304\n",
            "Iteration: 5541; loss: 0.697791337966919\n",
            "Iteration: 5542; loss: 0.6942242383956909\n",
            "Iteration: 5543; loss: 0.6964979767799377\n",
            "Iteration: 5544; loss: 0.6917157173156738\n",
            "Iteration: 5545; loss: 0.6955999732017517\n",
            "Iteration: 5546; loss: 0.69340580701828\n",
            "Iteration: 5547; loss: 0.6897005438804626\n",
            "Iteration: 5548; loss: 0.691284716129303\n",
            "Iteration: 5549; loss: 0.6936541795730591\n",
            "Iteration: 5550; loss: 0.6949439644813538\n",
            "Iteration: 5551; loss: 0.6930548548698425\n",
            "Iteration: 5552; loss: 0.6964454650878906\n",
            "Iteration: 5553; loss: 0.6902065873146057\n",
            "Iteration: 5554; loss: 0.6925073266029358\n",
            "Iteration: 5555; loss: 0.6914690732955933\n",
            "Iteration: 5556; loss: 0.69382643699646\n",
            "Iteration: 5557; loss: 0.6936639547348022\n",
            "Iteration: 5558; loss: 0.6942118406295776\n",
            "Iteration: 5559; loss: 0.6933400630950928\n",
            "Iteration: 5560; loss: 0.6916397213935852\n",
            "Iteration: 5561; loss: 0.6904672384262085\n",
            "Iteration: 5562; loss: 0.6925015449523926\n",
            "Iteration: 5563; loss: 0.691020667552948\n",
            "Iteration: 5564; loss: 0.6934434771537781\n",
            "Iteration: 5565; loss: 0.688137948513031\n",
            "Iteration: 5566; loss: 0.6962386965751648\n",
            "Iteration: 5567; loss: 0.6903722286224365\n",
            "Iteration: 5568; loss: 0.6941133141517639\n",
            "Iteration: 5569; loss: 0.6951417922973633\n",
            "Iteration: 5570; loss: 0.6937640905380249\n",
            "Iteration: 5571; loss: 0.6949288845062256\n",
            "Iteration: 5572; loss: 0.6955344676971436\n",
            "Iteration: 5573; loss: 0.6927868723869324\n",
            "Iteration: 5574; loss: 0.6927422285079956\n",
            "Iteration: 5575; loss: 0.6904790997505188\n",
            "Iteration: 5576; loss: 0.6950365900993347\n",
            "Iteration: 5577; loss: 0.6952897310256958\n",
            "Iteration: 5578; loss: 0.6943190097808838\n",
            "Iteration: 5579; loss: 0.6933121085166931\n",
            "Iteration: 5580; loss: 0.6902357935905457\n",
            "Iteration: 5581; loss: 0.693288266658783\n",
            "Iteration: 5582; loss: 0.6927497982978821\n",
            "Iteration: 5583; loss: 0.6943808197975159\n",
            "Iteration: 5584; loss: 0.6927319169044495\n",
            "Iteration: 5585; loss: 0.6962035894393921\n",
            "Iteration: 5586; loss: 0.6949620246887207\n",
            "Iteration: 5587; loss: 0.6977843642234802\n",
            "Iteration: 5588; loss: 0.6937039494514465\n",
            "Iteration: 5589; loss: 0.6941359043121338\n",
            "Iteration: 5590; loss: 0.6936138272285461\n",
            "Iteration: 5591; loss: 0.6920093297958374\n",
            "Iteration: 5592; loss: 0.6947078704833984\n",
            "Iteration: 5593; loss: 0.6901021003723145\n",
            "Iteration: 5594; loss: 0.6927223205566406\n",
            "Iteration: 5595; loss: 0.6953349113464355\n",
            "Iteration: 5596; loss: 0.690119743347168\n",
            "Iteration: 5597; loss: 0.6940755844116211\n",
            "Iteration: 5598; loss: 0.6958063244819641\n",
            "Iteration: 5599; loss: 0.6935173273086548\n",
            "Iteration: 5600; loss: 0.6936686635017395\n",
            "Iteration: 5601; loss: 0.6947510838508606\n",
            "Iteration: 5602; loss: 0.6943647265434265\n",
            "Iteration: 5603; loss: 0.6919268369674683\n",
            "Iteration: 5604; loss: 0.6932851076126099\n",
            "Iteration: 5605; loss: 0.6946625113487244\n",
            "Iteration: 5606; loss: 0.6925680637359619\n",
            "Iteration: 5607; loss: 0.6902944445610046\n",
            "Iteration: 5608; loss: 0.6926156878471375\n",
            "Iteration: 5609; loss: 0.6966183185577393\n",
            "Iteration: 5610; loss: 0.6931310892105103\n",
            "Iteration: 5611; loss: 0.689923882484436\n",
            "Iteration: 5612; loss: 0.6924583315849304\n",
            "Iteration: 5613; loss: 0.6929037570953369\n",
            "Iteration: 5614; loss: 0.6940439939498901\n",
            "Iteration: 5615; loss: 0.6927652955055237\n",
            "Iteration: 5616; loss: 0.6936414241790771\n",
            "Iteration: 5617; loss: 0.6952399015426636\n",
            "Iteration: 5618; loss: 0.6923705339431763\n",
            "Iteration: 5619; loss: 0.6918540000915527\n",
            "Iteration: 5620; loss: 0.6942462921142578\n",
            "Iteration: 5621; loss: 0.695215106010437\n",
            "Iteration: 5622; loss: 0.6922870874404907\n",
            "Iteration: 5623; loss: 0.6897899508476257\n",
            "Iteration: 5624; loss: 0.6918975114822388\n",
            "Iteration: 5625; loss: 0.6960886716842651\n",
            "Iteration: 5626; loss: 0.6923437118530273\n",
            "Iteration: 5627; loss: 0.6940497159957886\n",
            "Iteration: 5628; loss: 0.6918261051177979\n",
            "Iteration: 5629; loss: 0.6942833065986633\n",
            "Iteration: 5630; loss: 0.6915926933288574\n",
            "Iteration: 5631; loss: 0.6927454471588135\n",
            "Iteration: 5632; loss: 0.6929636001586914\n",
            "Iteration: 5633; loss: 0.691074788570404\n",
            "Iteration: 5634; loss: 0.6937257647514343\n",
            "Iteration: 5635; loss: 0.6940727829933167\n",
            "Iteration: 5636; loss: 0.6923422813415527\n",
            "Iteration: 5637; loss: 0.6932528018951416\n",
            "Iteration: 5638; loss: 0.6916049718856812\n",
            "Iteration: 5639; loss: 0.6957567930221558\n",
            "Iteration: 5640; loss: 0.6941297054290771\n",
            "Iteration: 5641; loss: 0.6906117796897888\n",
            "Iteration: 5642; loss: 0.6931987404823303\n",
            "Iteration: 5643; loss: 0.6940828561782837\n",
            "Iteration: 5644; loss: 0.6945502758026123\n",
            "Iteration: 5645; loss: 0.6933186054229736\n",
            "Iteration: 5646; loss: 0.6933883428573608\n",
            "Iteration: 5647; loss: 0.6938493251800537\n",
            "Iteration: 5648; loss: 0.6918615102767944\n",
            "Iteration: 5649; loss: 0.6927445530891418\n",
            "Iteration: 5650; loss: 0.6941871047019958\n",
            "Iteration: 5651; loss: 0.6916441321372986\n",
            "Iteration: 5652; loss: 0.6925892233848572\n",
            "Iteration: 5653; loss: 0.6907047033309937\n",
            "Iteration: 5654; loss: 0.6921533346176147\n",
            "Iteration: 5655; loss: 0.6929320693016052\n",
            "Iteration: 5656; loss: 0.6959658265113831\n",
            "Iteration: 5657; loss: 0.6922136545181274\n",
            "Iteration: 5658; loss: 0.6925123929977417\n",
            "Iteration: 5659; loss: 0.6953732967376709\n",
            "Iteration: 5660; loss: 0.6912755966186523\n",
            "Iteration: 5661; loss: 0.6946736574172974\n",
            "Iteration: 5662; loss: 0.694014847278595\n",
            "Iteration: 5663; loss: 0.6933138370513916\n",
            "Iteration: 5664; loss: 0.6917818188667297\n",
            "Iteration: 5665; loss: 0.6892638802528381\n",
            "Iteration: 5666; loss: 0.6919417977333069\n",
            "Iteration: 5667; loss: 0.69185471534729\n",
            "Iteration: 5668; loss: 0.6915368437767029\n",
            "Iteration: 5669; loss: 0.6922807097434998\n",
            "Iteration: 5670; loss: 0.6940666437149048\n",
            "Iteration: 5671; loss: 0.6906834840774536\n",
            "Iteration: 5672; loss: 0.6963562965393066\n",
            "Iteration: 5673; loss: 0.694042980670929\n",
            "Iteration: 5674; loss: 0.6955769062042236\n",
            "Iteration: 5675; loss: 0.6929892897605896\n",
            "Iteration: 5676; loss: 0.6952582597732544\n",
            "Iteration: 5677; loss: 0.6957947611808777\n",
            "Iteration: 5678; loss: 0.692536473274231\n",
            "Iteration: 5679; loss: 0.6928814053535461\n",
            "Iteration: 5680; loss: 0.694172203540802\n",
            "Iteration: 5681; loss: 0.6958975791931152\n",
            "Iteration: 5682; loss: 0.6931276917457581\n",
            "Iteration: 5683; loss: 0.69292151927948\n",
            "Iteration: 5684; loss: 0.6917238831520081\n",
            "Iteration: 5685; loss: 0.691277801990509\n",
            "Iteration: 5686; loss: 0.6916782855987549\n",
            "Iteration: 5687; loss: 0.6934947371482849\n",
            "Iteration: 5688; loss: 0.6916394829750061\n",
            "Iteration: 5689; loss: 0.6947421431541443\n",
            "Iteration: 5690; loss: 0.6900225281715393\n",
            "Iteration: 5691; loss: 0.6921692490577698\n",
            "Iteration: 5692; loss: 0.6922047138214111\n",
            "Iteration: 5693; loss: 0.6910979747772217\n",
            "Iteration: 5694; loss: 0.6923152208328247\n",
            "Iteration: 5695; loss: 0.6965541243553162\n",
            "Iteration: 5696; loss: 0.6915751695632935\n",
            "Iteration: 5697; loss: 0.6939912438392639\n",
            "Iteration: 5698; loss: 0.6903848648071289\n",
            "Iteration: 5699; loss: 0.694108247756958\n",
            "Iteration: 5700; loss: 0.6963956356048584\n",
            "Iteration: 5701; loss: 0.6890453100204468\n",
            "Iteration: 5702; loss: 0.6949527859687805\n",
            "Iteration: 5703; loss: 0.6902682185173035\n",
            "Iteration: 5704; loss: 0.6923923492431641\n",
            "Iteration: 5705; loss: 0.6960942149162292\n",
            "Iteration: 5706; loss: 0.6919558644294739\n",
            "Iteration: 5707; loss: 0.6912655234336853\n",
            "Iteration: 5708; loss: 0.695332407951355\n",
            "Iteration: 5709; loss: 0.6922820210456848\n",
            "Iteration: 5710; loss: 0.6951770186424255\n",
            "Iteration: 5711; loss: 0.6911776065826416\n",
            "Iteration: 5712; loss: 0.6886523962020874\n",
            "Iteration: 5713; loss: 0.6953943371772766\n",
            "Iteration: 5714; loss: 0.6929566860198975\n",
            "Iteration: 5715; loss: 0.6936720013618469\n",
            "Iteration: 5716; loss: 0.6874822378158569\n",
            "Iteration: 5717; loss: 0.6916099786758423\n",
            "Iteration: 5718; loss: 0.694707989692688\n",
            "Iteration: 5719; loss: 0.6898031234741211\n",
            "Iteration: 5720; loss: 0.6917330622673035\n",
            "Iteration: 5721; loss: 0.693846583366394\n",
            "Iteration: 5722; loss: 0.6938750147819519\n",
            "Iteration: 5723; loss: 0.6923986077308655\n",
            "Iteration: 5724; loss: 0.6958593726158142\n",
            "Iteration: 5725; loss: 0.6916068196296692\n",
            "Iteration: 5726; loss: 0.6958032250404358\n",
            "Iteration: 5727; loss: 0.6949787139892578\n",
            "Iteration: 5728; loss: 0.6944252252578735\n",
            "Iteration: 5729; loss: 0.6964247822761536\n",
            "Iteration: 5730; loss: 0.6954724192619324\n",
            "Iteration: 5731; loss: 0.6943779587745667\n",
            "Iteration: 5732; loss: 0.6953645348548889\n",
            "Iteration: 5733; loss: 0.6922034025192261\n",
            "Iteration: 5734; loss: 0.6918943524360657\n",
            "Iteration: 5735; loss: 0.693267822265625\n",
            "Iteration: 5736; loss: 0.6927046775817871\n",
            "Iteration: 5737; loss: 0.6951858997344971\n",
            "Iteration: 5738; loss: 0.6943539977073669\n",
            "Iteration: 5739; loss: 0.6949390172958374\n",
            "Iteration: 5740; loss: 0.6922711730003357\n",
            "Iteration: 5741; loss: 0.6915677785873413\n",
            "Iteration: 5742; loss: 0.6925556063652039\n",
            "Iteration: 5743; loss: 0.694746196269989\n",
            "Iteration: 5744; loss: 0.692671537399292\n",
            "Iteration: 5745; loss: 0.6933392286300659\n",
            "Iteration: 5746; loss: 0.6911895871162415\n",
            "Iteration: 5747; loss: 0.6939364671707153\n",
            "Iteration: 5748; loss: 0.6948061585426331\n",
            "Iteration: 5749; loss: 0.6930177211761475\n",
            "Iteration: 5750; loss: 0.6946170926094055\n",
            "Iteration: 5751; loss: 0.6954930424690247\n",
            "Iteration: 5752; loss: 0.6905059814453125\n",
            "Iteration: 5753; loss: 0.6952617764472961\n",
            "Iteration: 5754; loss: 0.6914680004119873\n",
            "Iteration: 5755; loss: 0.6917788982391357\n",
            "Iteration: 5756; loss: 0.6943004727363586\n",
            "Iteration: 5757; loss: 0.6933243870735168\n",
            "Iteration: 5758; loss: 0.6949880123138428\n",
            "Iteration: 5759; loss: 0.6958327293395996\n",
            "Iteration: 5760; loss: 0.6925203800201416\n",
            "Iteration: 5761; loss: 0.6939343810081482\n",
            "Iteration: 5762; loss: 0.6938386559486389\n",
            "Iteration: 5763; loss: 0.6944630146026611\n",
            "Iteration: 5764; loss: 0.6926695704460144\n",
            "Iteration: 5765; loss: 0.693241536617279\n",
            "Iteration: 5766; loss: 0.6974582672119141\n",
            "Iteration: 5767; loss: 0.6912387609481812\n",
            "Iteration: 5768; loss: 0.6906717419624329\n",
            "Iteration: 5769; loss: 0.692924976348877\n",
            "Iteration: 5770; loss: 0.6955836415290833\n",
            "Iteration: 5771; loss: 0.6923669576644897\n",
            "Iteration: 5772; loss: 0.6920565366744995\n",
            "Iteration: 5773; loss: 0.6936341524124146\n",
            "Iteration: 5774; loss: 0.6939455270767212\n",
            "Iteration: 5775; loss: 0.6927950382232666\n",
            "Iteration: 5776; loss: 0.6923713684082031\n",
            "Iteration: 5777; loss: 0.6926998496055603\n",
            "Iteration: 5778; loss: 0.693469226360321\n",
            "Iteration: 5779; loss: 0.6930465698242188\n",
            "Iteration: 5780; loss: 0.6934720277786255\n",
            "Iteration: 5781; loss: 0.6907339692115784\n",
            "Iteration: 5782; loss: 0.691464364528656\n",
            "Iteration: 5783; loss: 0.6925559043884277\n",
            "Iteration: 5784; loss: 0.6956043839454651\n",
            "Iteration: 5785; loss: 0.6915398836135864\n",
            "Iteration: 5786; loss: 0.6924898028373718\n",
            "Iteration: 5787; loss: 0.6941206455230713\n",
            "Iteration: 5788; loss: 0.6907703280448914\n",
            "Iteration: 5789; loss: 0.6951044797897339\n",
            "Iteration: 5790; loss: 0.690348744392395\n",
            "Iteration: 5791; loss: 0.6949888467788696\n",
            "Iteration: 5792; loss: 0.692570686340332\n",
            "Iteration: 5793; loss: 0.6922270059585571\n",
            "Iteration: 5794; loss: 0.6944890022277832\n",
            "Iteration: 5795; loss: 0.6944345831871033\n",
            "Iteration: 5796; loss: 0.6905667781829834\n",
            "Iteration: 5797; loss: 0.6928604245185852\n",
            "Iteration: 5798; loss: 0.6918565630912781\n",
            "Iteration: 5799; loss: 0.6926192045211792\n",
            "Iteration: 5800; loss: 0.695810079574585\n",
            "Iteration: 5801; loss: 0.6925598382949829\n",
            "Iteration: 5802; loss: 0.6918875575065613\n",
            "Iteration: 5803; loss: 0.6936656832695007\n",
            "Iteration: 5804; loss: 0.693588137626648\n",
            "Iteration: 5805; loss: 0.6928914189338684\n",
            "Iteration: 5806; loss: 0.6965294480323792\n",
            "Iteration: 5807; loss: 0.692265510559082\n",
            "Iteration: 5808; loss: 0.6943471431732178\n",
            "Iteration: 5809; loss: 0.6913647651672363\n",
            "Iteration: 5810; loss: 0.6974614858627319\n",
            "Iteration: 5811; loss: 0.6893365383148193\n",
            "Iteration: 5812; loss: 0.6911882162094116\n",
            "Iteration: 5813; loss: 0.6925657987594604\n",
            "Iteration: 5814; loss: 0.6920217871665955\n",
            "Iteration: 5815; loss: 0.6936134099960327\n",
            "Iteration: 5816; loss: 0.6926856637001038\n",
            "Iteration: 5817; loss: 0.6920855641365051\n",
            "Iteration: 5818; loss: 0.6936190128326416\n",
            "Iteration: 5819; loss: 0.6933539509773254\n",
            "Iteration: 5820; loss: 0.6934754252433777\n",
            "Iteration: 5821; loss: 0.6963738203048706\n",
            "Iteration: 5822; loss: 0.6938204169273376\n",
            "Iteration: 5823; loss: 0.6889537572860718\n",
            "Iteration: 5824; loss: 0.691785991191864\n",
            "Iteration: 5825; loss: 0.6924641132354736\n",
            "Iteration: 5826; loss: 0.693518877029419\n",
            "Iteration: 5827; loss: 0.6909172534942627\n",
            "Iteration: 5828; loss: 0.6920600533485413\n",
            "Iteration: 5829; loss: 0.6936760544776917\n",
            "Iteration: 5830; loss: 0.6952685117721558\n",
            "Iteration: 5831; loss: 0.6948530673980713\n",
            "Iteration: 5832; loss: 0.6881905198097229\n",
            "Iteration: 5833; loss: 0.6959840059280396\n",
            "Iteration: 5834; loss: 0.6893934011459351\n",
            "Iteration: 5835; loss: 0.695483922958374\n",
            "Iteration: 5836; loss: 0.6931524276733398\n",
            "Iteration: 5837; loss: 0.6945446729660034\n",
            "Iteration: 5838; loss: 0.6945278644561768\n",
            "Iteration: 5839; loss: 0.6914839148521423\n",
            "Iteration: 5840; loss: 0.6942077875137329\n",
            "Iteration: 5841; loss: 0.6954532265663147\n",
            "Iteration: 5842; loss: 0.6916223764419556\n",
            "Iteration: 5843; loss: 0.6901997327804565\n",
            "Iteration: 5844; loss: 0.692408561706543\n",
            "Iteration: 5845; loss: 0.6909565925598145\n",
            "Iteration: 5846; loss: 0.6938429474830627\n",
            "Iteration: 5847; loss: 0.6949153542518616\n",
            "Iteration: 5848; loss: 0.6897639036178589\n",
            "Iteration: 5849; loss: 0.6894080638885498\n",
            "Iteration: 5850; loss: 0.694341778755188\n",
            "Iteration: 5851; loss: 0.696943998336792\n",
            "Iteration: 5852; loss: 0.6943684816360474\n",
            "Iteration: 5853; loss: 0.6937190294265747\n",
            "Iteration: 5854; loss: 0.6951383352279663\n",
            "Iteration: 5855; loss: 0.6932536363601685\n",
            "Iteration: 5856; loss: 0.6945774555206299\n",
            "Iteration: 5857; loss: 0.69367915391922\n",
            "Iteration: 5858; loss: 0.6925300359725952\n",
            "Iteration: 5859; loss: 0.6913864612579346\n",
            "Iteration: 5860; loss: 0.6921755075454712\n",
            "Iteration: 5861; loss: 0.6942789554595947\n",
            "Iteration: 5862; loss: 0.6953514814376831\n",
            "Iteration: 5863; loss: 0.6912031769752502\n",
            "Iteration: 5864; loss: 0.694579005241394\n",
            "Iteration: 5865; loss: 0.6904305219650269\n",
            "Iteration: 5866; loss: 0.6941422820091248\n",
            "Iteration: 5867; loss: 0.6911574006080627\n",
            "Iteration: 5868; loss: 0.6930016279220581\n",
            "Iteration: 5869; loss: 0.6920150518417358\n",
            "Iteration: 5870; loss: 0.6939965486526489\n",
            "Iteration: 5871; loss: 0.6960921287536621\n",
            "Iteration: 5872; loss: 0.6941131353378296\n",
            "Iteration: 5873; loss: 0.6929107308387756\n",
            "Iteration: 5874; loss: 0.6940592527389526\n",
            "Iteration: 5875; loss: 0.6923562288284302\n",
            "Iteration: 5876; loss: 0.6901705265045166\n",
            "Iteration: 5877; loss: 0.6967918872833252\n",
            "Iteration: 5878; loss: 0.6907352209091187\n",
            "Iteration: 5879; loss: 0.694057822227478\n",
            "Iteration: 5880; loss: 0.6911208033561707\n",
            "Iteration: 5881; loss: 0.6969541311264038\n",
            "Iteration: 5882; loss: 0.6910356283187866\n",
            "Iteration: 5883; loss: 0.693242073059082\n",
            "Iteration: 5884; loss: 0.6908381581306458\n",
            "Iteration: 5885; loss: 0.6908633708953857\n",
            "Iteration: 5886; loss: 0.6973952651023865\n",
            "Iteration: 5887; loss: 0.6910169124603271\n",
            "Iteration: 5888; loss: 0.6960283517837524\n",
            "Iteration: 5889; loss: 0.6942873001098633\n",
            "Iteration: 5890; loss: 0.694158673286438\n",
            "Iteration: 5891; loss: 0.6895123720169067\n",
            "Iteration: 5892; loss: 0.6946218609809875\n",
            "Iteration: 5893; loss: 0.6930988430976868\n",
            "Iteration: 5894; loss: 0.6938460469245911\n",
            "Iteration: 5895; loss: 0.6939063668251038\n",
            "Iteration: 5896; loss: 0.6908662915229797\n",
            "Iteration: 5897; loss: 0.6945288181304932\n",
            "Iteration: 5898; loss: 0.6913629174232483\n",
            "Iteration: 5899; loss: 0.6929253339767456\n",
            "Iteration: 5900; loss: 0.6930365562438965\n",
            "Iteration: 5901; loss: 0.6918386220932007\n",
            "Iteration: 5902; loss: 0.6894471645355225\n",
            "Iteration: 5903; loss: 0.6928424835205078\n",
            "Iteration: 5904; loss: 0.6937917470932007\n",
            "Iteration: 5905; loss: 0.6965116858482361\n",
            "Iteration: 5906; loss: 0.691294252872467\n",
            "Iteration: 5907; loss: 0.6932381391525269\n",
            "Iteration: 5908; loss: 0.6929296851158142\n",
            "Iteration: 5909; loss: 0.6908769607543945\n",
            "Iteration: 5910; loss: 0.6911689043045044\n",
            "Iteration: 5911; loss: 0.6955816149711609\n",
            "Iteration: 5912; loss: 0.6937351822853088\n",
            "Iteration: 5913; loss: 0.6926831007003784\n",
            "Iteration: 5914; loss: 0.6957706212997437\n",
            "Iteration: 5915; loss: 0.6914092302322388\n",
            "Iteration: 5916; loss: 0.6965768337249756\n",
            "Iteration: 5917; loss: 0.6935397982597351\n",
            "Iteration: 5918; loss: 0.6931236982345581\n",
            "Iteration: 5919; loss: 0.6932629942893982\n",
            "Iteration: 5920; loss: 0.6937373280525208\n",
            "Iteration: 5921; loss: 0.6938455104827881\n",
            "Iteration: 5922; loss: 0.6950699090957642\n",
            "Iteration: 5923; loss: 0.6925138235092163\n",
            "Iteration: 5924; loss: 0.6931499242782593\n",
            "Iteration: 5925; loss: 0.688367486000061\n",
            "Iteration: 5926; loss: 0.6897833943367004\n",
            "Iteration: 5927; loss: 0.695716142654419\n",
            "Iteration: 5928; loss: 0.6927618980407715\n",
            "Iteration: 5929; loss: 0.6904873251914978\n",
            "Iteration: 5930; loss: 0.692472517490387\n",
            "Iteration: 5931; loss: 0.6935306787490845\n",
            "Iteration: 5932; loss: 0.6925754547119141\n",
            "Iteration: 5933; loss: 0.6919955611228943\n",
            "Iteration: 5934; loss: 0.6913686990737915\n",
            "Iteration: 5935; loss: 0.6944369077682495\n",
            "Iteration: 5936; loss: 0.692253053188324\n",
            "Iteration: 5937; loss: 0.6942139267921448\n",
            "Iteration: 5938; loss: 0.6937485337257385\n",
            "Iteration: 5939; loss: 0.6911625862121582\n",
            "Iteration: 5940; loss: 0.6879910826683044\n",
            "Iteration: 5941; loss: 0.6917696595191956\n",
            "Iteration: 5942; loss: 0.6947866082191467\n",
            "Iteration: 5943; loss: 0.6947787404060364\n",
            "Iteration: 5944; loss: 0.6932004690170288\n",
            "Iteration: 5945; loss: 0.692990243434906\n",
            "Iteration: 5946; loss: 0.6919312477111816\n",
            "Iteration: 5947; loss: 0.6935850977897644\n",
            "Iteration: 5948; loss: 0.6937918663024902\n",
            "Iteration: 5949; loss: 0.6940032243728638\n",
            "Iteration: 5950; loss: 0.6945109367370605\n",
            "Iteration: 5951; loss: 0.6942312717437744\n",
            "Iteration: 5952; loss: 0.6933258771896362\n",
            "Iteration: 5953; loss: 0.6932098269462585\n",
            "Iteration: 5954; loss: 0.6920084953308105\n",
            "Iteration: 5955; loss: 0.6956474781036377\n",
            "Iteration: 5956; loss: 0.694023847579956\n",
            "Iteration: 5957; loss: 0.6913950443267822\n",
            "Iteration: 5958; loss: 0.692392885684967\n",
            "Iteration: 5959; loss: 0.6930727362632751\n",
            "Iteration: 5960; loss: 0.6905869841575623\n",
            "Iteration: 5961; loss: 0.6935854554176331\n",
            "Iteration: 5962; loss: 0.693752646446228\n",
            "Iteration: 5963; loss: 0.6927710175514221\n",
            "Iteration: 5964; loss: 0.6919338703155518\n",
            "Iteration: 5965; loss: 0.6951148509979248\n",
            "Iteration: 5966; loss: 0.6968994736671448\n",
            "Iteration: 5967; loss: 0.6930269002914429\n",
            "Iteration: 5968; loss: 0.6935189962387085\n",
            "Iteration: 5969; loss: 0.6952250003814697\n",
            "Iteration: 5970; loss: 0.6881335377693176\n",
            "Iteration: 5971; loss: 0.6929210424423218\n",
            "Iteration: 5972; loss: 0.6940836310386658\n",
            "Iteration: 5973; loss: 0.69242262840271\n",
            "Iteration: 5974; loss: 0.6884902715682983\n",
            "Iteration: 5975; loss: 0.6953896880149841\n",
            "Iteration: 5976; loss: 0.6913447976112366\n",
            "Iteration: 5977; loss: 0.6926373839378357\n",
            "Iteration: 5978; loss: 0.6921569108963013\n",
            "Iteration: 5979; loss: 0.6946999430656433\n",
            "Iteration: 5980; loss: 0.6925228238105774\n",
            "Iteration: 5981; loss: 0.6965749263763428\n",
            "Iteration: 5982; loss: 0.6913485527038574\n",
            "Iteration: 5983; loss: 0.6912559270858765\n",
            "Iteration: 5984; loss: 0.6914838552474976\n",
            "Iteration: 5985; loss: 0.691264808177948\n",
            "Iteration: 5986; loss: 0.6935241222381592\n",
            "Iteration: 5987; loss: 0.695270299911499\n",
            "Iteration: 5988; loss: 0.6913763284683228\n",
            "Iteration: 5989; loss: 0.69573575258255\n",
            "Iteration: 5990; loss: 0.6928021907806396\n",
            "Iteration: 5991; loss: 0.6904761791229248\n",
            "Iteration: 5992; loss: 0.6920701861381531\n",
            "Iteration: 5993; loss: 0.693152666091919\n",
            "Iteration: 5994; loss: 0.6932199001312256\n",
            "Iteration: 5995; loss: 0.6933320760726929\n",
            "Iteration: 5996; loss: 0.6910262107849121\n",
            "Iteration: 5997; loss: 0.6929875612258911\n",
            "Iteration: 5998; loss: 0.6973828673362732\n",
            "Iteration: 5999; loss: 0.6951069235801697\n",
            "Iteration: 6000; loss: 0.6915374398231506\n",
            "Iteration: 6001; loss: 0.6950104832649231\n",
            "Iteration: 6002; loss: 0.6943404078483582\n",
            "Iteration: 6003; loss: 0.6928150057792664\n",
            "Iteration: 6004; loss: 0.695475697517395\n",
            "Iteration: 6005; loss: 0.6914973258972168\n",
            "Iteration: 6006; loss: 0.6927633285522461\n",
            "Iteration: 6007; loss: 0.6917198300361633\n",
            "Iteration: 6008; loss: 0.6947102546691895\n",
            "Iteration: 6009; loss: 0.6923282146453857\n",
            "Iteration: 6010; loss: 0.6992537379264832\n",
            "Iteration: 6011; loss: 0.6916264295578003\n",
            "Iteration: 6012; loss: 0.6900979280471802\n",
            "Iteration: 6013; loss: 0.6918080449104309\n",
            "Iteration: 6014; loss: 0.694334089756012\n",
            "Iteration: 6015; loss: 0.6914350390434265\n",
            "Iteration: 6016; loss: 0.6889116764068604\n",
            "Iteration: 6017; loss: 0.6931517124176025\n",
            "Iteration: 6018; loss: 0.692365288734436\n",
            "Iteration: 6019; loss: 0.6964985132217407\n",
            "Iteration: 6020; loss: 0.6945564150810242\n",
            "Iteration: 6021; loss: 0.6923096776008606\n",
            "Iteration: 6022; loss: 0.6923782825469971\n",
            "Iteration: 6023; loss: 0.6940914392471313\n",
            "Iteration: 6024; loss: 0.6938230991363525\n",
            "Iteration: 6025; loss: 0.6947466135025024\n",
            "Iteration: 6026; loss: 0.6953299641609192\n",
            "Iteration: 6027; loss: 0.6916760206222534\n",
            "Iteration: 6028; loss: 0.6944482326507568\n",
            "Iteration: 6029; loss: 0.6932562589645386\n",
            "Iteration: 6030; loss: 0.6947695016860962\n",
            "Iteration: 6031; loss: 0.6943387985229492\n",
            "Iteration: 6032; loss: 0.6931405663490295\n",
            "Iteration: 6033; loss: 0.6934077143669128\n",
            "Iteration: 6034; loss: 0.6908758878707886\n",
            "Iteration: 6035; loss: 0.6949757933616638\n",
            "Iteration: 6036; loss: 0.6920555233955383\n",
            "Iteration: 6037; loss: 0.691766083240509\n",
            "Iteration: 6038; loss: 0.693827211856842\n",
            "Iteration: 6039; loss: 0.6945692896842957\n",
            "Iteration: 6040; loss: 0.6927249431610107\n",
            "Iteration: 6041; loss: 0.6928414106369019\n",
            "Iteration: 6042; loss: 0.6938170194625854\n",
            "Iteration: 6043; loss: 0.6937780380249023\n",
            "Iteration: 6044; loss: 0.6957366466522217\n",
            "Iteration: 6045; loss: 0.6926698684692383\n",
            "Iteration: 6046; loss: 0.6965311169624329\n",
            "Iteration: 6047; loss: 0.6949428915977478\n",
            "Iteration: 6048; loss: 0.6939258575439453\n",
            "Iteration: 6049; loss: 0.693118691444397\n",
            "Iteration: 6050; loss: 0.6906965374946594\n",
            "Iteration: 6051; loss: 0.6932136416435242\n",
            "Iteration: 6052; loss: 0.6908315420150757\n",
            "Iteration: 6053; loss: 0.6960852742195129\n",
            "Iteration: 6054; loss: 0.6895120739936829\n",
            "Iteration: 6055; loss: 0.6945145726203918\n",
            "Iteration: 6056; loss: 0.6918977499008179\n",
            "Iteration: 6057; loss: 0.6936306953430176\n",
            "Iteration: 6058; loss: 0.6926920413970947\n",
            "Iteration: 6059; loss: 0.6951664090156555\n",
            "Iteration: 6060; loss: 0.6907938718795776\n",
            "Iteration: 6061; loss: 0.6923638582229614\n",
            "Iteration: 6062; loss: 0.6918620467185974\n",
            "Iteration: 6063; loss: 0.6927993297576904\n",
            "Iteration: 6064; loss: 0.6950087547302246\n",
            "Iteration: 6065; loss: 0.6927101016044617\n",
            "Iteration: 6066; loss: 0.6934271454811096\n",
            "Iteration: 6067; loss: 0.6928516030311584\n",
            "Iteration: 6068; loss: 0.6904364228248596\n",
            "Iteration: 6069; loss: 0.6943421363830566\n",
            "Iteration: 6070; loss: 0.6913183927536011\n",
            "Iteration: 6071; loss: 0.6930912137031555\n",
            "Iteration: 6072; loss: 0.6922429203987122\n",
            "Iteration: 6073; loss: 0.6919102668762207\n",
            "Iteration: 6074; loss: 0.6924142241477966\n",
            "Iteration: 6075; loss: 0.6944831609725952\n",
            "Iteration: 6076; loss: 0.6905598640441895\n",
            "Iteration: 6077; loss: 0.6945368647575378\n",
            "Iteration: 6078; loss: 0.6905097961425781\n",
            "Iteration: 6079; loss: 0.694388210773468\n",
            "Iteration: 6080; loss: 0.6952440738677979\n",
            "Iteration: 6081; loss: 0.6919558644294739\n",
            "Iteration: 6082; loss: 0.6925738453865051\n",
            "Iteration: 6083; loss: 0.6952025294303894\n",
            "Iteration: 6084; loss: 0.6895133256912231\n",
            "Iteration: 6085; loss: 0.6929938197135925\n",
            "Iteration: 6086; loss: 0.6918858289718628\n",
            "Iteration: 6087; loss: 0.6930963397026062\n",
            "Iteration: 6088; loss: 0.6967758536338806\n",
            "Iteration: 6089; loss: 0.6901496052742004\n",
            "Iteration: 6090; loss: 0.6975583434104919\n",
            "Iteration: 6091; loss: 0.6961419582366943\n",
            "Iteration: 6092; loss: 0.6912203431129456\n",
            "Iteration: 6093; loss: 0.6904140114784241\n",
            "Iteration: 6094; loss: 0.6901471018791199\n",
            "Iteration: 6095; loss: 0.6904670000076294\n",
            "Iteration: 6096; loss: 0.6947919130325317\n",
            "Iteration: 6097; loss: 0.6938971281051636\n",
            "Iteration: 6098; loss: 0.6911513209342957\n",
            "Iteration: 6099; loss: 0.6923339366912842\n",
            "Iteration: 6100; loss: 0.6957763433456421\n",
            "Iteration: 6101; loss: 0.6935721635818481\n",
            "Iteration: 6102; loss: 0.6895129084587097\n",
            "Iteration: 6103; loss: 0.6936519742012024\n",
            "Iteration: 6104; loss: 0.693885087966919\n",
            "Iteration: 6105; loss: 0.6929595470428467\n",
            "Iteration: 6106; loss: 0.6934081315994263\n",
            "Iteration: 6107; loss: 0.6937350034713745\n",
            "Iteration: 6108; loss: 0.6933274269104004\n",
            "Iteration: 6109; loss: 0.6932097673416138\n",
            "Iteration: 6110; loss: 0.6916394233703613\n",
            "Iteration: 6111; loss: 0.6963344812393188\n",
            "Iteration: 6112; loss: 0.6936004161834717\n",
            "Iteration: 6113; loss: 0.6944974660873413\n",
            "Iteration: 6114; loss: 0.6948714852333069\n",
            "Iteration: 6115; loss: 0.6979564428329468\n",
            "Iteration: 6116; loss: 0.6948025226593018\n",
            "Iteration: 6117; loss: 0.691177487373352\n",
            "Iteration: 6118; loss: 0.6947765946388245\n",
            "Iteration: 6119; loss: 0.6944324970245361\n",
            "Iteration: 6120; loss: 0.6951895952224731\n",
            "Iteration: 6121; loss: 0.6920576095581055\n",
            "Iteration: 6122; loss: 0.6911550760269165\n",
            "Iteration: 6123; loss: 0.6911011338233948\n",
            "Iteration: 6124; loss: 0.6917730569839478\n",
            "Iteration: 6125; loss: 0.6947269439697266\n",
            "Iteration: 6126; loss: 0.6954001784324646\n",
            "Iteration: 6127; loss: 0.6909871697425842\n",
            "Iteration: 6128; loss: 0.6945421099662781\n",
            "Iteration: 6129; loss: 0.6932490468025208\n",
            "Iteration: 6130; loss: 0.6952885389328003\n",
            "Iteration: 6131; loss: 0.6936247944831848\n",
            "Iteration: 6132; loss: 0.6909298300743103\n",
            "Iteration: 6133; loss: 0.693397581577301\n",
            "Iteration: 6134; loss: 0.6952338218688965\n",
            "Iteration: 6135; loss: 0.6937915086746216\n",
            "Iteration: 6136; loss: 0.6976733207702637\n",
            "Iteration: 6137; loss: 0.6935070157051086\n",
            "Iteration: 6138; loss: 0.6900351643562317\n",
            "Iteration: 6139; loss: 0.6908436417579651\n",
            "Iteration: 6140; loss: 0.6923185586929321\n",
            "Iteration: 6141; loss: 0.6969853043556213\n",
            "Iteration: 6142; loss: 0.6972277164459229\n",
            "Iteration: 6143; loss: 0.6950123310089111\n",
            "Iteration: 6144; loss: 0.6949362754821777\n",
            "Iteration: 6145; loss: 0.6956005096435547\n",
            "Iteration: 6146; loss: 0.694091796875\n",
            "Iteration: 6147; loss: 0.6950748562812805\n",
            "Iteration: 6148; loss: 0.6926082372665405\n",
            "Iteration: 6149; loss: 0.6941533088684082\n",
            "Iteration: 6150; loss: 0.691885232925415\n",
            "Iteration: 6151; loss: 0.6946577429771423\n",
            "Iteration: 6152; loss: 0.6948893666267395\n",
            "Iteration: 6153; loss: 0.6928542852401733\n",
            "Iteration: 6154; loss: 0.6930303573608398\n",
            "Iteration: 6155; loss: 0.6949779987335205\n",
            "Iteration: 6156; loss: 0.6924607157707214\n",
            "Iteration: 6157; loss: 0.692841649055481\n",
            "Iteration: 6158; loss: 0.6932722926139832\n",
            "Iteration: 6159; loss: 0.6949662566184998\n",
            "Iteration: 6160; loss: 0.6914182901382446\n",
            "Iteration: 6161; loss: 0.6935864686965942\n",
            "Iteration: 6162; loss: 0.6924642324447632\n",
            "Iteration: 6163; loss: 0.6921103000640869\n",
            "Iteration: 6164; loss: 0.6913389563560486\n",
            "Iteration: 6165; loss: 0.6969640851020813\n",
            "Iteration: 6166; loss: 0.6934542655944824\n",
            "Iteration: 6167; loss: 0.6938127279281616\n",
            "Iteration: 6168; loss: 0.6946356296539307\n",
            "Iteration: 6169; loss: 0.6914639472961426\n",
            "Iteration: 6170; loss: 0.6929558515548706\n",
            "Iteration: 6171; loss: 0.6955463290214539\n",
            "Iteration: 6172; loss: 0.6939190626144409\n",
            "Iteration: 6173; loss: 0.6947163939476013\n",
            "Iteration: 6174; loss: 0.6920630931854248\n",
            "Iteration: 6175; loss: 0.6939709782600403\n",
            "Iteration: 6176; loss: 0.6926265358924866\n",
            "Iteration: 6177; loss: 0.6960293054580688\n",
            "Iteration: 6178; loss: 0.6923295259475708\n",
            "Iteration: 6179; loss: 0.6945511102676392\n",
            "Iteration: 6180; loss: 0.6947943568229675\n",
            "Iteration: 6181; loss: 0.693915843963623\n",
            "Iteration: 6182; loss: 0.6940147876739502\n",
            "Iteration: 6183; loss: 0.6934211254119873\n",
            "Iteration: 6184; loss: 0.6922335624694824\n",
            "Iteration: 6185; loss: 0.6956850290298462\n",
            "Iteration: 6186; loss: 0.6944483518600464\n",
            "Iteration: 6187; loss: 0.6944670677185059\n",
            "Iteration: 6188; loss: 0.6920021772384644\n",
            "Iteration: 6189; loss: 0.6923930644989014\n",
            "Iteration: 6190; loss: 0.6941372752189636\n",
            "Iteration: 6191; loss: 0.6930205821990967\n",
            "Iteration: 6192; loss: 0.6926679015159607\n",
            "Iteration: 6193; loss: 0.6947606205940247\n",
            "Iteration: 6194; loss: 0.6901564598083496\n",
            "Iteration: 6195; loss: 0.6931218504905701\n",
            "Iteration: 6196; loss: 0.6923220157623291\n",
            "Iteration: 6197; loss: 0.693664014339447\n",
            "Iteration: 6198; loss: 0.6928045153617859\n",
            "Iteration: 6199; loss: 0.6930924654006958\n",
            "Iteration: 6200; loss: 0.6932977437973022\n",
            "Iteration: 6201; loss: 0.691445529460907\n",
            "Iteration: 6202; loss: 0.6934201717376709\n",
            "Iteration: 6203; loss: 0.6908984780311584\n",
            "Iteration: 6204; loss: 0.6939278244972229\n",
            "Iteration: 6205; loss: 0.6907350420951843\n",
            "Iteration: 6206; loss: 0.6932584047317505\n",
            "Iteration: 6207; loss: 0.692727267742157\n",
            "Iteration: 6208; loss: 0.6934078335762024\n",
            "Iteration: 6209; loss: 0.6928865909576416\n",
            "Iteration: 6210; loss: 0.6922159790992737\n",
            "Iteration: 6211; loss: 0.6955366134643555\n",
            "Iteration: 6212; loss: 0.6932217478752136\n",
            "Iteration: 6213; loss: 0.692594587802887\n",
            "Iteration: 6214; loss: 0.6942620277404785\n",
            "Iteration: 6215; loss: 0.6936030983924866\n",
            "Iteration: 6216; loss: 0.6943084597587585\n",
            "Iteration: 6217; loss: 0.6938269138336182\n",
            "Iteration: 6218; loss: 0.6923596858978271\n",
            "Iteration: 6219; loss: 0.6911627054214478\n",
            "Iteration: 6220; loss: 0.6928400993347168\n",
            "Iteration: 6221; loss: 0.6946877241134644\n",
            "Iteration: 6222; loss: 0.6939957737922668\n",
            "Iteration: 6223; loss: 0.6936940550804138\n",
            "Iteration: 6224; loss: 0.6960003972053528\n",
            "Iteration: 6225; loss: 0.69529128074646\n",
            "Iteration: 6226; loss: 0.6925698518753052\n",
            "Iteration: 6227; loss: 0.6935567259788513\n",
            "Iteration: 6228; loss: 0.694241464138031\n",
            "Iteration: 6229; loss: 0.6963400840759277\n",
            "Iteration: 6230; loss: 0.6923767924308777\n",
            "Iteration: 6231; loss: 0.6915802955627441\n",
            "Iteration: 6232; loss: 0.692896842956543\n",
            "Iteration: 6233; loss: 0.6938742399215698\n",
            "Iteration: 6234; loss: 0.6924156546592712\n",
            "Iteration: 6235; loss: 0.6929609179496765\n",
            "Iteration: 6236; loss: 0.6927593946456909\n",
            "Iteration: 6237; loss: 0.6930909156799316\n",
            "Iteration: 6238; loss: 0.6928913593292236\n",
            "Iteration: 6239; loss: 0.6958901882171631\n",
            "Iteration: 6240; loss: 0.6929130554199219\n",
            "Iteration: 6241; loss: 0.6933128237724304\n",
            "Iteration: 6242; loss: 0.6936197876930237\n",
            "Iteration: 6243; loss: 0.6950091123580933\n",
            "Iteration: 6244; loss: 0.6946325302124023\n",
            "Iteration: 6245; loss: 0.6937056183815002\n",
            "Iteration: 6246; loss: 0.6931508779525757\n",
            "Iteration: 6247; loss: 0.691963791847229\n",
            "Iteration: 6248; loss: 0.6947119235992432\n",
            "Iteration: 6249; loss: 0.6933961510658264\n",
            "Iteration: 6250; loss: 0.6940078735351562\n",
            "Iteration: 6251; loss: 0.6944926381111145\n",
            "Iteration: 6252; loss: 0.6927372217178345\n",
            "Iteration: 6253; loss: 0.6922398209571838\n",
            "Iteration: 6254; loss: 0.6915588974952698\n",
            "Iteration: 6255; loss: 0.6917682886123657\n",
            "Iteration: 6256; loss: 0.6942175626754761\n",
            "Iteration: 6257; loss: 0.6928341388702393\n",
            "Iteration: 6258; loss: 0.6925440430641174\n",
            "Iteration: 6259; loss: 0.6954959034919739\n",
            "Iteration: 6260; loss: 0.6920260190963745\n",
            "Iteration: 6261; loss: 0.6952705383300781\n",
            "Iteration: 6262; loss: 0.6921563744544983\n",
            "Iteration: 6263; loss: 0.695075511932373\n",
            "Iteration: 6264; loss: 0.6930634379386902\n",
            "Iteration: 6265; loss: 0.6923184990882874\n",
            "Iteration: 6266; loss: 0.6933910250663757\n",
            "Iteration: 6267; loss: 0.6917280554771423\n",
            "Iteration: 6268; loss: 0.6959764361381531\n",
            "Iteration: 6269; loss: 0.693398118019104\n",
            "Iteration: 6270; loss: 0.6931058168411255\n",
            "Iteration: 6271; loss: 0.6905478239059448\n",
            "Iteration: 6272; loss: 0.6917377710342407\n",
            "Iteration: 6273; loss: 0.6948862075805664\n",
            "Iteration: 6274; loss: 0.692880392074585\n",
            "Iteration: 6275; loss: 0.6925804018974304\n",
            "Iteration: 6276; loss: 0.6898475289344788\n",
            "Iteration: 6277; loss: 0.6921195387840271\n",
            "Iteration: 6278; loss: 0.6922429203987122\n",
            "Iteration: 6279; loss: 0.6938320994377136\n",
            "Iteration: 6280; loss: 0.6936224699020386\n",
            "Iteration: 6281; loss: 0.6931715607643127\n",
            "Iteration: 6282; loss: 0.693128764629364\n",
            "Iteration: 6283; loss: 0.6953674554824829\n",
            "Iteration: 6284; loss: 0.6911622881889343\n",
            "Iteration: 6285; loss: 0.6919578909873962\n",
            "Iteration: 6286; loss: 0.6953191161155701\n",
            "Iteration: 6287; loss: 0.695666491985321\n",
            "Iteration: 6288; loss: 0.6932402849197388\n",
            "Iteration: 6289; loss: 0.692893385887146\n",
            "Iteration: 6290; loss: 0.6935206651687622\n",
            "Iteration: 6291; loss: 0.6918505430221558\n",
            "Iteration: 6292; loss: 0.6933256983757019\n",
            "Iteration: 6293; loss: 0.6943162679672241\n",
            "Iteration: 6294; loss: 0.6918104887008667\n",
            "Iteration: 6295; loss: 0.6922374963760376\n",
            "Iteration: 6296; loss: 0.6926401257514954\n",
            "Iteration: 6297; loss: 0.6935409903526306\n",
            "Iteration: 6298; loss: 0.6956662535667419\n",
            "Iteration: 6299; loss: 0.6932135224342346\n",
            "Iteration: 6300; loss: 0.6938083171844482\n",
            "Iteration: 6301; loss: 0.6932345628738403\n",
            "Iteration: 6302; loss: 0.6933395266532898\n",
            "Iteration: 6303; loss: 0.693954586982727\n",
            "Iteration: 6304; loss: 0.6940067410469055\n",
            "Iteration: 6305; loss: 0.6939695477485657\n",
            "Iteration: 6306; loss: 0.6943346858024597\n",
            "Iteration: 6307; loss: 0.6961675882339478\n",
            "Iteration: 6308; loss: 0.6949527263641357\n",
            "Iteration: 6309; loss: 0.6945571899414062\n",
            "Iteration: 6310; loss: 0.6916455030441284\n",
            "Iteration: 6311; loss: 0.6925466060638428\n",
            "Iteration: 6312; loss: 0.6928497552871704\n",
            "Iteration: 6313; loss: 0.6936579346656799\n",
            "Iteration: 6314; loss: 0.6922934055328369\n",
            "Iteration: 6315; loss: 0.6926445960998535\n",
            "Iteration: 6316; loss: 0.6945168972015381\n",
            "Iteration: 6317; loss: 0.692047119140625\n",
            "Iteration: 6318; loss: 0.6926356554031372\n",
            "Iteration: 6319; loss: 0.6944704055786133\n",
            "Iteration: 6320; loss: 0.692895770072937\n",
            "Iteration: 6321; loss: 0.6924184560775757\n",
            "Iteration: 6322; loss: 0.689790666103363\n",
            "Iteration: 6323; loss: 0.6924258470535278\n",
            "Iteration: 6324; loss: 0.6938327550888062\n",
            "Iteration: 6325; loss: 0.6928597688674927\n",
            "Iteration: 6326; loss: 0.6945580840110779\n",
            "Iteration: 6327; loss: 0.6921417713165283\n",
            "Iteration: 6328; loss: 0.6941419243812561\n",
            "Iteration: 6329; loss: 0.6912488341331482\n",
            "Iteration: 6330; loss: 0.6938024759292603\n",
            "Iteration: 6331; loss: 0.6945130825042725\n",
            "Iteration: 6332; loss: 0.6937819719314575\n",
            "Iteration: 6333; loss: 0.6956248879432678\n",
            "Iteration: 6334; loss: 0.6930736899375916\n",
            "Iteration: 6335; loss: 0.691392719745636\n",
            "Iteration: 6336; loss: 0.6942047476768494\n",
            "Iteration: 6337; loss: 0.6914792060852051\n",
            "Iteration: 6338; loss: 0.691283106803894\n",
            "Iteration: 6339; loss: 0.6937645077705383\n",
            "Iteration: 6340; loss: 0.6953679323196411\n",
            "Iteration: 6341; loss: 0.6941849589347839\n",
            "Iteration: 6342; loss: 0.6926289796829224\n",
            "Iteration: 6343; loss: 0.6936294436454773\n",
            "Iteration: 6344; loss: 0.6902191638946533\n",
            "Iteration: 6345; loss: 0.6923937797546387\n",
            "Iteration: 6346; loss: 0.6918541193008423\n",
            "Iteration: 6347; loss: 0.6909591555595398\n",
            "Iteration: 6348; loss: 0.6948536038398743\n",
            "Iteration: 6349; loss: 0.6904488801956177\n",
            "Iteration: 6350; loss: 0.6927303075790405\n",
            "Iteration: 6351; loss: 0.6941487789154053\n",
            "Iteration: 6352; loss: 0.6947166919708252\n",
            "Iteration: 6353; loss: 0.6918057203292847\n",
            "Iteration: 6354; loss: 0.6944175958633423\n",
            "Iteration: 6355; loss: 0.692125678062439\n",
            "Iteration: 6356; loss: 0.6934443116188049\n",
            "Iteration: 6357; loss: 0.6925912499427795\n",
            "Iteration: 6358; loss: 0.6914771795272827\n",
            "Iteration: 6359; loss: 0.6928002834320068\n",
            "Iteration: 6360; loss: 0.6944982409477234\n",
            "Iteration: 6361; loss: 0.6946231126785278\n",
            "Iteration: 6362; loss: 0.6946028470993042\n",
            "Iteration: 6363; loss: 0.6924700140953064\n",
            "Iteration: 6364; loss: 0.6950589418411255\n",
            "Iteration: 6365; loss: 0.6955893039703369\n",
            "Iteration: 6366; loss: 0.6901260614395142\n",
            "Iteration: 6367; loss: 0.6913161277770996\n",
            "Iteration: 6368; loss: 0.6959179639816284\n",
            "Iteration: 6369; loss: 0.693292498588562\n",
            "Iteration: 6370; loss: 0.6922680735588074\n",
            "Iteration: 6371; loss: 0.6926475763320923\n",
            "Iteration: 6372; loss: 0.6942921876907349\n",
            "Iteration: 6373; loss: 0.6932007670402527\n",
            "Iteration: 6374; loss: 0.6949554681777954\n",
            "Iteration: 6375; loss: 0.6940458416938782\n",
            "Iteration: 6376; loss: 0.6926048398017883\n",
            "Iteration: 6377; loss: 0.6962631344795227\n",
            "Iteration: 6378; loss: 0.6935229301452637\n",
            "Iteration: 6379; loss: 0.6950969099998474\n",
            "Iteration: 6380; loss: 0.6935615539550781\n",
            "Iteration: 6381; loss: 0.6929442882537842\n",
            "Iteration: 6382; loss: 0.6937922239303589\n",
            "Iteration: 6383; loss: 0.6929095983505249\n",
            "Iteration: 6384; loss: 0.6944417953491211\n",
            "Iteration: 6385; loss: 0.693379282951355\n",
            "Iteration: 6386; loss: 0.692241907119751\n",
            "Iteration: 6387; loss: 0.6926939487457275\n",
            "Iteration: 6388; loss: 0.6932729482650757\n",
            "Iteration: 6389; loss: 0.6919715404510498\n",
            "Iteration: 6390; loss: 0.6912623047828674\n",
            "Iteration: 6391; loss: 0.692550539970398\n",
            "Iteration: 6392; loss: 0.694729745388031\n",
            "Iteration: 6393; loss: 0.6913950443267822\n",
            "Iteration: 6394; loss: 0.6916784048080444\n",
            "Iteration: 6395; loss: 0.6928437352180481\n",
            "Iteration: 6396; loss: 0.6919475793838501\n",
            "Iteration: 6397; loss: 0.6942824721336365\n",
            "Iteration: 6398; loss: 0.6923035383224487\n",
            "Iteration: 6399; loss: 0.6936798691749573\n",
            "Iteration: 6400; loss: 0.6924186944961548\n",
            "Iteration: 6401; loss: 0.6934236884117126\n",
            "Iteration: 6402; loss: 0.6936622858047485\n",
            "Iteration: 6403; loss: 0.6932433843612671\n",
            "Iteration: 6404; loss: 0.6915737390518188\n",
            "Iteration: 6405; loss: 0.6932340860366821\n",
            "Iteration: 6406; loss: 0.6936569809913635\n",
            "Iteration: 6407; loss: 0.6927587389945984\n",
            "Iteration: 6408; loss: 0.6927165389060974\n",
            "Iteration: 6409; loss: 0.6927991509437561\n",
            "Iteration: 6410; loss: 0.6929903626441956\n",
            "Iteration: 6411; loss: 0.6920952796936035\n",
            "Iteration: 6412; loss: 0.6940797567367554\n",
            "Iteration: 6413; loss: 0.6922881007194519\n",
            "Iteration: 6414; loss: 0.6930832862854004\n",
            "Iteration: 6415; loss: 0.6922352910041809\n",
            "Iteration: 6416; loss: 0.6953145265579224\n",
            "Iteration: 6417; loss: 0.6937946677207947\n",
            "Iteration: 6418; loss: 0.6936343312263489\n",
            "Iteration: 6419; loss: 0.6929818987846375\n",
            "Iteration: 6420; loss: 0.693912148475647\n",
            "Iteration: 6421; loss: 0.6918233633041382\n",
            "Iteration: 6422; loss: 0.6923807859420776\n",
            "Iteration: 6423; loss: 0.6959461569786072\n",
            "Iteration: 6424; loss: 0.6918331384658813\n",
            "Iteration: 6425; loss: 0.6924365758895874\n",
            "Iteration: 6426; loss: 0.6932318210601807\n",
            "Iteration: 6427; loss: 0.6902944445610046\n",
            "Iteration: 6428; loss: 0.6933109760284424\n",
            "Iteration: 6429; loss: 0.6973105669021606\n",
            "Iteration: 6430; loss: 0.6930481791496277\n",
            "Iteration: 6431; loss: 0.6924477219581604\n",
            "Iteration: 6432; loss: 0.6915068030357361\n",
            "Iteration: 6433; loss: 0.6939591765403748\n",
            "Iteration: 6434; loss: 0.692229151725769\n",
            "Iteration: 6435; loss: 0.6937623023986816\n",
            "Iteration: 6436; loss: 0.691797137260437\n",
            "Iteration: 6437; loss: 0.6908404231071472\n",
            "Iteration: 6438; loss: 0.6921722292900085\n",
            "Iteration: 6439; loss: 0.6952213644981384\n",
            "Iteration: 6440; loss: 0.6936944723129272\n",
            "Iteration: 6441; loss: 0.6936157941818237\n",
            "Iteration: 6442; loss: 0.6952714323997498\n",
            "Iteration: 6443; loss: 0.6918462514877319\n",
            "Iteration: 6444; loss: 0.6953392624855042\n",
            "Iteration: 6445; loss: 0.6927129030227661\n",
            "Iteration: 6446; loss: 0.694524884223938\n",
            "Iteration: 6447; loss: 0.694298505783081\n",
            "Iteration: 6448; loss: 0.695418119430542\n",
            "Iteration: 6449; loss: 0.6947042942047119\n",
            "Iteration: 6450; loss: 0.6934416890144348\n",
            "Iteration: 6451; loss: 0.6920875310897827\n",
            "Iteration: 6452; loss: 0.6932554244995117\n",
            "Iteration: 6453; loss: 0.6923617124557495\n",
            "Iteration: 6454; loss: 0.6922226548194885\n",
            "Iteration: 6455; loss: 0.692706823348999\n",
            "Iteration: 6456; loss: 0.6929438710212708\n",
            "Iteration: 6457; loss: 0.692754328250885\n",
            "Iteration: 6458; loss: 0.6930652260780334\n",
            "Iteration: 6459; loss: 0.6920674443244934\n",
            "Iteration: 6460; loss: 0.6933234930038452\n",
            "Iteration: 6461; loss: 0.6910774111747742\n",
            "Iteration: 6462; loss: 0.6926429867744446\n",
            "Iteration: 6463; loss: 0.6922414302825928\n",
            "Iteration: 6464; loss: 0.6921108961105347\n",
            "Iteration: 6465; loss: 0.6959011554718018\n",
            "Iteration: 6466; loss: 0.6931215524673462\n",
            "Iteration: 6467; loss: 0.691259503364563\n",
            "Iteration: 6468; loss: 0.6935163140296936\n",
            "Iteration: 6469; loss: 0.69224613904953\n",
            "Iteration: 6470; loss: 0.6912074685096741\n",
            "Iteration: 6471; loss: 0.6925624012947083\n",
            "Iteration: 6472; loss: 0.6936492919921875\n",
            "Iteration: 6473; loss: 0.6936241388320923\n",
            "Iteration: 6474; loss: 0.6942428350448608\n",
            "Iteration: 6475; loss: 0.6964037418365479\n",
            "Iteration: 6476; loss: 0.6949467062950134\n",
            "Iteration: 6477; loss: 0.6909011006355286\n",
            "Iteration: 6478; loss: 0.6922333240509033\n",
            "Iteration: 6479; loss: 0.6929828524589539\n",
            "Iteration: 6480; loss: 0.6948574185371399\n",
            "Iteration: 6481; loss: 0.6944575309753418\n",
            "Iteration: 6482; loss: 0.6920541524887085\n",
            "Iteration: 6483; loss: 0.6936609148979187\n",
            "Iteration: 6484; loss: 0.6943327784538269\n",
            "Iteration: 6485; loss: 0.6929771900177002\n",
            "Iteration: 6486; loss: 0.6930790543556213\n",
            "Iteration: 6487; loss: 0.6942868232727051\n",
            "Iteration: 6488; loss: 0.694256603717804\n",
            "Iteration: 6489; loss: 0.6901281476020813\n",
            "Iteration: 6490; loss: 0.6928769946098328\n",
            "Iteration: 6491; loss: 0.6924571990966797\n",
            "Iteration: 6492; loss: 0.6945913434028625\n",
            "Iteration: 6493; loss: 0.6933715343475342\n",
            "Iteration: 6494; loss: 0.6943559050559998\n",
            "Iteration: 6495; loss: 0.6948018670082092\n",
            "Iteration: 6496; loss: 0.692071795463562\n",
            "Iteration: 6497; loss: 0.6925060153007507\n",
            "Iteration: 6498; loss: 0.6917241215705872\n",
            "Iteration: 6499; loss: 0.6946247816085815\n",
            "Iteration: 6500; loss: 0.6930822730064392\n",
            "Iteration: 6501; loss: 0.694105327129364\n",
            "Iteration: 6502; loss: 0.6923602819442749\n",
            "Iteration: 6503; loss: 0.6937965154647827\n",
            "Iteration: 6504; loss: 0.6938905715942383\n",
            "Iteration: 6505; loss: 0.6913596391677856\n",
            "Iteration: 6506; loss: 0.6939689517021179\n",
            "Iteration: 6507; loss: 0.6960859894752502\n",
            "Iteration: 6508; loss: 0.6956290006637573\n",
            "Iteration: 6509; loss: 0.6935644149780273\n",
            "Iteration: 6510; loss: 0.6929212808609009\n",
            "Iteration: 6511; loss: 0.6921648979187012\n",
            "Iteration: 6512; loss: 0.6924870014190674\n",
            "Iteration: 6513; loss: 0.692228376865387\n",
            "Iteration: 6514; loss: 0.6902920603752136\n",
            "Iteration: 6515; loss: 0.6935074925422668\n",
            "Iteration: 6516; loss: 0.692303478717804\n",
            "Iteration: 6517; loss: 0.6920968294143677\n",
            "Iteration: 6518; loss: 0.6915329098701477\n",
            "Iteration: 6519; loss: 0.691355288028717\n",
            "Iteration: 6520; loss: 0.6935857534408569\n",
            "Iteration: 6521; loss: 0.6913959383964539\n",
            "Iteration: 6522; loss: 0.6942952871322632\n",
            "Iteration: 6523; loss: 0.6918646693229675\n",
            "Iteration: 6524; loss: 0.6942746639251709\n",
            "Iteration: 6525; loss: 0.6927306652069092\n",
            "Iteration: 6526; loss: 0.6947312355041504\n",
            "Iteration: 6527; loss: 0.6920696496963501\n",
            "Iteration: 6528; loss: 0.6925201416015625\n",
            "Iteration: 6529; loss: 0.6930921673774719\n",
            "Iteration: 6530; loss: 0.6932879686355591\n",
            "Iteration: 6531; loss: 0.6924624443054199\n",
            "Iteration: 6532; loss: 0.69123774766922\n",
            "Iteration: 6533; loss: 0.693073034286499\n",
            "Iteration: 6534; loss: 0.6923902034759521\n",
            "Iteration: 6535; loss: 0.6938858032226562\n",
            "Iteration: 6536; loss: 0.6920965313911438\n",
            "Iteration: 6537; loss: 0.6920008659362793\n",
            "Iteration: 6538; loss: 0.692094087600708\n",
            "Iteration: 6539; loss: 0.6919569969177246\n",
            "Iteration: 6540; loss: 0.6930664777755737\n",
            "Iteration: 6541; loss: 0.6935919523239136\n",
            "Iteration: 6542; loss: 0.6942766904830933\n",
            "Iteration: 6543; loss: 0.6960216760635376\n",
            "Iteration: 6544; loss: 0.6919236183166504\n",
            "Iteration: 6545; loss: 0.6926146149635315\n",
            "Iteration: 6546; loss: 0.6917469501495361\n",
            "Iteration: 6547; loss: 0.6943899393081665\n",
            "Iteration: 6548; loss: 0.6921551823616028\n",
            "Iteration: 6549; loss: 0.6934762597084045\n",
            "Iteration: 6550; loss: 0.6936696171760559\n",
            "Iteration: 6551; loss: 0.6915570497512817\n",
            "Iteration: 6552; loss: 0.6908635497093201\n",
            "Iteration: 6553; loss: 0.6935874223709106\n",
            "Iteration: 6554; loss: 0.6922587752342224\n",
            "Iteration: 6555; loss: 0.6938666701316833\n",
            "Iteration: 6556; loss: 0.6915591955184937\n",
            "Iteration: 6557; loss: 0.6942158341407776\n",
            "Iteration: 6558; loss: 0.695412814617157\n",
            "Iteration: 6559; loss: 0.6918225884437561\n",
            "Iteration: 6560; loss: 0.6902062296867371\n",
            "Iteration: 6561; loss: 0.6927152276039124\n",
            "Iteration: 6562; loss: 0.6919658184051514\n",
            "Iteration: 6563; loss: 0.6924354434013367\n",
            "Iteration: 6564; loss: 0.6952351331710815\n",
            "Iteration: 6565; loss: 0.694821834564209\n",
            "Iteration: 6566; loss: 0.6926319599151611\n",
            "Iteration: 6567; loss: 0.691216230392456\n",
            "Iteration: 6568; loss: 0.6934105157852173\n",
            "Iteration: 6569; loss: 0.6929555535316467\n",
            "Iteration: 6570; loss: 0.6930575966835022\n",
            "Iteration: 6571; loss: 0.6914703845977783\n",
            "Iteration: 6572; loss: 0.6898604035377502\n",
            "Iteration: 6573; loss: 0.6923320889472961\n",
            "Iteration: 6574; loss: 0.6917164921760559\n",
            "Iteration: 6575; loss: 0.6931145787239075\n",
            "Iteration: 6576; loss: 0.6915566325187683\n",
            "Iteration: 6577; loss: 0.6946116089820862\n",
            "Iteration: 6578; loss: 0.6946794390678406\n",
            "Iteration: 6579; loss: 0.6920009851455688\n",
            "Iteration: 6580; loss: 0.6916521787643433\n",
            "Iteration: 6581; loss: 0.6957089900970459\n",
            "Iteration: 6582; loss: 0.6916835904121399\n",
            "Iteration: 6583; loss: 0.6955531239509583\n",
            "Iteration: 6584; loss: 0.6905412077903748\n",
            "Iteration: 6585; loss: 0.6938371658325195\n",
            "Iteration: 6586; loss: 0.6904940605163574\n",
            "Iteration: 6587; loss: 0.6930901408195496\n",
            "Iteration: 6588; loss: 0.6908606886863708\n",
            "Iteration: 6589; loss: 0.6930166482925415\n",
            "Iteration: 6590; loss: 0.694898247718811\n",
            "Iteration: 6591; loss: 0.6936132311820984\n",
            "Iteration: 6592; loss: 0.6919974088668823\n",
            "Iteration: 6593; loss: 0.6917967796325684\n",
            "Iteration: 6594; loss: 0.6905070543289185\n",
            "Iteration: 6595; loss: 0.6913461089134216\n",
            "Iteration: 6596; loss: 0.687351405620575\n",
            "Iteration: 6597; loss: 0.6905786395072937\n",
            "Iteration: 6598; loss: 0.693159818649292\n",
            "Iteration: 6599; loss: 0.6894319653511047\n",
            "Iteration: 6600; loss: 0.6903316974639893\n",
            "Iteration: 6601; loss: 0.6918156147003174\n",
            "Iteration: 6602; loss: 0.6946925520896912\n",
            "Iteration: 6603; loss: 0.6882137656211853\n",
            "Iteration: 6604; loss: 0.6905497312545776\n",
            "Iteration: 6605; loss: 0.695947527885437\n",
            "Iteration: 6606; loss: 0.6940650939941406\n",
            "Iteration: 6607; loss: 0.6913667917251587\n",
            "Iteration: 6608; loss: 0.6956087350845337\n",
            "Iteration: 6609; loss: 0.6976345181465149\n",
            "Iteration: 6610; loss: 0.6920773386955261\n",
            "Iteration: 6611; loss: 0.6885555982589722\n",
            "Iteration: 6612; loss: 0.6979658603668213\n",
            "Iteration: 6613; loss: 0.6944552659988403\n",
            "Iteration: 6614; loss: 0.6989237666130066\n",
            "Iteration: 6615; loss: 0.6932598352432251\n",
            "Iteration: 6616; loss: 0.6929547786712646\n",
            "Iteration: 6617; loss: 0.6931633353233337\n",
            "Iteration: 6618; loss: 0.6926522254943848\n",
            "Iteration: 6619; loss: 0.6931102871894836\n",
            "Iteration: 6620; loss: 0.691131591796875\n",
            "Iteration: 6621; loss: 0.698128342628479\n",
            "Iteration: 6622; loss: 0.6924389600753784\n",
            "Iteration: 6623; loss: 0.6924715042114258\n",
            "Iteration: 6624; loss: 0.6907095313072205\n",
            "Iteration: 6625; loss: 0.6956452131271362\n",
            "Iteration: 6626; loss: 0.6945404410362244\n",
            "Iteration: 6627; loss: 0.6935049891471863\n",
            "Iteration: 6628; loss: 0.6905089020729065\n",
            "Iteration: 6629; loss: 0.6935283541679382\n",
            "Iteration: 6630; loss: 0.6971613764762878\n",
            "Iteration: 6631; loss: 0.6958642601966858\n",
            "Iteration: 6632; loss: 0.6896251440048218\n",
            "Iteration: 6633; loss: 0.690324068069458\n",
            "Iteration: 6634; loss: 0.6950525641441345\n",
            "Iteration: 6635; loss: 0.6924533843994141\n",
            "Iteration: 6636; loss: 0.6933043599128723\n",
            "Iteration: 6637; loss: 0.694438636302948\n",
            "Iteration: 6638; loss: 0.6969478726387024\n",
            "Iteration: 6639; loss: 0.6925249099731445\n",
            "Iteration: 6640; loss: 0.6942702531814575\n",
            "Iteration: 6641; loss: 0.6912345886230469\n",
            "Iteration: 6642; loss: 0.6948791146278381\n",
            "Iteration: 6643; loss: 0.6938111782073975\n",
            "Iteration: 6644; loss: 0.6929478049278259\n",
            "Iteration: 6645; loss: 0.6958320140838623\n",
            "Iteration: 6646; loss: 0.6910997629165649\n",
            "Iteration: 6647; loss: 0.6955955028533936\n",
            "Iteration: 6648; loss: 0.6896243691444397\n",
            "Iteration: 6649; loss: 0.6936789751052856\n",
            "Iteration: 6650; loss: 0.6926618218421936\n",
            "Iteration: 6651; loss: 0.6928080320358276\n",
            "Iteration: 6652; loss: 0.6911181211471558\n",
            "Iteration: 6653; loss: 0.6972593069076538\n",
            "Iteration: 6654; loss: 0.6925866603851318\n",
            "Iteration: 6655; loss: 0.6930781602859497\n",
            "Iteration: 6656; loss: 0.6943637728691101\n",
            "Iteration: 6657; loss: 0.6941155195236206\n",
            "Iteration: 6658; loss: 0.6937233209609985\n",
            "Iteration: 6659; loss: 0.6938243508338928\n",
            "Iteration: 6660; loss: 0.6924499869346619\n",
            "Iteration: 6661; loss: 0.6904207468032837\n",
            "Iteration: 6662; loss: 0.6922193765640259\n",
            "Iteration: 6663; loss: 0.6930449604988098\n",
            "Iteration: 6664; loss: 0.6918848156929016\n",
            "Iteration: 6665; loss: 0.6930303573608398\n",
            "Iteration: 6666; loss: 0.6936598420143127\n",
            "Iteration: 6667; loss: 0.696164071559906\n",
            "Iteration: 6668; loss: 0.6969733834266663\n",
            "Iteration: 6669; loss: 0.6941943764686584\n",
            "Iteration: 6670; loss: 0.6908260583877563\n",
            "Iteration: 6671; loss: 0.6926422715187073\n",
            "Iteration: 6672; loss: 0.6941782236099243\n",
            "Iteration: 6673; loss: 0.693011999130249\n",
            "Iteration: 6674; loss: 0.6941962242126465\n",
            "Iteration: 6675; loss: 0.6921002268791199\n",
            "Iteration: 6676; loss: 0.6953199505805969\n",
            "Iteration: 6677; loss: 0.6939774751663208\n",
            "Iteration: 6678; loss: 0.6933966875076294\n",
            "Iteration: 6679; loss: 0.6902042627334595\n",
            "Iteration: 6680; loss: 0.693257749080658\n",
            "Iteration: 6681; loss: 0.6931310892105103\n",
            "Iteration: 6682; loss: 0.6942704319953918\n",
            "Iteration: 6683; loss: 0.6915646195411682\n",
            "Iteration: 6684; loss: 0.6929100155830383\n",
            "Iteration: 6685; loss: 0.6968685984611511\n",
            "Iteration: 6686; loss: 0.6940746307373047\n",
            "Iteration: 6687; loss: 0.6930640935897827\n",
            "Iteration: 6688; loss: 0.6941762566566467\n",
            "Iteration: 6689; loss: 0.6935818195343018\n",
            "Iteration: 6690; loss: 0.6937334537506104\n",
            "Iteration: 6691; loss: 0.6916784048080444\n",
            "Iteration: 6692; loss: 0.6929547190666199\n",
            "Iteration: 6693; loss: 0.6940252780914307\n",
            "Iteration: 6694; loss: 0.6923738718032837\n",
            "Iteration: 6695; loss: 0.6919735670089722\n",
            "Iteration: 6696; loss: 0.6915174722671509\n",
            "Iteration: 6697; loss: 0.6918935775756836\n",
            "Iteration: 6698; loss: 0.6911765336990356\n",
            "Iteration: 6699; loss: 0.6933843493461609\n",
            "Iteration: 6700; loss: 0.6925695538520813\n",
            "Iteration: 6701; loss: 0.6954512596130371\n",
            "Iteration: 6702; loss: 0.6939273476600647\n",
            "Iteration: 6703; loss: 0.6936787962913513\n",
            "Iteration: 6704; loss: 0.6938812136650085\n",
            "Iteration: 6705; loss: 0.6930873394012451\n",
            "Iteration: 6706; loss: 0.6909604072570801\n",
            "Iteration: 6707; loss: 0.6937580704689026\n",
            "Iteration: 6708; loss: 0.6935113668441772\n",
            "Iteration: 6709; loss: 0.6925357580184937\n",
            "Iteration: 6710; loss: 0.6928601264953613\n",
            "Iteration: 6711; loss: 0.6945635676383972\n",
            "Iteration: 6712; loss: 0.6928538680076599\n",
            "Iteration: 6713; loss: 0.6958047151565552\n",
            "Iteration: 6714; loss: 0.693608283996582\n",
            "Iteration: 6715; loss: 0.6929802298545837\n",
            "Iteration: 6716; loss: 0.693206250667572\n",
            "Iteration: 6717; loss: 0.6927996873855591\n",
            "Iteration: 6718; loss: 0.6934952139854431\n",
            "Iteration: 6719; loss: 0.6922945976257324\n",
            "Iteration: 6720; loss: 0.6912155151367188\n",
            "Iteration: 6721; loss: 0.6951742768287659\n",
            "Iteration: 6722; loss: 0.6944736838340759\n",
            "Iteration: 6723; loss: 0.6933842897415161\n",
            "Iteration: 6724; loss: 0.693324863910675\n",
            "Iteration: 6725; loss: 0.6948570013046265\n",
            "Iteration: 6726; loss: 0.694951057434082\n",
            "Iteration: 6727; loss: 0.6919366121292114\n",
            "Iteration: 6728; loss: 0.6916804313659668\n",
            "Iteration: 6729; loss: 0.6930388808250427\n",
            "Iteration: 6730; loss: 0.6934846639633179\n",
            "Iteration: 6731; loss: 0.6936130523681641\n",
            "Iteration: 6732; loss: 0.6921207904815674\n",
            "Iteration: 6733; loss: 0.6947150826454163\n",
            "Iteration: 6734; loss: 0.6899356842041016\n",
            "Iteration: 6735; loss: 0.6934531927108765\n",
            "Iteration: 6736; loss: 0.6918839812278748\n",
            "Iteration: 6737; loss: 0.6931463479995728\n",
            "Iteration: 6738; loss: 0.6931852102279663\n",
            "Iteration: 6739; loss: 0.6937445998191833\n",
            "Iteration: 6740; loss: 0.6927131414413452\n",
            "Iteration: 6741; loss: 0.6939648389816284\n",
            "Iteration: 6742; loss: 0.693846583366394\n",
            "Iteration: 6743; loss: 0.6932842135429382\n",
            "Iteration: 6744; loss: 0.6920521259307861\n",
            "Iteration: 6745; loss: 0.6929799318313599\n",
            "Iteration: 6746; loss: 0.6929910778999329\n",
            "Iteration: 6747; loss: 0.6913126111030579\n",
            "Iteration: 6748; loss: 0.6886053681373596\n",
            "Iteration: 6749; loss: 0.6926071047782898\n",
            "Iteration: 6750; loss: 0.694635808467865\n",
            "Iteration: 6751; loss: 0.6959425806999207\n",
            "Iteration: 6752; loss: 0.6920111179351807\n",
            "Iteration: 6753; loss: 0.6880820989608765\n",
            "Iteration: 6754; loss: 0.6911185383796692\n",
            "Iteration: 6755; loss: 0.6911157369613647\n",
            "Iteration: 6756; loss: 0.6938021779060364\n",
            "Iteration: 6757; loss: 0.69342440366745\n",
            "Iteration: 6758; loss: 0.69289231300354\n",
            "Iteration: 6759; loss: 0.6908143758773804\n",
            "Iteration: 6760; loss: 0.6921554803848267\n",
            "Iteration: 6761; loss: 0.6937638521194458\n",
            "Iteration: 6762; loss: 0.6924206018447876\n",
            "Iteration: 6763; loss: 0.6934642791748047\n",
            "Iteration: 6764; loss: 0.6911517381668091\n",
            "Iteration: 6765; loss: 0.6907351016998291\n",
            "Iteration: 6766; loss: 0.6915791034698486\n",
            "Iteration: 6767; loss: 0.6866326928138733\n",
            "Iteration: 6768; loss: 0.6926860809326172\n",
            "Iteration: 6769; loss: 0.6913683414459229\n",
            "Iteration: 6770; loss: 0.6939733028411865\n",
            "Iteration: 6771; loss: 0.690273642539978\n",
            "Iteration: 6772; loss: 0.6894044280052185\n",
            "Iteration: 6773; loss: 0.6927384734153748\n",
            "Iteration: 6774; loss: 0.6947685480117798\n",
            "Iteration: 6775; loss: 0.6947132349014282\n",
            "Iteration: 6776; loss: 0.6926846504211426\n",
            "Iteration: 6777; loss: 0.692533016204834\n",
            "Iteration: 6778; loss: 0.6900469660758972\n",
            "Iteration: 6779; loss: 0.6921324133872986\n",
            "Iteration: 6780; loss: 0.6963311433792114\n",
            "Iteration: 6781; loss: 0.6954349875450134\n",
            "Iteration: 6782; loss: 0.6906988620758057\n",
            "Iteration: 6783; loss: 0.6958616375923157\n",
            "Iteration: 6784; loss: 0.6990605592727661\n",
            "Iteration: 6785; loss: 0.695105791091919\n",
            "Iteration: 6786; loss: 0.6937223672866821\n",
            "Iteration: 6787; loss: 0.6980625987052917\n",
            "Iteration: 6788; loss: 0.6909797191619873\n",
            "Iteration: 6789; loss: 0.6915327310562134\n",
            "Iteration: 6790; loss: 0.6922886967658997\n",
            "Iteration: 6791; loss: 0.6896212697029114\n",
            "Iteration: 6792; loss: 0.6911630630493164\n",
            "Iteration: 6793; loss: 0.6923655271530151\n",
            "Iteration: 6794; loss: 0.6912673115730286\n",
            "Iteration: 6795; loss: 0.6923456192016602\n",
            "Iteration: 6796; loss: 0.6949377059936523\n",
            "Iteration: 6797; loss: 0.6923719048500061\n",
            "Iteration: 6798; loss: 0.6895110011100769\n",
            "Iteration: 6799; loss: 0.6913551092147827\n",
            "Iteration: 6800; loss: 0.6933955550193787\n",
            "Iteration: 6801; loss: 0.690473735332489\n",
            "Iteration: 6802; loss: 0.6917436122894287\n",
            "Iteration: 6803; loss: 0.6950200796127319\n",
            "Iteration: 6804; loss: 0.6948952674865723\n",
            "Iteration: 6805; loss: 0.6920435428619385\n",
            "Iteration: 6806; loss: 0.6944296360015869\n",
            "Iteration: 6807; loss: 0.6908279657363892\n",
            "Iteration: 6808; loss: 0.690159261226654\n",
            "Iteration: 6809; loss: 0.6950458288192749\n",
            "Iteration: 6810; loss: 0.689802348613739\n",
            "Iteration: 6811; loss: 0.6918210983276367\n",
            "Iteration: 6812; loss: 0.691820502281189\n",
            "Iteration: 6813; loss: 0.6906793713569641\n",
            "Iteration: 6814; loss: 0.693245530128479\n",
            "Iteration: 6815; loss: 0.6918100118637085\n",
            "Iteration: 6816; loss: 0.6924346685409546\n",
            "Iteration: 6817; loss: 0.6925628185272217\n",
            "Iteration: 6818; loss: 0.6930578947067261\n",
            "Iteration: 6819; loss: 0.6929969787597656\n",
            "Iteration: 6820; loss: 0.6962246894836426\n",
            "Iteration: 6821; loss: 0.6979315876960754\n",
            "Iteration: 6822; loss: 0.6867196559906006\n",
            "Iteration: 6823; loss: 0.6901754140853882\n",
            "Iteration: 6824; loss: 0.6921243667602539\n",
            "Iteration: 6825; loss: 0.6924518942832947\n",
            "Iteration: 6826; loss: 0.6945311427116394\n",
            "Iteration: 6827; loss: 0.6958178281784058\n",
            "Iteration: 6828; loss: 0.6943094730377197\n",
            "Iteration: 6829; loss: 0.6905818581581116\n",
            "Iteration: 6830; loss: 0.6903055906295776\n",
            "Iteration: 6831; loss: 0.6970738172531128\n",
            "Iteration: 6832; loss: 0.6942717432975769\n",
            "Iteration: 6833; loss: 0.6983892917633057\n",
            "Iteration: 6834; loss: 0.6933939456939697\n",
            "Iteration: 6835; loss: 0.6945912837982178\n",
            "Iteration: 6836; loss: 0.6933477520942688\n",
            "Iteration: 6837; loss: 0.6969172954559326\n",
            "Iteration: 6838; loss: 0.6919949054718018\n",
            "Iteration: 6839; loss: 0.6975680589675903\n",
            "Iteration: 6840; loss: 0.6889159679412842\n",
            "Iteration: 6841; loss: 0.6949253082275391\n",
            "Iteration: 6842; loss: 0.6929205060005188\n",
            "Iteration: 6843; loss: 0.6928408145904541\n",
            "Iteration: 6844; loss: 0.69291752576828\n",
            "Iteration: 6845; loss: 0.6899964809417725\n",
            "Iteration: 6846; loss: 0.6958797574043274\n",
            "Iteration: 6847; loss: 0.6885175108909607\n",
            "Iteration: 6848; loss: 0.693417489528656\n",
            "Iteration: 6849; loss: 0.6915104389190674\n",
            "Iteration: 6850; loss: 0.6913020610809326\n",
            "Iteration: 6851; loss: 0.6943792104721069\n",
            "Iteration: 6852; loss: 0.6932602524757385\n",
            "Iteration: 6853; loss: 0.6940699815750122\n",
            "Iteration: 6854; loss: 0.6894093155860901\n",
            "Iteration: 6855; loss: 0.6896689534187317\n",
            "Iteration: 6856; loss: 0.6951882243156433\n",
            "Iteration: 6857; loss: 0.6909401416778564\n",
            "Iteration: 6858; loss: 0.6936246752738953\n",
            "Iteration: 6859; loss: 0.6929376721382141\n",
            "Iteration: 6860; loss: 0.6903289556503296\n",
            "Iteration: 6861; loss: 0.6980939507484436\n",
            "Iteration: 6862; loss: 0.6929370760917664\n",
            "Iteration: 6863; loss: 0.6913836598396301\n",
            "Iteration: 6864; loss: 0.6993548274040222\n",
            "Iteration: 6865; loss: 0.696091890335083\n",
            "Iteration: 6866; loss: 0.6864063739776611\n",
            "Iteration: 6867; loss: 0.6946905255317688\n",
            "Iteration: 6868; loss: 0.6911476254463196\n",
            "Iteration: 6869; loss: 0.6952393651008606\n",
            "Iteration: 6870; loss: 0.6929205060005188\n",
            "Iteration: 6871; loss: 0.6921514272689819\n",
            "Iteration: 6872; loss: 0.6959866285324097\n",
            "Iteration: 6873; loss: 0.6938501596450806\n",
            "Iteration: 6874; loss: 0.6875331401824951\n",
            "Iteration: 6875; loss: 0.6934350728988647\n",
            "Iteration: 6876; loss: 0.692444920539856\n",
            "Iteration: 6877; loss: 0.6902449727058411\n",
            "Iteration: 6878; loss: 0.6971608400344849\n",
            "Iteration: 6879; loss: 0.6926258206367493\n",
            "Iteration: 6880; loss: 0.695152223110199\n",
            "Iteration: 6881; loss: 0.6914494037628174\n",
            "Iteration: 6882; loss: 0.6973344087600708\n",
            "Iteration: 6883; loss: 0.6944512128829956\n",
            "Iteration: 6884; loss: 0.6933393478393555\n",
            "Iteration: 6885; loss: 0.6912702322006226\n",
            "Iteration: 6886; loss: 0.693451464176178\n",
            "Iteration: 6887; loss: 0.6919304132461548\n",
            "Iteration: 6888; loss: 0.6897873878479004\n",
            "Iteration: 6889; loss: 0.692391037940979\n",
            "Iteration: 6890; loss: 0.692267894744873\n",
            "Iteration: 6891; loss: 0.6929064989089966\n",
            "Iteration: 6892; loss: 0.6936111450195312\n",
            "Iteration: 6893; loss: 0.6953123211860657\n",
            "Iteration: 6894; loss: 0.6901847720146179\n",
            "Iteration: 6895; loss: 0.6934469938278198\n",
            "Iteration: 6896; loss: 0.6955384016036987\n",
            "Iteration: 6897; loss: 0.6933020353317261\n",
            "Iteration: 6898; loss: 0.6897334456443787\n",
            "Iteration: 6899; loss: 0.6968721151351929\n",
            "Iteration: 6900; loss: 0.6922816634178162\n",
            "Iteration: 6901; loss: 0.6973729729652405\n",
            "Iteration: 6902; loss: 0.6911628246307373\n",
            "Iteration: 6903; loss: 0.6951873302459717\n",
            "Iteration: 6904; loss: 0.6921009421348572\n",
            "Iteration: 6905; loss: 0.6933315396308899\n",
            "Iteration: 6906; loss: 0.6889984607696533\n",
            "Iteration: 6907; loss: 0.692188560962677\n",
            "Iteration: 6908; loss: 0.691094696521759\n",
            "Iteration: 6909; loss: 0.6926265358924866\n",
            "Iteration: 6910; loss: 0.6890227794647217\n",
            "Iteration: 6911; loss: 0.690711498260498\n",
            "Iteration: 6912; loss: 0.694489598274231\n",
            "Iteration: 6913; loss: 0.6929847002029419\n",
            "Iteration: 6914; loss: 0.6957694292068481\n",
            "Iteration: 6915; loss: 0.6910701394081116\n",
            "Iteration: 6916; loss: 0.6901973485946655\n",
            "Iteration: 6917; loss: 0.6911406517028809\n",
            "Iteration: 6918; loss: 0.6924808025360107\n",
            "Iteration: 6919; loss: 0.6948451995849609\n",
            "Iteration: 6920; loss: 0.6932540535926819\n",
            "Iteration: 6921; loss: 0.6920145153999329\n",
            "Iteration: 6922; loss: 0.6911056637763977\n",
            "Iteration: 6923; loss: 0.6911962628364563\n",
            "Iteration: 6924; loss: 0.6968898177146912\n",
            "Iteration: 6925; loss: 0.6939002275466919\n",
            "Iteration: 6926; loss: 0.689106822013855\n",
            "Iteration: 6927; loss: 0.6955223083496094\n",
            "Iteration: 6928; loss: 0.6955571174621582\n",
            "Iteration: 6929; loss: 0.6918706893920898\n",
            "Iteration: 6930; loss: 0.6935470104217529\n",
            "Iteration: 6931; loss: 0.6952195167541504\n",
            "Iteration: 6932; loss: 0.6910271644592285\n",
            "Iteration: 6933; loss: 0.6915509104728699\n",
            "Iteration: 6934; loss: 0.6958087682723999\n",
            "Iteration: 6935; loss: 0.6881211400032043\n",
            "Iteration: 6936; loss: 0.6945139169692993\n",
            "Iteration: 6937; loss: 0.6923285722732544\n",
            "Iteration: 6938; loss: 0.6950483322143555\n",
            "Iteration: 6939; loss: 0.6926242709159851\n",
            "Iteration: 6940; loss: 0.6918694972991943\n",
            "Iteration: 6941; loss: 0.6949558258056641\n",
            "Iteration: 6942; loss: 0.6870567798614502\n",
            "Iteration: 6943; loss: 0.6936765313148499\n",
            "Iteration: 6944; loss: 0.6939356923103333\n",
            "Iteration: 6945; loss: 0.694595456123352\n",
            "Iteration: 6946; loss: 0.6924893856048584\n",
            "Iteration: 6947; loss: 0.6912665963172913\n",
            "Iteration: 6948; loss: 0.6894255876541138\n",
            "Iteration: 6949; loss: 0.6901969909667969\n",
            "Iteration: 6950; loss: 0.6915770769119263\n",
            "Iteration: 6951; loss: 0.6903752684593201\n",
            "Iteration: 6952; loss: 0.6907634139060974\n",
            "Iteration: 6953; loss: 0.6957619786262512\n",
            "Iteration: 6954; loss: 0.6938436031341553\n",
            "Iteration: 6955; loss: 0.6901485323905945\n",
            "Iteration: 6956; loss: 0.6883638501167297\n",
            "Iteration: 6957; loss: 0.6904779076576233\n",
            "Iteration: 6958; loss: 0.6847455501556396\n",
            "Iteration: 6959; loss: 0.6911055445671082\n",
            "Iteration: 6960; loss: 0.6893486380577087\n",
            "Iteration: 6961; loss: 0.6848886609077454\n",
            "Iteration: 6962; loss: 0.6894205808639526\n",
            "Iteration: 6963; loss: 0.6990832090377808\n",
            "Iteration: 6964; loss: 0.6886831521987915\n",
            "Iteration: 6965; loss: 0.6909157633781433\n",
            "Iteration: 6966; loss: 0.6954585313796997\n",
            "Iteration: 6967; loss: 0.6955183148384094\n",
            "Iteration: 6968; loss: 0.6995171308517456\n",
            "Iteration: 6969; loss: 0.6951884627342224\n",
            "Iteration: 6970; loss: 0.6930940747261047\n",
            "Iteration: 6971; loss: 0.695723295211792\n",
            "Iteration: 6972; loss: 0.6873792409896851\n",
            "Iteration: 6973; loss: 0.6954818964004517\n",
            "Iteration: 6974; loss: 0.6916863918304443\n",
            "Iteration: 6975; loss: 0.6874597072601318\n",
            "Iteration: 6976; loss: 0.6898669600486755\n",
            "Iteration: 6977; loss: 0.6924898624420166\n",
            "Iteration: 6978; loss: 0.6958196759223938\n",
            "Iteration: 6979; loss: 0.6937100887298584\n",
            "Iteration: 6980; loss: 0.6892318725585938\n",
            "Iteration: 6981; loss: 0.694442629814148\n",
            "Iteration: 6982; loss: 0.6915372610092163\n",
            "Iteration: 6983; loss: 0.6942666172981262\n",
            "Iteration: 6984; loss: 0.6928920149803162\n",
            "Iteration: 6985; loss: 0.6957884430885315\n",
            "Iteration: 6986; loss: 0.6935176253318787\n",
            "Iteration: 6987; loss: 0.687549889087677\n",
            "Iteration: 6988; loss: 0.6955746412277222\n",
            "Iteration: 6989; loss: 0.6966359615325928\n",
            "Iteration: 6990; loss: 0.6958125829696655\n",
            "Iteration: 6991; loss: 0.6909838914871216\n",
            "Iteration: 6992; loss: 0.6949169635772705\n",
            "Iteration: 6993; loss: 0.6903491616249084\n",
            "Iteration: 6994; loss: 0.6941682696342468\n",
            "Iteration: 6995; loss: 0.696469247341156\n",
            "Iteration: 6996; loss: 0.6883035898208618\n",
            "Iteration: 6997; loss: 0.690711259841919\n",
            "Iteration: 6998; loss: 0.6938514709472656\n",
            "Iteration: 6999; loss: 0.689923107624054\n",
            "Iteration: 7000; loss: 0.6937167644500732\n",
            "Iteration: 7001; loss: 0.6903156638145447\n",
            "Iteration: 7002; loss: 0.6938194036483765\n",
            "Iteration: 7003; loss: 0.695138156414032\n",
            "Iteration: 7004; loss: 0.6928649544715881\n",
            "Iteration: 7005; loss: 0.6940565705299377\n",
            "Iteration: 7006; loss: 0.693862795829773\n",
            "Iteration: 7007; loss: 0.693977952003479\n",
            "Iteration: 7008; loss: 0.6953521966934204\n",
            "Iteration: 7009; loss: 0.6923002004623413\n",
            "Iteration: 7010; loss: 0.6923167705535889\n",
            "Iteration: 7011; loss: 0.6920746564865112\n",
            "Iteration: 7012; loss: 0.6895169615745544\n",
            "Iteration: 7013; loss: 0.6914232969284058\n",
            "Iteration: 7014; loss: 0.6914689540863037\n",
            "Iteration: 7015; loss: 0.6932566165924072\n",
            "Iteration: 7016; loss: 0.6907116770744324\n",
            "Iteration: 7017; loss: 0.6915647387504578\n",
            "Iteration: 7018; loss: 0.694987952709198\n",
            "Iteration: 7019; loss: 0.694410502910614\n",
            "Iteration: 7020; loss: 0.6943500638008118\n",
            "Iteration: 7021; loss: 0.6926001310348511\n",
            "Iteration: 7022; loss: 0.6963571906089783\n",
            "Iteration: 7023; loss: 0.6942042112350464\n",
            "Iteration: 7024; loss: 0.6914913654327393\n",
            "Iteration: 7025; loss: 0.6893192529678345\n",
            "Iteration: 7026; loss: 0.6905643343925476\n",
            "Iteration: 7027; loss: 0.6949577331542969\n",
            "Iteration: 7028; loss: 0.6973172426223755\n",
            "Iteration: 7029; loss: 0.6886608600616455\n",
            "Iteration: 7030; loss: 0.690941572189331\n",
            "Iteration: 7031; loss: 0.6924142837524414\n",
            "Iteration: 7032; loss: 0.6943622827529907\n",
            "Iteration: 7033; loss: 0.6930091381072998\n",
            "Iteration: 7034; loss: 0.6941372752189636\n",
            "Iteration: 7035; loss: 0.6946929693222046\n",
            "Iteration: 7036; loss: 0.695605456829071\n",
            "Iteration: 7037; loss: 0.6862004995346069\n",
            "Iteration: 7038; loss: 0.6946418285369873\n",
            "Iteration: 7039; loss: 0.6865150332450867\n",
            "Iteration: 7040; loss: 0.6956097483634949\n",
            "Iteration: 7041; loss: 0.689671516418457\n",
            "Iteration: 7042; loss: 0.6906875967979431\n",
            "Iteration: 7043; loss: 0.6919583678245544\n",
            "Iteration: 7044; loss: 0.69109046459198\n",
            "Iteration: 7045; loss: 0.6935374736785889\n",
            "Iteration: 7046; loss: 0.6897767186164856\n",
            "Iteration: 7047; loss: 0.6929178833961487\n",
            "Iteration: 7048; loss: 0.690504252910614\n",
            "Iteration: 7049; loss: 0.6944725513458252\n",
            "Iteration: 7050; loss: 0.6918658018112183\n",
            "Iteration: 7051; loss: 0.6906082630157471\n",
            "Iteration: 7052; loss: 0.694028377532959\n",
            "Iteration: 7053; loss: 0.6901249885559082\n",
            "Iteration: 7054; loss: 0.6934863328933716\n",
            "Iteration: 7055; loss: 0.6955441236495972\n",
            "Iteration: 7056; loss: 0.6954503655433655\n",
            "Iteration: 7057; loss: 0.6897759437561035\n",
            "Iteration: 7058; loss: 0.6890506744384766\n",
            "Iteration: 7059; loss: 0.6970351338386536\n",
            "Iteration: 7060; loss: 0.6885926127433777\n",
            "Iteration: 7061; loss: 0.6915367841720581\n",
            "Iteration: 7062; loss: 0.6924317479133606\n",
            "Iteration: 7063; loss: 0.6940844058990479\n",
            "Iteration: 7064; loss: 0.692011296749115\n",
            "Iteration: 7065; loss: 0.6916356086730957\n",
            "Iteration: 7066; loss: 0.6927748322486877\n",
            "Iteration: 7067; loss: 0.6943732500076294\n",
            "Iteration: 7068; loss: 0.6946803331375122\n",
            "Iteration: 7069; loss: 0.68843674659729\n",
            "Iteration: 7070; loss: 0.694036066532135\n",
            "Iteration: 7071; loss: 0.6911835670471191\n",
            "Iteration: 7072; loss: 0.6955649256706238\n",
            "Iteration: 7073; loss: 0.6930067539215088\n",
            "Iteration: 7074; loss: 0.6919845342636108\n",
            "Iteration: 7075; loss: 0.688034176826477\n",
            "Iteration: 7076; loss: 0.6907293200492859\n",
            "Iteration: 7077; loss: 0.6917895674705505\n",
            "Iteration: 7078; loss: 0.688252866268158\n",
            "Iteration: 7079; loss: 0.6972360610961914\n",
            "Iteration: 7080; loss: 0.6963855624198914\n",
            "Iteration: 7081; loss: 0.6941124200820923\n",
            "Iteration: 7082; loss: 0.6944259405136108\n",
            "Iteration: 7083; loss: 0.693080484867096\n",
            "Iteration: 7084; loss: 0.6900172233581543\n",
            "Iteration: 7085; loss: 0.6932334899902344\n",
            "Iteration: 7086; loss: 0.6940135955810547\n",
            "Iteration: 7087; loss: 0.7008382678031921\n",
            "Iteration: 7088; loss: 0.69126296043396\n",
            "Iteration: 7089; loss: 0.6926496624946594\n",
            "Iteration: 7090; loss: 0.6923485398292542\n",
            "Iteration: 7091; loss: 0.6937723755836487\n",
            "Iteration: 7092; loss: 0.6892555356025696\n",
            "Iteration: 7093; loss: 0.6865704655647278\n",
            "Iteration: 7094; loss: 0.6892120838165283\n",
            "Iteration: 7095; loss: 0.6901937127113342\n",
            "Iteration: 7096; loss: 0.6906824111938477\n",
            "Iteration: 7097; loss: 0.6941322088241577\n",
            "Iteration: 7098; loss: 0.6904513239860535\n",
            "Iteration: 7099; loss: 0.6935304999351501\n",
            "Iteration: 7100; loss: 0.6948060989379883\n",
            "Iteration: 7101; loss: 0.7004122734069824\n",
            "Iteration: 7102; loss: 0.6916790008544922\n",
            "Iteration: 7103; loss: 0.6968250274658203\n",
            "Iteration: 7104; loss: 0.6950231790542603\n",
            "Iteration: 7105; loss: 0.694850742816925\n",
            "Iteration: 7106; loss: 0.6864761114120483\n",
            "Iteration: 7107; loss: 0.6896920204162598\n",
            "Iteration: 7108; loss: 0.6924100518226624\n",
            "Iteration: 7109; loss: 0.6890896558761597\n",
            "Iteration: 7110; loss: 0.6941683888435364\n",
            "Iteration: 7111; loss: 0.6894161105155945\n",
            "Iteration: 7112; loss: 0.6895520687103271\n",
            "Iteration: 7113; loss: 0.6875392198562622\n",
            "Iteration: 7114; loss: 0.6871791481971741\n",
            "Iteration: 7115; loss: 0.6902087926864624\n",
            "Iteration: 7116; loss: 0.6948725581169128\n",
            "Iteration: 7117; loss: 0.6913761496543884\n",
            "Iteration: 7118; loss: 0.6925438046455383\n",
            "Iteration: 7119; loss: 0.6912449598312378\n",
            "Iteration: 7120; loss: 0.688007116317749\n",
            "Iteration: 7121; loss: 0.689877986907959\n",
            "Iteration: 7122; loss: 0.6975212693214417\n",
            "Iteration: 7123; loss: 0.6874438524246216\n",
            "Iteration: 7124; loss: 0.695925235748291\n",
            "Iteration: 7125; loss: 0.6898157596588135\n",
            "Iteration: 7126; loss: 0.6936948299407959\n",
            "Iteration: 7127; loss: 0.6956014633178711\n",
            "Iteration: 7128; loss: 0.6916106343269348\n",
            "Iteration: 7129; loss: 0.6969025135040283\n",
            "Iteration: 7130; loss: 0.6887513399124146\n",
            "Iteration: 7131; loss: 0.69673752784729\n",
            "Iteration: 7132; loss: 0.6954858303070068\n",
            "Iteration: 7133; loss: 0.6899895668029785\n",
            "Iteration: 7134; loss: 0.690375804901123\n",
            "Iteration: 7135; loss: 0.6890740394592285\n",
            "Iteration: 7136; loss: 0.6848936080932617\n",
            "Iteration: 7137; loss: 0.6941214799880981\n",
            "Iteration: 7138; loss: 0.698287844657898\n",
            "Iteration: 7139; loss: 0.695549726486206\n",
            "Iteration: 7140; loss: 0.6932830810546875\n",
            "Iteration: 7141; loss: 0.6896873116493225\n",
            "Iteration: 7142; loss: 0.6907275915145874\n",
            "Iteration: 7143; loss: 0.6939281225204468\n",
            "Iteration: 7144; loss: 0.6927719116210938\n",
            "Iteration: 7145; loss: 0.6930155754089355\n",
            "Iteration: 7146; loss: 0.6956560611724854\n",
            "Iteration: 7147; loss: 0.6927351951599121\n",
            "Iteration: 7148; loss: 0.694591760635376\n",
            "Iteration: 7149; loss: 0.6928207278251648\n",
            "Iteration: 7150; loss: 0.691690981388092\n",
            "Iteration: 7151; loss: 0.6978757977485657\n",
            "Iteration: 7152; loss: 0.6958774328231812\n",
            "Iteration: 7153; loss: 0.693548858165741\n",
            "Iteration: 7154; loss: 0.6948318481445312\n",
            "Iteration: 7155; loss: 0.6983644962310791\n",
            "Iteration: 7156; loss: 0.6932339668273926\n",
            "Iteration: 7157; loss: 0.691562294960022\n",
            "Iteration: 7158; loss: 0.6922290325164795\n",
            "Iteration: 7159; loss: 0.6955655813217163\n",
            "Iteration: 7160; loss: 0.6948426365852356\n",
            "Iteration: 7161; loss: 0.6949941515922546\n",
            "Iteration: 7162; loss: 0.6940812468528748\n",
            "Iteration: 7163; loss: 0.6896193027496338\n",
            "Iteration: 7164; loss: 0.6935107707977295\n",
            "Iteration: 7165; loss: 0.6918981075286865\n",
            "Iteration: 7166; loss: 0.6932263374328613\n",
            "Iteration: 7167; loss: 0.6981377005577087\n",
            "Iteration: 7168; loss: 0.6906865835189819\n",
            "Iteration: 7169; loss: 0.687977135181427\n",
            "Iteration: 7170; loss: 0.6858753561973572\n",
            "Iteration: 7171; loss: 0.6878923773765564\n",
            "Iteration: 7172; loss: 0.6865231990814209\n",
            "Iteration: 7173; loss: 0.6885626912117004\n",
            "Iteration: 7174; loss: 0.693751335144043\n",
            "Iteration: 7175; loss: 0.6949763298034668\n",
            "Iteration: 7176; loss: 0.6963854432106018\n",
            "Iteration: 7177; loss: 0.691952645778656\n",
            "Iteration: 7178; loss: 0.6872828602790833\n",
            "Iteration: 7179; loss: 0.6955249905586243\n",
            "Iteration: 7180; loss: 0.6949707269668579\n",
            "Iteration: 7181; loss: 0.6888935565948486\n",
            "Iteration: 7182; loss: 0.6962403059005737\n",
            "Iteration: 7183; loss: 0.6947845816612244\n",
            "Iteration: 7184; loss: 0.6915271282196045\n",
            "Iteration: 7185; loss: 0.6966500878334045\n",
            "Iteration: 7186; loss: 0.6894637942314148\n",
            "Iteration: 7187; loss: 0.6913941502571106\n",
            "Iteration: 7188; loss: 0.6922039985656738\n",
            "Iteration: 7189; loss: 0.6871941685676575\n",
            "Iteration: 7190; loss: 0.6883479952812195\n",
            "Iteration: 7191; loss: 0.6982620358467102\n",
            "Iteration: 7192; loss: 0.6967586874961853\n",
            "Iteration: 7193; loss: 0.6941962242126465\n",
            "Iteration: 7194; loss: 0.6948881149291992\n",
            "Iteration: 7195; loss: 0.6889183521270752\n",
            "Iteration: 7196; loss: 0.6917548179626465\n",
            "Iteration: 7197; loss: 0.6948651671409607\n",
            "Iteration: 7198; loss: 0.6836309432983398\n",
            "Iteration: 7199; loss: 0.6925182342529297\n",
            "Iteration: 7200; loss: 0.6908265948295593\n",
            "Iteration: 7201; loss: 0.7010054588317871\n",
            "Iteration: 7202; loss: 0.6926703453063965\n",
            "Iteration: 7203; loss: 0.6916816234588623\n",
            "Iteration: 7204; loss: 0.6938284635543823\n",
            "Iteration: 7205; loss: 0.6979019045829773\n",
            "Iteration: 7206; loss: 0.687399685382843\n",
            "Iteration: 7207; loss: 0.6956905126571655\n",
            "Iteration: 7208; loss: 0.6953336000442505\n",
            "Iteration: 7209; loss: 0.6922207474708557\n",
            "Iteration: 7210; loss: 0.6880664229393005\n",
            "Iteration: 7211; loss: 0.6948702335357666\n",
            "Iteration: 7212; loss: 0.6915804147720337\n",
            "Iteration: 7213; loss: 0.690748929977417\n",
            "Iteration: 7214; loss: 0.6949955224990845\n",
            "Iteration: 7215; loss: 0.6926655173301697\n",
            "Iteration: 7216; loss: 0.689874529838562\n",
            "Iteration: 7217; loss: 0.6925135254859924\n",
            "Iteration: 7218; loss: 0.6940382719039917\n",
            "Iteration: 7219; loss: 0.6857131123542786\n",
            "Iteration: 7220; loss: 0.692134439945221\n",
            "Iteration: 7221; loss: 0.6905331015586853\n",
            "Iteration: 7222; loss: 0.6894523501396179\n",
            "Iteration: 7223; loss: 0.6924616694450378\n",
            "Iteration: 7224; loss: 0.696897029876709\n",
            "Iteration: 7225; loss: 0.693947434425354\n",
            "Iteration: 7226; loss: 0.6940128207206726\n",
            "Iteration: 7227; loss: 0.6915298104286194\n",
            "Iteration: 7228; loss: 0.6979807019233704\n",
            "Iteration: 7229; loss: 0.6930122971534729\n",
            "Iteration: 7230; loss: 0.6910947561264038\n",
            "Iteration: 7231; loss: 0.6850524544715881\n",
            "Iteration: 7232; loss: 0.6932350993156433\n",
            "Iteration: 7233; loss: 0.687529981136322\n",
            "Iteration: 7234; loss: 0.695991039276123\n",
            "Iteration: 7235; loss: 0.6925302743911743\n",
            "Iteration: 7236; loss: 0.6914244890213013\n",
            "Iteration: 7237; loss: 0.6906481385231018\n",
            "Iteration: 7238; loss: 0.6946156620979309\n",
            "Iteration: 7239; loss: 0.6931855082511902\n",
            "Iteration: 7240; loss: 0.6889099478721619\n",
            "Iteration: 7241; loss: 0.6893507242202759\n",
            "Iteration: 7242; loss: 0.6936668753623962\n",
            "Iteration: 7243; loss: 0.6943683624267578\n",
            "Iteration: 7244; loss: 0.6930657029151917\n",
            "Iteration: 7245; loss: 0.6875296235084534\n",
            "Iteration: 7246; loss: 0.6860924959182739\n",
            "Iteration: 7247; loss: 0.6990398168563843\n",
            "Iteration: 7248; loss: 0.6938528418540955\n",
            "Iteration: 7249; loss: 0.688384473323822\n",
            "Iteration: 7250; loss: 0.6942471265792847\n",
            "Iteration: 7251; loss: 0.6910786628723145\n",
            "Iteration: 7252; loss: 0.6940852999687195\n",
            "Iteration: 7253; loss: 0.6943907141685486\n",
            "Iteration: 7254; loss: 0.6973202228546143\n",
            "Iteration: 7255; loss: 0.6901491284370422\n",
            "Iteration: 7256; loss: 0.6935517191886902\n",
            "Iteration: 7257; loss: 0.6925216913223267\n",
            "Iteration: 7258; loss: 0.6929195523262024\n",
            "Iteration: 7259; loss: 0.6896755695343018\n",
            "Iteration: 7260; loss: 0.6919642686843872\n",
            "Iteration: 7261; loss: 0.6907860636711121\n",
            "Iteration: 7262; loss: 0.6932653188705444\n",
            "Iteration: 7263; loss: 0.6933495998382568\n",
            "Iteration: 7264; loss: 0.6878724694252014\n",
            "Iteration: 7265; loss: 0.6932492256164551\n",
            "Iteration: 7266; loss: 0.6944612264633179\n",
            "Iteration: 7267; loss: 0.6947324872016907\n",
            "Iteration: 7268; loss: 0.6953321099281311\n",
            "Iteration: 7269; loss: 0.6861174702644348\n",
            "Iteration: 7270; loss: 0.6855281591415405\n",
            "Iteration: 7271; loss: 0.6959658861160278\n",
            "Iteration: 7272; loss: 0.6866527199745178\n",
            "Iteration: 7273; loss: 0.6918571591377258\n",
            "Iteration: 7274; loss: 0.6945091485977173\n",
            "Iteration: 7275; loss: 0.6936108469963074\n",
            "Iteration: 7276; loss: 0.6911934614181519\n",
            "Iteration: 7277; loss: 0.6938621997833252\n",
            "Iteration: 7278; loss: 0.6857922673225403\n",
            "Iteration: 7279; loss: 0.6918331384658813\n",
            "Iteration: 7280; loss: 0.6948381662368774\n",
            "Iteration: 7281; loss: 0.6916968822479248\n",
            "Iteration: 7282; loss: 0.6931973695755005\n",
            "Iteration: 7283; loss: 0.6907162666320801\n",
            "Iteration: 7284; loss: 0.6869746446609497\n",
            "Iteration: 7285; loss: 0.6899838447570801\n",
            "Iteration: 7286; loss: 0.6955011487007141\n",
            "Iteration: 7287; loss: 0.6899924278259277\n",
            "Iteration: 7288; loss: 0.6899054050445557\n",
            "Iteration: 7289; loss: 0.6963825225830078\n",
            "Iteration: 7290; loss: 0.6936047077178955\n",
            "Iteration: 7291; loss: 0.6952692270278931\n",
            "Iteration: 7292; loss: 0.6984256505966187\n",
            "Iteration: 7293; loss: 0.6852706670761108\n",
            "Iteration: 7294; loss: 0.6919870376586914\n",
            "Iteration: 7295; loss: 0.694930911064148\n",
            "Iteration: 7296; loss: 0.6969969868659973\n",
            "Iteration: 7297; loss: 0.694874107837677\n",
            "Iteration: 7298; loss: 0.6922397613525391\n",
            "Iteration: 7299; loss: 0.6940175890922546\n",
            "Iteration: 7300; loss: 0.6965851187705994\n",
            "Iteration: 7301; loss: 0.6885401010513306\n",
            "Iteration: 7302; loss: 0.6888784170150757\n",
            "Iteration: 7303; loss: 0.6907232403755188\n",
            "Iteration: 7304; loss: 0.6975538730621338\n",
            "Iteration: 7305; loss: 0.6904767751693726\n",
            "Iteration: 7306; loss: 0.6988086700439453\n",
            "Iteration: 7307; loss: 0.6989010572433472\n",
            "Iteration: 7308; loss: 0.6929569840431213\n",
            "Iteration: 7309; loss: 0.6939106583595276\n",
            "Iteration: 7310; loss: 0.6901144981384277\n",
            "Iteration: 7311; loss: 0.687770664691925\n",
            "Iteration: 7312; loss: 0.6952767968177795\n",
            "Iteration: 7313; loss: 0.6869731545448303\n",
            "Iteration: 7314; loss: 0.6916226148605347\n",
            "Iteration: 7315; loss: 0.6902050375938416\n",
            "Iteration: 7316; loss: 0.6922286748886108\n",
            "Iteration: 7317; loss: 0.6934759020805359\n",
            "Iteration: 7318; loss: 0.694034218788147\n",
            "Iteration: 7319; loss: 0.6945489645004272\n",
            "Iteration: 7320; loss: 0.6909723877906799\n",
            "Iteration: 7321; loss: 0.6951307058334351\n",
            "Iteration: 7322; loss: 0.6938337683677673\n",
            "Iteration: 7323; loss: 0.6974973082542419\n",
            "Iteration: 7324; loss: 0.6940143704414368\n",
            "Iteration: 7325; loss: 0.6913126111030579\n",
            "Iteration: 7326; loss: 0.6911872029304504\n",
            "Iteration: 7327; loss: 0.6912310123443604\n",
            "Iteration: 7328; loss: 0.6951228380203247\n",
            "Iteration: 7329; loss: 0.6912024021148682\n",
            "Iteration: 7330; loss: 0.6903151273727417\n",
            "Iteration: 7331; loss: 0.6890698075294495\n",
            "Iteration: 7332; loss: 0.686489999294281\n",
            "Iteration: 7333; loss: 0.6929543614387512\n",
            "Iteration: 7334; loss: 0.6906963586807251\n",
            "Iteration: 7335; loss: 0.6878191232681274\n",
            "Iteration: 7336; loss: 0.6957852244377136\n",
            "Iteration: 7337; loss: 0.6968344449996948\n",
            "Iteration: 7338; loss: 0.6903479695320129\n",
            "Iteration: 7339; loss: 0.6958411335945129\n",
            "Iteration: 7340; loss: 0.6943367719650269\n",
            "Iteration: 7341; loss: 0.6854902505874634\n",
            "Iteration: 7342; loss: 0.6896713972091675\n",
            "Iteration: 7343; loss: 0.6955861449241638\n",
            "Iteration: 7344; loss: 0.6957983374595642\n",
            "Iteration: 7345; loss: 0.6935704350471497\n",
            "Iteration: 7346; loss: 0.6991311311721802\n",
            "Iteration: 7347; loss: 0.6907753348350525\n",
            "Iteration: 7348; loss: 0.6868789792060852\n",
            "Iteration: 7349; loss: 0.690795361995697\n",
            "Iteration: 7350; loss: 0.6945949196815491\n",
            "Iteration: 7351; loss: 0.6932914853096008\n",
            "Iteration: 7352; loss: 0.687082052230835\n",
            "Iteration: 7353; loss: 0.6963016390800476\n",
            "Iteration: 7354; loss: 0.6855303049087524\n",
            "Iteration: 7355; loss: 0.6861681938171387\n",
            "Iteration: 7356; loss: 0.6917434334754944\n",
            "Iteration: 7357; loss: 0.6929493546485901\n",
            "Iteration: 7358; loss: 0.6886885762214661\n",
            "Iteration: 7359; loss: 0.6920357346534729\n",
            "Iteration: 7360; loss: 0.6979900598526001\n",
            "Iteration: 7361; loss: 0.6917786002159119\n",
            "Iteration: 7362; loss: 0.6899380683898926\n",
            "Iteration: 7363; loss: 0.693720281124115\n",
            "Iteration: 7364; loss: 0.6922552585601807\n",
            "Iteration: 7365; loss: 0.6858509182929993\n",
            "Iteration: 7366; loss: 0.6883144378662109\n",
            "Iteration: 7367; loss: 0.6890639066696167\n",
            "Iteration: 7368; loss: 0.6971929669380188\n",
            "Iteration: 7369; loss: 0.693031907081604\n",
            "Iteration: 7370; loss: 0.6937563419342041\n",
            "Iteration: 7371; loss: 0.6923174858093262\n",
            "Iteration: 7372; loss: 0.6980597972869873\n",
            "Iteration: 7373; loss: 0.6840131282806396\n",
            "Iteration: 7374; loss: 0.6853548288345337\n",
            "Iteration: 7375; loss: 0.6833498477935791\n",
            "Iteration: 7376; loss: 0.6946284770965576\n",
            "Iteration: 7377; loss: 0.6926121711730957\n",
            "Iteration: 7378; loss: 0.6882382035255432\n",
            "Iteration: 7379; loss: 0.6949113011360168\n",
            "Iteration: 7380; loss: 0.6857757568359375\n",
            "Iteration: 7381; loss: 0.6912142038345337\n",
            "Iteration: 7382; loss: 0.6913195252418518\n",
            "Iteration: 7383; loss: 0.6932786703109741\n",
            "Iteration: 7384; loss: 0.6921279430389404\n",
            "Iteration: 7385; loss: 0.6873852610588074\n",
            "Iteration: 7386; loss: 0.6925528049468994\n",
            "Iteration: 7387; loss: 0.7007859945297241\n",
            "Iteration: 7388; loss: 0.691638708114624\n",
            "Iteration: 7389; loss: 0.6949626207351685\n",
            "Iteration: 7390; loss: 0.6941669583320618\n",
            "Iteration: 7391; loss: 0.689005970954895\n",
            "Iteration: 7392; loss: 0.698810338973999\n",
            "Iteration: 7393; loss: 0.6995812654495239\n",
            "Iteration: 7394; loss: 0.6865997314453125\n",
            "Iteration: 7395; loss: 0.68784499168396\n",
            "Iteration: 7396; loss: 0.6938858032226562\n",
            "Iteration: 7397; loss: 0.6868622303009033\n",
            "Iteration: 7398; loss: 0.6851621866226196\n",
            "Iteration: 7399; loss: 0.688163697719574\n",
            "Iteration: 7400; loss: 0.6937696933746338\n",
            "Iteration: 7401; loss: 0.685291588306427\n",
            "Iteration: 7402; loss: 0.6955991983413696\n",
            "Iteration: 7403; loss: 0.6888343691825867\n",
            "Iteration: 7404; loss: 0.6959093809127808\n",
            "Iteration: 7405; loss: 0.6878077387809753\n",
            "Iteration: 7406; loss: 0.6932114362716675\n",
            "Iteration: 7407; loss: 0.6842342615127563\n",
            "Iteration: 7408; loss: 0.7003830671310425\n",
            "Iteration: 7409; loss: 0.6999548077583313\n",
            "Iteration: 7410; loss: 0.685962975025177\n",
            "Iteration: 7411; loss: 0.6875801682472229\n",
            "Iteration: 7412; loss: 0.6841756105422974\n",
            "Iteration: 7413; loss: 0.6911492943763733\n",
            "Iteration: 7414; loss: 0.6912858486175537\n",
            "Iteration: 7415; loss: 0.6945292949676514\n",
            "Iteration: 7416; loss: 0.691929817199707\n",
            "Iteration: 7417; loss: 0.6889786720275879\n",
            "Iteration: 7418; loss: 0.6917288303375244\n",
            "Iteration: 7419; loss: 0.6880140900611877\n",
            "Iteration: 7420; loss: 0.6890857815742493\n",
            "Iteration: 7421; loss: 0.6888449192047119\n",
            "Iteration: 7422; loss: 0.6873252987861633\n",
            "Iteration: 7423; loss: 0.6900434494018555\n",
            "Iteration: 7424; loss: 0.6864889860153198\n",
            "Iteration: 7425; loss: 0.6911285519599915\n",
            "Iteration: 7426; loss: 0.684792697429657\n",
            "Iteration: 7427; loss: 0.6903929710388184\n",
            "Iteration: 7428; loss: 0.6933143734931946\n",
            "Iteration: 7429; loss: 0.6922531723976135\n",
            "Iteration: 7430; loss: 0.6912527084350586\n",
            "Iteration: 7431; loss: 0.6941198110580444\n",
            "Iteration: 7432; loss: 0.6836785078048706\n",
            "Iteration: 7433; loss: 0.684194028377533\n",
            "Iteration: 7434; loss: 0.6938750743865967\n",
            "Iteration: 7435; loss: 0.6881179213523865\n",
            "Iteration: 7436; loss: 0.695438027381897\n",
            "Iteration: 7437; loss: 0.6870728135108948\n",
            "Iteration: 7438; loss: 0.6926557421684265\n",
            "Iteration: 7439; loss: 0.6967417597770691\n",
            "Iteration: 7440; loss: 0.6933020353317261\n",
            "Iteration: 7441; loss: 0.6911565661430359\n",
            "Iteration: 7442; loss: 0.6939855813980103\n",
            "Iteration: 7443; loss: 0.6894361972808838\n",
            "Iteration: 7444; loss: 0.6874555349349976\n",
            "Iteration: 7445; loss: 0.6877346038818359\n",
            "Iteration: 7446; loss: 0.6982753276824951\n",
            "Iteration: 7447; loss: 0.6970945000648499\n",
            "Iteration: 7448; loss: 0.6935973763465881\n",
            "Iteration: 7449; loss: 0.6865391731262207\n",
            "Iteration: 7450; loss: 0.6895567774772644\n",
            "Iteration: 7451; loss: 0.6855952739715576\n",
            "Iteration: 7452; loss: 0.6900018453598022\n",
            "Iteration: 7453; loss: 0.6907883286476135\n",
            "Iteration: 7454; loss: 0.6915338039398193\n",
            "Iteration: 7455; loss: 0.7007813453674316\n",
            "Iteration: 7456; loss: 0.689453661441803\n",
            "Iteration: 7457; loss: 0.6858999729156494\n",
            "Iteration: 7458; loss: 0.6859222650527954\n",
            "Iteration: 7459; loss: 0.681282103061676\n",
            "Iteration: 7460; loss: 0.688101053237915\n",
            "Iteration: 7461; loss: 0.6832403540611267\n",
            "Iteration: 7462; loss: 0.6878945827484131\n",
            "Iteration: 7463; loss: 0.6909157037734985\n",
            "Iteration: 7464; loss: 0.6933475732803345\n",
            "Iteration: 7465; loss: 0.6890501379966736\n",
            "Iteration: 7466; loss: 0.6970925331115723\n",
            "Iteration: 7467; loss: 0.6932523846626282\n",
            "Iteration: 7468; loss: 0.687074601650238\n",
            "Iteration: 7469; loss: 0.6939551830291748\n",
            "Iteration: 7470; loss: 0.6961190104484558\n",
            "Iteration: 7471; loss: 0.6894091963768005\n",
            "Iteration: 7472; loss: 0.694088339805603\n",
            "Iteration: 7473; loss: 0.6924142241477966\n",
            "Iteration: 7474; loss: 0.683401346206665\n",
            "Iteration: 7475; loss: 0.6994155645370483\n",
            "Iteration: 7476; loss: 0.6930152177810669\n",
            "Iteration: 7477; loss: 0.684895932674408\n",
            "Iteration: 7478; loss: 0.6887813210487366\n",
            "Iteration: 7479; loss: 0.687706470489502\n",
            "Iteration: 7480; loss: 0.6947318911552429\n",
            "Iteration: 7481; loss: 0.6954578161239624\n",
            "Iteration: 7482; loss: 0.6883387565612793\n",
            "Iteration: 7483; loss: 0.6894267797470093\n",
            "Iteration: 7484; loss: 0.6934303045272827\n",
            "Iteration: 7485; loss: 0.7000190019607544\n",
            "Iteration: 7486; loss: 0.7005637884140015\n",
            "Iteration: 7487; loss: 0.6918447017669678\n",
            "Iteration: 7488; loss: 0.6917720437049866\n",
            "Iteration: 7489; loss: 0.6834994554519653\n",
            "Iteration: 7490; loss: 0.6909857988357544\n",
            "Iteration: 7491; loss: 0.6879208087921143\n",
            "Iteration: 7492; loss: 0.6908693313598633\n",
            "Iteration: 7493; loss: 0.6894770860671997\n",
            "Iteration: 7494; loss: 0.6874513030052185\n",
            "Iteration: 7495; loss: 0.6893118023872375\n",
            "Iteration: 7496; loss: 0.6903319954872131\n",
            "Iteration: 7497; loss: 0.6913406252861023\n",
            "Iteration: 7498; loss: 0.6839008331298828\n",
            "Iteration: 7499; loss: 0.685117244720459\n",
            "Iteration: 7500; loss: 0.6908486485481262\n",
            "Iteration: 7501; loss: 0.6957724690437317\n",
            "Iteration: 7502; loss: 0.6912644505500793\n",
            "Iteration: 7503; loss: 0.6905419230461121\n",
            "Iteration: 7504; loss: 0.6883339881896973\n",
            "Iteration: 7505; loss: 0.6901686191558838\n",
            "Iteration: 7506; loss: 0.696590006351471\n",
            "Iteration: 7507; loss: 0.693284809589386\n",
            "Iteration: 7508; loss: 0.6924673914909363\n",
            "Iteration: 7509; loss: 0.6847353577613831\n",
            "Iteration: 7510; loss: 0.6874734163284302\n",
            "Iteration: 7511; loss: 0.6823135614395142\n",
            "Iteration: 7512; loss: 0.6847183704376221\n",
            "Iteration: 7513; loss: 0.6937691569328308\n",
            "Iteration: 7514; loss: 0.686633825302124\n",
            "Iteration: 7515; loss: 0.6930544972419739\n",
            "Iteration: 7516; loss: 0.6935250163078308\n",
            "Iteration: 7517; loss: 0.6930227279663086\n",
            "Iteration: 7518; loss: 0.6897337436676025\n",
            "Iteration: 7519; loss: 0.6971014142036438\n",
            "Iteration: 7520; loss: 0.6907467842102051\n",
            "Iteration: 7521; loss: 0.6847895979881287\n",
            "Iteration: 7522; loss: 0.6866737008094788\n",
            "Iteration: 7523; loss: 0.6793882846832275\n",
            "Iteration: 7524; loss: 0.6911963224411011\n",
            "Iteration: 7525; loss: 0.6884410381317139\n",
            "Iteration: 7526; loss: 0.6911134719848633\n",
            "Iteration: 7527; loss: 0.6974380016326904\n",
            "Iteration: 7528; loss: 0.6945416927337646\n",
            "Iteration: 7529; loss: 0.6998794674873352\n",
            "Iteration: 7530; loss: 0.6903344988822937\n",
            "Iteration: 7531; loss: 0.6977782249450684\n",
            "Iteration: 7532; loss: 0.6966822743415833\n",
            "Iteration: 7533; loss: 0.6883670687675476\n",
            "Iteration: 7534; loss: 0.695918619632721\n",
            "Iteration: 7535; loss: 0.6866238117218018\n",
            "Iteration: 7536; loss: 0.6858891844749451\n",
            "Iteration: 7537; loss: 0.6909895539283752\n",
            "Iteration: 7538; loss: 0.6771534085273743\n",
            "Iteration: 7539; loss: 0.6784392595291138\n",
            "Iteration: 7540; loss: 0.6916279792785645\n",
            "Iteration: 7541; loss: 0.6953601837158203\n",
            "Iteration: 7542; loss: 0.6885477900505066\n",
            "Iteration: 7543; loss: 0.691818356513977\n",
            "Iteration: 7544; loss: 0.6934393644332886\n",
            "Iteration: 7545; loss: 0.6914362907409668\n",
            "Iteration: 7546; loss: 0.6900140047073364\n",
            "Iteration: 7547; loss: 0.6779113411903381\n",
            "Iteration: 7548; loss: 0.6985833644866943\n",
            "Iteration: 7549; loss: 0.6882424354553223\n",
            "Iteration: 7550; loss: 0.691461443901062\n",
            "Iteration: 7551; loss: 0.6888700723648071\n",
            "Iteration: 7552; loss: 0.6913605332374573\n",
            "Iteration: 7553; loss: 0.6862174868583679\n",
            "Iteration: 7554; loss: 0.6879906058311462\n",
            "Iteration: 7555; loss: 0.700442373752594\n",
            "Iteration: 7556; loss: 0.6881484389305115\n",
            "Iteration: 7557; loss: 0.6963686943054199\n",
            "Iteration: 7558; loss: 0.6879521012306213\n",
            "Iteration: 7559; loss: 0.6877233982086182\n",
            "Iteration: 7560; loss: 0.6842899918556213\n",
            "Iteration: 7561; loss: 0.6862765550613403\n",
            "Iteration: 7562; loss: 0.6923920512199402\n",
            "Iteration: 7563; loss: 0.6939309239387512\n",
            "Iteration: 7564; loss: 0.6809610724449158\n",
            "Iteration: 7565; loss: 0.6817449331283569\n",
            "Iteration: 7566; loss: 0.6918954849243164\n",
            "Iteration: 7567; loss: 0.6978734731674194\n",
            "Iteration: 7568; loss: 0.6933509111404419\n",
            "Iteration: 7569; loss: 0.6895646452903748\n",
            "Iteration: 7570; loss: 0.6928738951683044\n",
            "Iteration: 7571; loss: 0.686384379863739\n",
            "Iteration: 7572; loss: 0.6886637210845947\n",
            "Iteration: 7573; loss: 0.6923624873161316\n",
            "Iteration: 7574; loss: 0.6900416612625122\n",
            "Iteration: 7575; loss: 0.6827594637870789\n",
            "Iteration: 7576; loss: 0.6921768188476562\n",
            "Iteration: 7577; loss: 0.6908666491508484\n",
            "Iteration: 7578; loss: 0.6946433782577515\n",
            "Iteration: 7579; loss: 0.6885095834732056\n",
            "Iteration: 7580; loss: 0.6923023462295532\n",
            "Iteration: 7581; loss: 0.687657356262207\n",
            "Iteration: 7582; loss: 0.6944929361343384\n",
            "Iteration: 7583; loss: 0.6947433352470398\n",
            "Iteration: 7584; loss: 0.6779794692993164\n",
            "Iteration: 7585; loss: 0.6888216733932495\n",
            "Iteration: 7586; loss: 0.6998946070671082\n",
            "Iteration: 7587; loss: 0.6990476846694946\n",
            "Iteration: 7588; loss: 0.680664598941803\n",
            "Iteration: 7589; loss: 0.6999291181564331\n",
            "Iteration: 7590; loss: 0.6977064609527588\n",
            "Iteration: 7591; loss: 0.6897624731063843\n",
            "Iteration: 7592; loss: 0.6798025369644165\n",
            "Iteration: 7593; loss: 0.6857591271400452\n",
            "Iteration: 7594; loss: 0.6959366798400879\n",
            "Iteration: 7595; loss: 0.6991028189659119\n",
            "Iteration: 7596; loss: 0.6877892017364502\n",
            "Iteration: 7597; loss: 0.688033401966095\n",
            "Iteration: 7598; loss: 0.6909540891647339\n",
            "Iteration: 7599; loss: 0.685967206954956\n",
            "Iteration: 7600; loss: 0.6882171034812927\n",
            "Iteration: 7601; loss: 0.6907868981361389\n",
            "Iteration: 7602; loss: 0.6908866167068481\n",
            "Iteration: 7603; loss: 0.6843155026435852\n",
            "Iteration: 7604; loss: 0.6909698843955994\n",
            "Iteration: 7605; loss: 0.6889757513999939\n",
            "Iteration: 7606; loss: 0.6827349662780762\n",
            "Iteration: 7607; loss: 0.6835454702377319\n",
            "Iteration: 7608; loss: 0.6884087324142456\n",
            "Iteration: 7609; loss: 0.6900002956390381\n",
            "Iteration: 7610; loss: 0.6871021389961243\n",
            "Iteration: 7611; loss: 0.6844450831413269\n",
            "Iteration: 7612; loss: 0.6907815337181091\n",
            "Iteration: 7613; loss: 0.6878334283828735\n",
            "Iteration: 7614; loss: 0.6861929297447205\n",
            "Iteration: 7615; loss: 0.6962473392486572\n",
            "Iteration: 7616; loss: 0.6888685822486877\n",
            "Iteration: 7617; loss: 0.677176296710968\n",
            "Iteration: 7618; loss: 0.6959919333457947\n",
            "Iteration: 7619; loss: 0.6988118886947632\n",
            "Iteration: 7620; loss: 0.6929764747619629\n",
            "Iteration: 7621; loss: 0.6877685785293579\n",
            "Iteration: 7622; loss: 0.6791719198226929\n",
            "Iteration: 7623; loss: 0.6823931336402893\n",
            "Iteration: 7624; loss: 0.6949795484542847\n",
            "Iteration: 7625; loss: 0.6889222860336304\n",
            "Iteration: 7626; loss: 0.697213888168335\n",
            "Iteration: 7627; loss: 0.6937787532806396\n",
            "Iteration: 7628; loss: 0.6785050630569458\n",
            "Iteration: 7629; loss: 0.691889762878418\n",
            "Iteration: 7630; loss: 0.6996522545814514\n",
            "Iteration: 7631; loss: 0.687325656414032\n",
            "Iteration: 7632; loss: 0.6973940134048462\n",
            "Iteration: 7633; loss: 0.6935418844223022\n",
            "Iteration: 7634; loss: 0.6963649392127991\n",
            "Iteration: 7635; loss: 0.6863679885864258\n",
            "Iteration: 7636; loss: 0.6935974955558777\n",
            "Iteration: 7637; loss: 0.6938762664794922\n",
            "Iteration: 7638; loss: 0.683637797832489\n",
            "Iteration: 7639; loss: 0.6990431547164917\n",
            "Iteration: 7640; loss: 0.6915885806083679\n",
            "Iteration: 7641; loss: 0.6915371417999268\n",
            "Iteration: 7642; loss: 0.6818838119506836\n",
            "Iteration: 7643; loss: 0.6882129311561584\n",
            "Iteration: 7644; loss: 0.6795089244842529\n",
            "Iteration: 7645; loss: 0.6906204223632812\n",
            "Iteration: 7646; loss: 0.6893473267555237\n",
            "Iteration: 7647; loss: 0.6842479705810547\n",
            "Iteration: 7648; loss: 0.6971537470817566\n",
            "Iteration: 7649; loss: 0.679423451423645\n",
            "Iteration: 7650; loss: 0.696386992931366\n",
            "Iteration: 7651; loss: 0.6978960037231445\n",
            "Iteration: 7652; loss: 0.6899236440658569\n",
            "Iteration: 7653; loss: 0.6783647537231445\n",
            "Iteration: 7654; loss: 0.6902109980583191\n",
            "Iteration: 7655; loss: 0.6845167875289917\n",
            "Iteration: 7656; loss: 0.6870542168617249\n",
            "Iteration: 7657; loss: 0.6941719651222229\n",
            "Iteration: 7658; loss: 0.6973297595977783\n",
            "Iteration: 7659; loss: 0.6950390338897705\n",
            "Iteration: 7660; loss: 0.6987051367759705\n",
            "Iteration: 7661; loss: 0.674392580986023\n",
            "Iteration: 7662; loss: 0.6875956058502197\n",
            "Iteration: 7663; loss: 0.6859121322631836\n",
            "Iteration: 7664; loss: 0.6821122765541077\n",
            "Iteration: 7665; loss: 0.6833271384239197\n",
            "Iteration: 7666; loss: 0.6857852935791016\n",
            "Iteration: 7667; loss: 0.6841676831245422\n",
            "Iteration: 7668; loss: 0.6877648234367371\n",
            "Iteration: 7669; loss: 0.6940920948982239\n",
            "Iteration: 7670; loss: 0.6838002800941467\n",
            "Iteration: 7671; loss: 0.7051962614059448\n",
            "Iteration: 7672; loss: 0.6852408051490784\n",
            "Iteration: 7673; loss: 0.6949853897094727\n",
            "Iteration: 7674; loss: 0.6822062730789185\n",
            "Iteration: 7675; loss: 0.6910940408706665\n",
            "Iteration: 7676; loss: 0.6982844471931458\n",
            "Iteration: 7677; loss: 0.6913980841636658\n",
            "Iteration: 7678; loss: 0.688840925693512\n",
            "Iteration: 7679; loss: 0.6858086585998535\n",
            "Iteration: 7680; loss: 0.6912407875061035\n",
            "Iteration: 7681; loss: 0.691688060760498\n",
            "Iteration: 7682; loss: 0.6873394846916199\n",
            "Iteration: 7683; loss: 0.6994215846061707\n",
            "Iteration: 7684; loss: 0.6855388879776001\n",
            "Iteration: 7685; loss: 0.6909840703010559\n",
            "Iteration: 7686; loss: 0.6989737749099731\n",
            "Iteration: 7687; loss: 0.6959641575813293\n",
            "Iteration: 7688; loss: 0.6790841817855835\n",
            "Iteration: 7689; loss: 0.6908671259880066\n",
            "Iteration: 7690; loss: 0.6922849416732788\n",
            "Iteration: 7691; loss: 0.6946617364883423\n",
            "Iteration: 7692; loss: 0.6859267950057983\n",
            "Iteration: 7693; loss: 0.6904833912849426\n",
            "Iteration: 7694; loss: 0.6882961988449097\n",
            "Iteration: 7695; loss: 0.6986679434776306\n",
            "Iteration: 7696; loss: 0.6939579248428345\n",
            "Iteration: 7697; loss: 0.6940041184425354\n",
            "Iteration: 7698; loss: 0.7002036571502686\n",
            "Iteration: 7699; loss: 0.688790500164032\n",
            "Iteration: 7700; loss: 0.690127968788147\n",
            "Iteration: 7701; loss: 0.689206600189209\n",
            "Iteration: 7702; loss: 0.694052517414093\n",
            "Iteration: 7703; loss: 0.6946532726287842\n",
            "Iteration: 7704; loss: 0.6878787875175476\n",
            "Iteration: 7705; loss: 0.6869910955429077\n",
            "Iteration: 7706; loss: 0.6850037574768066\n",
            "Iteration: 7707; loss: 0.687756359577179\n",
            "Iteration: 7708; loss: 0.6860617399215698\n",
            "Iteration: 7709; loss: 0.689649760723114\n",
            "Iteration: 7710; loss: 0.6911938190460205\n",
            "Iteration: 7711; loss: 0.6896945834159851\n",
            "Iteration: 7712; loss: 0.6902585029602051\n",
            "Iteration: 7713; loss: 0.6930840015411377\n",
            "Iteration: 7714; loss: 0.6870148777961731\n",
            "Iteration: 7715; loss: 0.6976445317268372\n",
            "Iteration: 7716; loss: 0.6983238458633423\n",
            "Iteration: 7717; loss: 0.6824589967727661\n",
            "Iteration: 7718; loss: 0.6796221733093262\n",
            "Iteration: 7719; loss: 0.6860485076904297\n",
            "Iteration: 7720; loss: 0.6898182034492493\n",
            "Iteration: 7721; loss: 0.7003095149993896\n",
            "Iteration: 7722; loss: 0.6890518665313721\n",
            "Iteration: 7723; loss: 0.6966065168380737\n",
            "Iteration: 7724; loss: 0.6903485059738159\n",
            "Iteration: 7725; loss: 0.6913971900939941\n",
            "Iteration: 7726; loss: 0.6982072591781616\n",
            "Iteration: 7727; loss: 0.6855307221412659\n",
            "Iteration: 7728; loss: 0.6846473813056946\n",
            "Iteration: 7729; loss: 0.6766865253448486\n",
            "Iteration: 7730; loss: 0.6795815825462341\n",
            "Iteration: 7731; loss: 0.6817435026168823\n",
            "Iteration: 7732; loss: 0.6937215328216553\n",
            "Iteration: 7733; loss: 0.6949533820152283\n",
            "Iteration: 7734; loss: 0.690243661403656\n",
            "Iteration: 7735; loss: 0.6806048154830933\n",
            "Iteration: 7736; loss: 0.6891921758651733\n",
            "Iteration: 7737; loss: 0.690076470375061\n",
            "Iteration: 7738; loss: 0.6954945921897888\n",
            "Iteration: 7739; loss: 0.6859790682792664\n",
            "Iteration: 7740; loss: 0.6941571235656738\n",
            "Iteration: 7741; loss: 0.6920500993728638\n",
            "Iteration: 7742; loss: 0.6882094740867615\n",
            "Iteration: 7743; loss: 0.6805380582809448\n",
            "Iteration: 7744; loss: 0.6956437826156616\n",
            "Iteration: 7745; loss: 0.6996670961380005\n",
            "Iteration: 7746; loss: 0.6998381018638611\n",
            "Iteration: 7747; loss: 0.6885471343994141\n",
            "Iteration: 7748; loss: 0.6902129650115967\n",
            "Iteration: 7749; loss: 0.6884697079658508\n",
            "Iteration: 7750; loss: 0.6951093673706055\n",
            "Iteration: 7751; loss: 0.6837280988693237\n",
            "Iteration: 7752; loss: 0.6807993650436401\n",
            "Iteration: 7753; loss: 0.6964719891548157\n",
            "Iteration: 7754; loss: 0.694698691368103\n",
            "Iteration: 7755; loss: 0.6947128772735596\n",
            "Iteration: 7756; loss: 0.6880828142166138\n",
            "Iteration: 7757; loss: 0.6780461072921753\n",
            "Iteration: 7758; loss: 0.6825293898582458\n",
            "Iteration: 7759; loss: 0.6834136247634888\n",
            "Iteration: 7760; loss: 0.6853787899017334\n",
            "Iteration: 7761; loss: 0.686400830745697\n",
            "Iteration: 7762; loss: 0.6829052567481995\n",
            "Iteration: 7763; loss: 0.6971398591995239\n",
            "Iteration: 7764; loss: 0.6780815720558167\n",
            "Iteration: 7765; loss: 0.6932364702224731\n",
            "Iteration: 7766; loss: 0.6858611106872559\n",
            "Iteration: 7767; loss: 0.6823731064796448\n",
            "Iteration: 7768; loss: 0.6889868378639221\n",
            "Iteration: 7769; loss: 0.6945905089378357\n",
            "Iteration: 7770; loss: 0.6849216222763062\n",
            "Iteration: 7771; loss: 0.6867700815200806\n",
            "Iteration: 7772; loss: 0.6964474320411682\n",
            "Iteration: 7773; loss: 0.6880027651786804\n",
            "Iteration: 7774; loss: 0.682631254196167\n",
            "Iteration: 7775; loss: 0.6955922842025757\n",
            "Iteration: 7776; loss: 0.6863870024681091\n",
            "Iteration: 7777; loss: 0.6818464398384094\n",
            "Iteration: 7778; loss: 0.6808673739433289\n",
            "Iteration: 7779; loss: 0.6864974498748779\n",
            "Iteration: 7780; loss: 0.694807767868042\n",
            "Iteration: 7781; loss: 0.6882972121238708\n",
            "Iteration: 7782; loss: 0.6855438351631165\n",
            "Iteration: 7783; loss: 0.7003995180130005\n",
            "Iteration: 7784; loss: 0.6996180415153503\n",
            "Iteration: 7785; loss: 0.6777740716934204\n",
            "Iteration: 7786; loss: 0.6741020679473877\n",
            "Iteration: 7787; loss: 0.6993331909179688\n",
            "Iteration: 7788; loss: 0.6908739805221558\n",
            "Iteration: 7789; loss: 0.6926419138908386\n",
            "Iteration: 7790; loss: 0.6935816407203674\n",
            "Iteration: 7791; loss: 0.6835513114929199\n",
            "Iteration: 7792; loss: 0.6953281164169312\n",
            "Iteration: 7793; loss: 0.6844128370285034\n",
            "Iteration: 7794; loss: 0.6866488456726074\n",
            "Iteration: 7795; loss: 0.6877075433731079\n",
            "Iteration: 7796; loss: 0.6802486181259155\n",
            "Iteration: 7797; loss: 0.6823961734771729\n",
            "Iteration: 7798; loss: 0.6901984214782715\n",
            "Iteration: 7799; loss: 0.688646137714386\n",
            "Iteration: 7800; loss: 0.6877608895301819\n",
            "Iteration: 7801; loss: 0.6869160532951355\n",
            "Iteration: 7802; loss: 0.6807246804237366\n",
            "Iteration: 7803; loss: 0.6842398643493652\n",
            "Iteration: 7804; loss: 0.6876957416534424\n",
            "Iteration: 7805; loss: 0.6977810263633728\n",
            "Iteration: 7806; loss: 0.6930968165397644\n",
            "Iteration: 7807; loss: 0.6858403086662292\n",
            "Iteration: 7808; loss: 0.6933241486549377\n",
            "Iteration: 7809; loss: 0.6865454912185669\n",
            "Iteration: 7810; loss: 0.6797502636909485\n",
            "Iteration: 7811; loss: 0.6860524415969849\n",
            "Iteration: 7812; loss: 0.6840721964836121\n",
            "Iteration: 7813; loss: 0.6861191987991333\n",
            "Iteration: 7814; loss: 0.6796978116035461\n",
            "Iteration: 7815; loss: 0.6746997237205505\n",
            "Iteration: 7816; loss: 0.6908912658691406\n",
            "Iteration: 7817; loss: 0.6819402575492859\n",
            "Iteration: 7818; loss: 0.692575216293335\n",
            "Iteration: 7819; loss: 0.6909573078155518\n",
            "Iteration: 7820; loss: 0.6940678954124451\n",
            "Iteration: 7821; loss: 0.678313672542572\n",
            "Iteration: 7822; loss: 0.6904416680335999\n",
            "Iteration: 7823; loss: 0.6920756697654724\n",
            "Iteration: 7824; loss: 0.6932200193405151\n",
            "Iteration: 7825; loss: 0.692833662033081\n",
            "Iteration: 7826; loss: 0.6951789259910583\n",
            "Iteration: 7827; loss: 0.6919550895690918\n",
            "Iteration: 7828; loss: 0.6925548315048218\n",
            "Iteration: 7829; loss: 0.6818809509277344\n",
            "Iteration: 7830; loss: 0.6801118850708008\n",
            "Iteration: 7831; loss: 0.6923173666000366\n",
            "Iteration: 7832; loss: 0.7022483348846436\n",
            "Iteration: 7833; loss: 0.693494975566864\n",
            "Iteration: 7834; loss: 0.6831263303756714\n",
            "Iteration: 7835; loss: 0.6980968713760376\n",
            "Iteration: 7836; loss: 0.6877050399780273\n",
            "Iteration: 7837; loss: 0.6883026361465454\n",
            "Iteration: 7838; loss: 0.6929138898849487\n",
            "Iteration: 7839; loss: 0.7019979953765869\n",
            "Iteration: 7840; loss: 0.6934775114059448\n",
            "Iteration: 7841; loss: 0.6921988725662231\n",
            "Iteration: 7842; loss: 0.6937256455421448\n",
            "Iteration: 7843; loss: 0.6883538961410522\n",
            "Iteration: 7844; loss: 0.6919264197349548\n",
            "Iteration: 7845; loss: 0.6690148115158081\n",
            "Iteration: 7846; loss: 0.6925182938575745\n",
            "Iteration: 7847; loss: 0.6874880790710449\n",
            "Iteration: 7848; loss: 0.6926940679550171\n",
            "Iteration: 7849; loss: 0.690989077091217\n",
            "Iteration: 7850; loss: 0.6957739591598511\n",
            "Iteration: 7851; loss: 0.6859039664268494\n",
            "Iteration: 7852; loss: 0.6801539659500122\n",
            "Iteration: 7853; loss: 0.685711145401001\n",
            "Iteration: 7854; loss: 0.6975753307342529\n",
            "Iteration: 7855; loss: 0.6798670291900635\n",
            "Iteration: 7856; loss: 0.6798890233039856\n",
            "Iteration: 7857; loss: 0.6880221962928772\n",
            "Iteration: 7858; loss: 0.6917451620101929\n",
            "Iteration: 7859; loss: 0.6804885864257812\n",
            "Iteration: 7860; loss: 0.6931676864624023\n",
            "Iteration: 7861; loss: 0.6850078701972961\n",
            "Iteration: 7862; loss: 0.6953726410865784\n",
            "Iteration: 7863; loss: 0.6814174056053162\n",
            "Iteration: 7864; loss: 0.6955574750900269\n",
            "Iteration: 7865; loss: 0.6792514324188232\n",
            "Iteration: 7866; loss: 0.6832292675971985\n",
            "Iteration: 7867; loss: 0.6925036311149597\n",
            "Iteration: 7868; loss: 0.6759821772575378\n",
            "Iteration: 7869; loss: 0.6975551843643188\n",
            "Iteration: 7870; loss: 0.6916672587394714\n",
            "Iteration: 7871; loss: 0.6861763000488281\n",
            "Iteration: 7872; loss: 0.6898534297943115\n",
            "Iteration: 7873; loss: 0.6776126027107239\n",
            "Iteration: 7874; loss: 0.6958915591239929\n",
            "Iteration: 7875; loss: 0.6962985396385193\n",
            "Iteration: 7876; loss: 0.6887474060058594\n",
            "Iteration: 7877; loss: 0.6810808777809143\n",
            "Iteration: 7878; loss: 0.6882235407829285\n",
            "Iteration: 7879; loss: 0.6785374879837036\n",
            "Iteration: 7880; loss: 0.6781912446022034\n",
            "Iteration: 7881; loss: 0.6887506246566772\n",
            "Iteration: 7882; loss: 0.6806894540786743\n",
            "Iteration: 7883; loss: 0.6749102473258972\n",
            "Iteration: 7884; loss: 0.6937786340713501\n",
            "Iteration: 7885; loss: 0.6854507327079773\n",
            "Iteration: 7886; loss: 0.6850398778915405\n",
            "Iteration: 7887; loss: 0.6816650032997131\n",
            "Iteration: 7888; loss: 0.6875860691070557\n",
            "Iteration: 7889; loss: 0.698820948600769\n",
            "Iteration: 7890; loss: 0.6875954270362854\n",
            "Iteration: 7891; loss: 0.6897315382957458\n",
            "Iteration: 7892; loss: 0.6890666484832764\n",
            "Iteration: 7893; loss: 0.6918528079986572\n",
            "Iteration: 7894; loss: 0.6820128560066223\n",
            "Iteration: 7895; loss: 0.6807560920715332\n",
            "Iteration: 7896; loss: 0.6841316819190979\n",
            "Iteration: 7897; loss: 0.6708266139030457\n",
            "Iteration: 7898; loss: 0.7000307440757751\n",
            "Iteration: 7899; loss: 0.6857758164405823\n",
            "Iteration: 7900; loss: 0.6828267574310303\n",
            "Iteration: 7901; loss: 0.6844501495361328\n",
            "Iteration: 7902; loss: 0.6751796007156372\n",
            "Iteration: 7903; loss: 0.7025675177574158\n",
            "Iteration: 7904; loss: 0.7033061981201172\n",
            "Iteration: 7905; loss: 0.6970669627189636\n",
            "Iteration: 7906; loss: 0.6878840327262878\n",
            "Iteration: 7907; loss: 0.6898442506790161\n",
            "Iteration: 7908; loss: 0.6901209354400635\n",
            "Iteration: 7909; loss: 0.6910996437072754\n",
            "Iteration: 7910; loss: 0.6835174560546875\n",
            "Iteration: 7911; loss: 0.6862136721611023\n",
            "Iteration: 7912; loss: 0.6856161952018738\n",
            "Iteration: 7913; loss: 0.6948349475860596\n",
            "Iteration: 7914; loss: 0.6917756199836731\n",
            "Iteration: 7915; loss: 0.680313766002655\n",
            "Iteration: 7916; loss: 0.671403706073761\n",
            "Iteration: 7917; loss: 0.6859837770462036\n",
            "Iteration: 7918; loss: 0.681294322013855\n",
            "Iteration: 7919; loss: 0.6795670390129089\n",
            "Iteration: 7920; loss: 0.6914035081863403\n",
            "Iteration: 7921; loss: 0.6974920630455017\n",
            "Iteration: 7922; loss: 0.6817941069602966\n",
            "Iteration: 7923; loss: 0.6807485222816467\n",
            "Iteration: 7924; loss: 0.6985347270965576\n",
            "Iteration: 7925; loss: 0.6897174715995789\n",
            "Iteration: 7926; loss: 0.6789745092391968\n",
            "Iteration: 7927; loss: 0.6743832230567932\n",
            "Iteration: 7928; loss: 0.6894707083702087\n",
            "Iteration: 7929; loss: 0.6804496049880981\n",
            "Iteration: 7930; loss: 0.6796913146972656\n",
            "Iteration: 7931; loss: 0.680921196937561\n",
            "Iteration: 7932; loss: 0.6727136373519897\n",
            "Iteration: 7933; loss: 0.6806066036224365\n",
            "Iteration: 7934; loss: 0.6946444511413574\n",
            "Iteration: 7935; loss: 0.6980125904083252\n",
            "Iteration: 7936; loss: 0.6843083500862122\n",
            "Iteration: 7937; loss: 0.6772009134292603\n",
            "Iteration: 7938; loss: 0.67094486951828\n",
            "Iteration: 7939; loss: 0.6800478100776672\n",
            "Iteration: 7940; loss: 0.6943785548210144\n",
            "Iteration: 7941; loss: 0.6908752918243408\n",
            "Iteration: 7942; loss: 0.7136976718902588\n",
            "Iteration: 7943; loss: 0.6801739931106567\n",
            "Iteration: 7944; loss: 0.7115830779075623\n",
            "Iteration: 7945; loss: 0.6739298105239868\n",
            "Iteration: 7946; loss: 0.6795110106468201\n",
            "Iteration: 7947; loss: 0.6789642572402954\n",
            "Iteration: 7948; loss: 0.6952639222145081\n",
            "Iteration: 7949; loss: 0.6998758912086487\n",
            "Iteration: 7950; loss: 0.6689088344573975\n",
            "Iteration: 7951; loss: 0.6920097470283508\n",
            "Iteration: 7952; loss: 0.6887791156768799\n",
            "Iteration: 7953; loss: 0.6791452169418335\n",
            "Iteration: 7954; loss: 0.6725694537162781\n",
            "Iteration: 7955; loss: 0.6880496144294739\n",
            "Iteration: 7956; loss: 0.6844280362129211\n",
            "Iteration: 7957; loss: 0.6756033897399902\n",
            "Iteration: 7958; loss: 0.6714658737182617\n",
            "Iteration: 7959; loss: 0.7011092901229858\n",
            "Iteration: 7960; loss: 0.6860425472259521\n",
            "Iteration: 7961; loss: 0.6875424981117249\n",
            "Iteration: 7962; loss: 0.6816078424453735\n",
            "Iteration: 7963; loss: 0.6801232099533081\n",
            "Iteration: 7964; loss: 0.6942605972290039\n",
            "Iteration: 7965; loss: 0.6833720803260803\n",
            "Iteration: 7966; loss: 0.6830679774284363\n",
            "Iteration: 7967; loss: 0.6674051284790039\n",
            "Iteration: 7968; loss: 0.6892852187156677\n",
            "Iteration: 7969; loss: 0.685019314289093\n",
            "Iteration: 7970; loss: 0.6858680248260498\n",
            "Iteration: 7971; loss: 0.683678150177002\n",
            "Iteration: 7972; loss: 0.6846157312393188\n",
            "Iteration: 7973; loss: 0.6626386642456055\n",
            "Iteration: 7974; loss: 0.6767114400863647\n",
            "Iteration: 7975; loss: 0.670782744884491\n",
            "Iteration: 7976; loss: 0.686291515827179\n",
            "Iteration: 7977; loss: 0.6891623735427856\n",
            "Iteration: 7978; loss: 0.6906046867370605\n",
            "Iteration: 7979; loss: 0.689584493637085\n",
            "Iteration: 7980; loss: 0.6997962594032288\n",
            "Iteration: 7981; loss: 0.6884932518005371\n",
            "Iteration: 7982; loss: 0.6867048144340515\n",
            "Iteration: 7983; loss: 0.6884914636611938\n",
            "Iteration: 7984; loss: 0.6895875930786133\n",
            "Iteration: 7985; loss: 0.696845531463623\n",
            "Iteration: 7986; loss: 0.6884579062461853\n",
            "Iteration: 7987; loss: 0.666785717010498\n",
            "Iteration: 7988; loss: 0.6834222078323364\n",
            "Iteration: 7989; loss: 0.690802812576294\n",
            "Iteration: 7990; loss: 0.6727672219276428\n",
            "Iteration: 7991; loss: 0.6843458414077759\n",
            "Iteration: 7992; loss: 0.6752085089683533\n",
            "Iteration: 7993; loss: 0.6685684323310852\n",
            "Iteration: 7994; loss: 0.6898806691169739\n",
            "Iteration: 7995; loss: 0.6833289265632629\n",
            "Iteration: 7996; loss: 0.6844288110733032\n",
            "Iteration: 7997; loss: 0.6993572115898132\n",
            "Iteration: 7998; loss: 0.689452052116394\n",
            "Iteration: 7999; loss: 0.7046818733215332\n",
            "Iteration: 8000; loss: 0.6819712519645691\n",
            "Iteration: 8001; loss: 0.7057502865791321\n",
            "Iteration: 8002; loss: 0.6942223906517029\n",
            "Iteration: 8003; loss: 0.673305332660675\n",
            "Iteration: 8004; loss: 0.6841832995414734\n",
            "Iteration: 8005; loss: 0.6801080107688904\n",
            "Iteration: 8006; loss: 0.6735661625862122\n",
            "Iteration: 8007; loss: 0.6857377290725708\n",
            "Iteration: 8008; loss: 0.6871640682220459\n",
            "Iteration: 8009; loss: 0.6866728067398071\n",
            "Iteration: 8010; loss: 0.6827982068061829\n",
            "Iteration: 8011; loss: 0.6840003728866577\n",
            "Iteration: 8012; loss: 0.6869332790374756\n",
            "Iteration: 8013; loss: 0.6766852736473083\n",
            "Iteration: 8014; loss: 0.6811305284500122\n",
            "Iteration: 8015; loss: 0.6931525468826294\n",
            "Iteration: 8016; loss: 0.6825835704803467\n",
            "Iteration: 8017; loss: 0.6963931322097778\n",
            "Iteration: 8018; loss: 0.6790434718132019\n",
            "Iteration: 8019; loss: 0.675163745880127\n",
            "Iteration: 8020; loss: 0.6955807209014893\n",
            "Iteration: 8021; loss: 0.6926246285438538\n",
            "Iteration: 8022; loss: 0.6812084317207336\n",
            "Iteration: 8023; loss: 0.6795206665992737\n",
            "Iteration: 8024; loss: 0.6948360800743103\n",
            "Iteration: 8025; loss: 0.6785499453544617\n",
            "Iteration: 8026; loss: 0.6716859936714172\n",
            "Iteration: 8027; loss: 0.6979615688323975\n",
            "Iteration: 8028; loss: 0.6998811364173889\n",
            "Iteration: 8029; loss: 0.6967891454696655\n",
            "Iteration: 8030; loss: 0.696841835975647\n",
            "Iteration: 8031; loss: 0.6790931224822998\n",
            "Iteration: 8032; loss: 0.6761102676391602\n",
            "Iteration: 8033; loss: 0.6865018010139465\n",
            "Iteration: 8034; loss: 0.6842265129089355\n",
            "Iteration: 8035; loss: 0.6978498697280884\n",
            "Iteration: 8036; loss: 0.6845492124557495\n",
            "Iteration: 8037; loss: 0.6796479225158691\n",
            "Iteration: 8038; loss: 0.70047527551651\n",
            "Iteration: 8039; loss: 0.6874572038650513\n",
            "Iteration: 8040; loss: 0.6782941222190857\n",
            "Iteration: 8041; loss: 0.6828380227088928\n",
            "Iteration: 8042; loss: 0.673966646194458\n",
            "Iteration: 8043; loss: 0.6737844347953796\n",
            "Iteration: 8044; loss: 0.7077803015708923\n",
            "Iteration: 8045; loss: 0.7013243436813354\n",
            "Iteration: 8046; loss: 0.6792608499526978\n",
            "Iteration: 8047; loss: 0.6681779026985168\n",
            "Iteration: 8048; loss: 0.6765211820602417\n",
            "Iteration: 8049; loss: 0.6701976656913757\n",
            "Iteration: 8050; loss: 0.6879224181175232\n",
            "Iteration: 8051; loss: 0.6696122884750366\n",
            "Iteration: 8052; loss: 0.6902447938919067\n",
            "Iteration: 8053; loss: 0.6888344883918762\n",
            "Iteration: 8054; loss: 0.6789512038230896\n",
            "Iteration: 8055; loss: 0.685804009437561\n",
            "Iteration: 8056; loss: 0.6962577104568481\n",
            "Iteration: 8057; loss: 0.7012030482292175\n",
            "Iteration: 8058; loss: 0.6828712821006775\n",
            "Iteration: 8059; loss: 0.692098081111908\n",
            "Iteration: 8060; loss: 0.6914417743682861\n",
            "Iteration: 8061; loss: 0.690670371055603\n",
            "Iteration: 8062; loss: 0.6802048087120056\n",
            "Iteration: 8063; loss: 0.688190758228302\n",
            "Iteration: 8064; loss: 0.6889985799789429\n",
            "Iteration: 8065; loss: 0.6884004473686218\n",
            "Iteration: 8066; loss: 0.6908007264137268\n",
            "Iteration: 8067; loss: 0.6883406639099121\n",
            "Iteration: 8068; loss: 0.6782768368721008\n",
            "Iteration: 8069; loss: 0.6811528205871582\n",
            "Iteration: 8070; loss: 0.6872036457061768\n",
            "Iteration: 8071; loss: 0.681808590888977\n",
            "Iteration: 8072; loss: 0.6743505001068115\n",
            "Iteration: 8073; loss: 0.676810622215271\n",
            "Iteration: 8074; loss: 0.6693887710571289\n",
            "Iteration: 8075; loss: 0.6761975288391113\n",
            "Iteration: 8076; loss: 0.6845811605453491\n",
            "Iteration: 8077; loss: 0.6854250431060791\n",
            "Iteration: 8078; loss: 0.6661132574081421\n",
            "Iteration: 8079; loss: 0.6813875436782837\n",
            "Iteration: 8080; loss: 0.6783735752105713\n",
            "Iteration: 8081; loss: 0.6742974519729614\n",
            "Iteration: 8082; loss: 0.6753342151641846\n",
            "Iteration: 8083; loss: 0.7067508101463318\n",
            "Iteration: 8084; loss: 0.6749129295349121\n",
            "Iteration: 8085; loss: 0.6894727349281311\n",
            "Iteration: 8086; loss: 0.6858359575271606\n",
            "Iteration: 8087; loss: 0.6652750968933105\n",
            "Iteration: 8088; loss: 0.6805953979492188\n",
            "Iteration: 8089; loss: 0.6928064227104187\n",
            "Iteration: 8090; loss: 0.6839845180511475\n",
            "Iteration: 8091; loss: 0.7065454721450806\n",
            "Iteration: 8092; loss: 0.6939478516578674\n",
            "Iteration: 8093; loss: 0.6770457625389099\n",
            "Iteration: 8094; loss: 0.694948136806488\n",
            "Iteration: 8095; loss: 0.6696582436561584\n",
            "Iteration: 8096; loss: 0.6793292760848999\n",
            "Iteration: 8097; loss: 0.6885470151901245\n",
            "Iteration: 8098; loss: 0.6993329524993896\n",
            "Iteration: 8099; loss: 0.6865018606185913\n",
            "Iteration: 8100; loss: 0.6804743409156799\n",
            "Iteration: 8101; loss: 0.6988180875778198\n",
            "Iteration: 8102; loss: 0.6877197623252869\n",
            "Iteration: 8103; loss: 0.6846070289611816\n",
            "Iteration: 8104; loss: 0.6809905767440796\n",
            "Iteration: 8105; loss: 0.690300703048706\n",
            "Iteration: 8106; loss: 0.6796467900276184\n",
            "Iteration: 8107; loss: 0.6899709105491638\n",
            "Iteration: 8108; loss: 0.680083692073822\n",
            "Iteration: 8109; loss: 0.6901782751083374\n",
            "Iteration: 8110; loss: 0.6879600286483765\n",
            "Iteration: 8111; loss: 0.6805525422096252\n",
            "Iteration: 8112; loss: 0.6820797324180603\n",
            "Iteration: 8113; loss: 0.676633894443512\n",
            "Iteration: 8114; loss: 0.6750771999359131\n",
            "Iteration: 8115; loss: 0.6687696576118469\n",
            "Iteration: 8116; loss: 0.6830164194107056\n",
            "Iteration: 8117; loss: 0.6858698129653931\n",
            "Iteration: 8118; loss: 0.6904078722000122\n",
            "Iteration: 8119; loss: 0.6800485849380493\n",
            "Iteration: 8120; loss: 0.6692355871200562\n",
            "Iteration: 8121; loss: 0.6670732498168945\n",
            "Iteration: 8122; loss: 0.6731109619140625\n",
            "Iteration: 8123; loss: 0.6749943494796753\n",
            "Iteration: 8124; loss: 0.671929121017456\n",
            "Iteration: 8125; loss: 0.6869735717773438\n",
            "Iteration: 8126; loss: 0.6900359392166138\n",
            "Iteration: 8127; loss: 0.6795412302017212\n",
            "Iteration: 8128; loss: 0.6863542795181274\n",
            "Iteration: 8129; loss: 0.6951972842216492\n",
            "Iteration: 8130; loss: 0.6839463114738464\n",
            "Iteration: 8131; loss: 0.6693288683891296\n",
            "Iteration: 8132; loss: 0.6738297939300537\n",
            "Iteration: 8133; loss: 0.6671611666679382\n",
            "Iteration: 8134; loss: 0.6803574562072754\n",
            "Iteration: 8135; loss: 0.687695324420929\n",
            "Iteration: 8136; loss: 0.6938617825508118\n",
            "Iteration: 8137; loss: 0.6785242557525635\n",
            "Iteration: 8138; loss: 0.6876431107521057\n",
            "Iteration: 8139; loss: 0.6899473667144775\n",
            "Iteration: 8140; loss: 0.6740395426750183\n",
            "Iteration: 8141; loss: 0.6990193724632263\n",
            "Iteration: 8142; loss: 0.6615241169929504\n",
            "Iteration: 8143; loss: 0.6933751702308655\n",
            "Iteration: 8144; loss: 0.6696487069129944\n",
            "Iteration: 8145; loss: 0.6808067560195923\n",
            "Iteration: 8146; loss: 0.6611554026603699\n",
            "Iteration: 8147; loss: 0.6874018311500549\n",
            "Iteration: 8148; loss: 0.6819472908973694\n",
            "Iteration: 8149; loss: 0.6792620420455933\n",
            "Iteration: 8150; loss: 0.6690912246704102\n",
            "Iteration: 8151; loss: 0.6807874441146851\n",
            "Iteration: 8152; loss: 0.6749318242073059\n",
            "Iteration: 8153; loss: 0.6759371757507324\n",
            "Iteration: 8154; loss: 0.6683098077774048\n",
            "Iteration: 8155; loss: 0.6812731027603149\n",
            "Iteration: 8156; loss: 0.6666412353515625\n",
            "Iteration: 8157; loss: 0.6852222084999084\n",
            "Iteration: 8158; loss: 0.6796890497207642\n",
            "Iteration: 8159; loss: 0.6830747127532959\n",
            "Iteration: 8160; loss: 0.6827837228775024\n",
            "Iteration: 8161; loss: 0.6864186525344849\n",
            "Iteration: 8162; loss: 0.6843382120132446\n",
            "Iteration: 8163; loss: 0.6878454089164734\n",
            "Iteration: 8164; loss: 0.6967241168022156\n",
            "Iteration: 8165; loss: 0.6743762493133545\n",
            "Iteration: 8166; loss: 0.6808773875236511\n",
            "Iteration: 8167; loss: 0.6822806000709534\n",
            "Iteration: 8168; loss: 0.6856817007064819\n",
            "Iteration: 8169; loss: 0.68156898021698\n",
            "Iteration: 8170; loss: 0.6842707991600037\n",
            "Iteration: 8171; loss: 0.6819148659706116\n",
            "Iteration: 8172; loss: 0.6702749729156494\n",
            "Iteration: 8173; loss: 0.6717351675033569\n",
            "Iteration: 8174; loss: 0.6768618226051331\n",
            "Iteration: 8175; loss: 0.6869207620620728\n",
            "Iteration: 8176; loss: 0.6651627421379089\n",
            "Iteration: 8177; loss: 0.6615262031555176\n",
            "Iteration: 8178; loss: 0.686793863773346\n",
            "Iteration: 8179; loss: 0.6546657085418701\n",
            "Iteration: 8180; loss: 0.6732123494148254\n",
            "Iteration: 8181; loss: 0.7009350061416626\n",
            "Iteration: 8182; loss: 0.6549931168556213\n",
            "Iteration: 8183; loss: 0.6693801879882812\n",
            "Iteration: 8184; loss: 0.6703300476074219\n",
            "Iteration: 8185; loss: 0.6874897480010986\n",
            "Iteration: 8186; loss: 0.6817353963851929\n",
            "Iteration: 8187; loss: 0.6738158464431763\n",
            "Iteration: 8188; loss: 0.6525943875312805\n",
            "Iteration: 8189; loss: 0.6785025596618652\n",
            "Iteration: 8190; loss: 0.7009515166282654\n",
            "Iteration: 8191; loss: 0.6895079016685486\n",
            "Iteration: 8192; loss: 0.6667108535766602\n",
            "Iteration: 8193; loss: 0.6652612686157227\n",
            "Iteration: 8194; loss: 0.6859701871871948\n",
            "Iteration: 8195; loss: 0.6724286079406738\n",
            "Iteration: 8196; loss: 0.6682274341583252\n",
            "Iteration: 8197; loss: 0.6705062985420227\n",
            "Iteration: 8198; loss: 0.6805028319358826\n",
            "Iteration: 8199; loss: 0.6771687269210815\n",
            "Iteration: 8200; loss: 0.689539909362793\n",
            "Iteration: 8201; loss: 0.6799336671829224\n",
            "Iteration: 8202; loss: 0.6758052110671997\n",
            "Iteration: 8203; loss: 0.6722318530082703\n",
            "Iteration: 8204; loss: 0.6770274639129639\n",
            "Iteration: 8205; loss: 0.682834267616272\n",
            "Iteration: 8206; loss: 0.6719337105751038\n",
            "Iteration: 8207; loss: 0.6907317042350769\n",
            "Iteration: 8208; loss: 0.6823148727416992\n",
            "Iteration: 8209; loss: 0.6768103837966919\n",
            "Iteration: 8210; loss: 0.6798437237739563\n",
            "Iteration: 8211; loss: 0.6636009216308594\n",
            "Iteration: 8212; loss: 0.6738023161888123\n",
            "Iteration: 8213; loss: 0.6629816293716431\n",
            "Iteration: 8214; loss: 0.6536378860473633\n",
            "Iteration: 8215; loss: 0.6828378438949585\n",
            "Iteration: 8216; loss: 0.6680727601051331\n",
            "Iteration: 8217; loss: 0.6709421277046204\n",
            "Iteration: 8218; loss: 0.6686358451843262\n",
            "Iteration: 8219; loss: 0.6713353395462036\n",
            "Iteration: 8220; loss: 0.6751916408538818\n",
            "Iteration: 8221; loss: 0.6689932942390442\n",
            "Iteration: 8222; loss: 0.6689194440841675\n",
            "Iteration: 8223; loss: 0.6838710308074951\n",
            "Iteration: 8224; loss: 0.6802912950515747\n",
            "Iteration: 8225; loss: 0.6737503409385681\n",
            "Iteration: 8226; loss: 0.6728237271308899\n",
            "Iteration: 8227; loss: 0.6788104772567749\n",
            "Iteration: 8228; loss: 0.6675719022750854\n",
            "Iteration: 8229; loss: 0.6530791521072388\n",
            "Iteration: 8230; loss: 0.6875253915786743\n",
            "Iteration: 8231; loss: 0.6573159098625183\n",
            "Iteration: 8232; loss: 0.6601314544677734\n",
            "Iteration: 8233; loss: 0.6874712705612183\n",
            "Iteration: 8234; loss: 0.6890599727630615\n",
            "Iteration: 8235; loss: 0.6790687441825867\n",
            "Iteration: 8236; loss: 0.6666562557220459\n",
            "Iteration: 8237; loss: 0.6851761937141418\n",
            "Iteration: 8238; loss: 0.6863947510719299\n",
            "Iteration: 8239; loss: 0.6627300977706909\n",
            "Iteration: 8240; loss: 0.6904192566871643\n",
            "Iteration: 8241; loss: 0.6623438000679016\n",
            "Iteration: 8242; loss: 0.6668959856033325\n",
            "Iteration: 8243; loss: 0.6551589369773865\n",
            "Iteration: 8244; loss: 0.6610814929008484\n",
            "Iteration: 8245; loss: 0.6823754906654358\n",
            "Iteration: 8246; loss: 0.6901906132698059\n",
            "Iteration: 8247; loss: 0.6783344149589539\n",
            "Iteration: 8248; loss: 0.6733818054199219\n",
            "Iteration: 8249; loss: 0.6680803298950195\n",
            "Iteration: 8250; loss: 0.6639153361320496\n",
            "Iteration: 8251; loss: 0.6653964519500732\n",
            "Iteration: 8252; loss: 0.6638197302818298\n",
            "Iteration: 8253; loss: 0.6831139326095581\n",
            "Iteration: 8254; loss: 0.6842942833900452\n",
            "Iteration: 8255; loss: 0.6558281183242798\n",
            "Iteration: 8256; loss: 0.6922386288642883\n",
            "Iteration: 8257; loss: 0.6914375424385071\n",
            "Iteration: 8258; loss: 0.6735223531723022\n",
            "Iteration: 8259; loss: 0.7019413709640503\n",
            "Iteration: 8260; loss: 0.6771970391273499\n",
            "Iteration: 8261; loss: 0.6615767478942871\n",
            "Iteration: 8262; loss: 0.7019416689872742\n",
            "Iteration: 8263; loss: 0.6879546046257019\n",
            "Iteration: 8264; loss: 0.677638828754425\n",
            "Iteration: 8265; loss: 0.6777117252349854\n",
            "Iteration: 8266; loss: 0.6704951524734497\n",
            "Iteration: 8267; loss: 0.6667623519897461\n",
            "Iteration: 8268; loss: 0.6728115081787109\n",
            "Iteration: 8269; loss: 0.6646154522895813\n",
            "Iteration: 8270; loss: 0.6676380038261414\n",
            "Iteration: 8271; loss: 0.6852605938911438\n",
            "Iteration: 8272; loss: 0.6703689694404602\n",
            "Iteration: 8273; loss: 0.6712048053741455\n",
            "Iteration: 8274; loss: 0.6627523899078369\n",
            "Iteration: 8275; loss: 0.6827837228775024\n",
            "Iteration: 8276; loss: 0.6449123620986938\n",
            "Iteration: 8277; loss: 0.6566646695137024\n",
            "Iteration: 8278; loss: 0.6988780498504639\n",
            "Iteration: 8279; loss: 0.6834689974784851\n",
            "Iteration: 8280; loss: 0.6532430052757263\n",
            "Iteration: 8281; loss: 0.648125410079956\n",
            "Iteration: 8282; loss: 0.6496809720993042\n",
            "Iteration: 8283; loss: 0.6413958072662354\n",
            "Iteration: 8284; loss: 0.6489892601966858\n",
            "Iteration: 8285; loss: 0.6432480812072754\n",
            "Iteration: 8286; loss: 0.6631721258163452\n",
            "Iteration: 8287; loss: 0.6544932723045349\n",
            "Iteration: 8288; loss: 0.6720234751701355\n",
            "Iteration: 8289; loss: 0.671660304069519\n",
            "Iteration: 8290; loss: 0.6667519807815552\n",
            "Iteration: 8291; loss: 0.689680278301239\n",
            "Iteration: 8292; loss: 0.6572920083999634\n",
            "Iteration: 8293; loss: 0.6723122000694275\n",
            "Iteration: 8294; loss: 0.6572519540786743\n",
            "Iteration: 8295; loss: 0.6974698901176453\n",
            "Iteration: 8296; loss: 0.6610571146011353\n",
            "Iteration: 8297; loss: 0.6632787585258484\n",
            "Iteration: 8298; loss: 0.6754050254821777\n",
            "Iteration: 8299; loss: 0.680477499961853\n",
            "Iteration: 8300; loss: 0.6329657435417175\n",
            "Iteration: 8301; loss: 0.6694202423095703\n",
            "Iteration: 8302; loss: 0.6644713878631592\n",
            "Iteration: 8303; loss: 0.6557061672210693\n",
            "Iteration: 8304; loss: 0.6676642298698425\n",
            "Iteration: 8305; loss: 0.672034740447998\n",
            "Iteration: 8306; loss: 0.6674153208732605\n",
            "Iteration: 8307; loss: 0.6312135457992554\n",
            "Iteration: 8308; loss: 0.6383060812950134\n",
            "Iteration: 8309; loss: 0.63044273853302\n",
            "Iteration: 8310; loss: 0.6910251379013062\n",
            "Iteration: 8311; loss: 0.6588779091835022\n",
            "Iteration: 8312; loss: 0.6523801684379578\n",
            "Iteration: 8313; loss: 0.6619026064872742\n",
            "Iteration: 8314; loss: 0.6620943546295166\n",
            "Iteration: 8315; loss: 0.6277282238006592\n",
            "Iteration: 8316; loss: 0.6717303395271301\n",
            "Iteration: 8317; loss: 0.6603450775146484\n",
            "Iteration: 8318; loss: 0.6438394784927368\n",
            "Iteration: 8319; loss: 0.6612755656242371\n",
            "Iteration: 8320; loss: 0.6772322058677673\n",
            "Iteration: 8321; loss: 0.6416143178939819\n",
            "Iteration: 8322; loss: 0.6908698081970215\n",
            "Iteration: 8323; loss: 0.6600355505943298\n",
            "Iteration: 8324; loss: 0.677501380443573\n",
            "Iteration: 8325; loss: 0.6756688356399536\n",
            "Iteration: 8326; loss: 0.6755350232124329\n",
            "Iteration: 8327; loss: 0.6739097237586975\n",
            "Iteration: 8328; loss: 0.6450203657150269\n",
            "Iteration: 8329; loss: 0.6656370162963867\n",
            "Iteration: 8330; loss: 0.6751124858856201\n",
            "Iteration: 8331; loss: 0.6535425186157227\n",
            "Iteration: 8332; loss: 0.6897796392440796\n",
            "Iteration: 8333; loss: 0.6563422083854675\n",
            "Iteration: 8334; loss: 0.6435546278953552\n",
            "Iteration: 8335; loss: 0.6439275741577148\n",
            "Iteration: 8336; loss: 0.6582066416740417\n",
            "Iteration: 8337; loss: 0.6680067181587219\n",
            "Iteration: 8338; loss: 0.669437825679779\n",
            "Iteration: 8339; loss: 0.6862114071846008\n",
            "Iteration: 8340; loss: 0.6230388879776001\n",
            "Iteration: 8341; loss: 0.6158356070518494\n",
            "Iteration: 8342; loss: 0.6385123133659363\n",
            "Iteration: 8343; loss: 0.6515906453132629\n",
            "Iteration: 8344; loss: 0.6542013883590698\n",
            "Iteration: 8345; loss: 0.6341661810874939\n",
            "Iteration: 8346; loss: 0.650391161441803\n",
            "Iteration: 8347; loss: 0.6656491756439209\n",
            "Iteration: 8348; loss: 0.6521670818328857\n",
            "Iteration: 8349; loss: 0.6422962546348572\n",
            "Iteration: 8350; loss: 0.6805175542831421\n",
            "Iteration: 8351; loss: 0.6500846743583679\n",
            "Iteration: 8352; loss: 0.6501139402389526\n",
            "Iteration: 8353; loss: 0.630450427532196\n",
            "Iteration: 8354; loss: 0.6504501700401306\n",
            "Iteration: 8355; loss: 0.6503802537918091\n",
            "Iteration: 8356; loss: 0.6544455885887146\n",
            "Iteration: 8357; loss: 0.6270750164985657\n",
            "Iteration: 8358; loss: 0.6512110233306885\n",
            "Iteration: 8359; loss: 0.6293217539787292\n",
            "Iteration: 8360; loss: 0.6722208261489868\n",
            "Iteration: 8361; loss: 0.654374361038208\n",
            "Iteration: 8362; loss: 0.6408930420875549\n",
            "Iteration: 8363; loss: 0.6318314671516418\n",
            "Iteration: 8364; loss: 0.6394822001457214\n",
            "Iteration: 8365; loss: 0.666700005531311\n",
            "Iteration: 8366; loss: 0.6531882286071777\n",
            "Iteration: 8367; loss: 0.6751779913902283\n",
            "Iteration: 8368; loss: 0.656164288520813\n",
            "Iteration: 8369; loss: 0.6597229242324829\n",
            "Iteration: 8370; loss: 0.6911287307739258\n",
            "Iteration: 8371; loss: 0.6415457725524902\n",
            "Iteration: 8372; loss: 0.6444516181945801\n",
            "Iteration: 8373; loss: 0.6481973528862\n",
            "Iteration: 8374; loss: 0.6460436582565308\n",
            "Iteration: 8375; loss: 0.632355809211731\n",
            "Iteration: 8376; loss: 0.6351490616798401\n",
            "Iteration: 8377; loss: 0.6531835198402405\n",
            "Iteration: 8378; loss: 0.645003080368042\n",
            "Iteration: 8379; loss: 0.6523637771606445\n",
            "Iteration: 8380; loss: 0.657536506652832\n",
            "Iteration: 8381; loss: 0.6417810916900635\n",
            "Iteration: 8382; loss: 0.6688470840454102\n",
            "Iteration: 8383; loss: 0.628228485584259\n",
            "Iteration: 8384; loss: 0.6595486998558044\n",
            "Iteration: 8385; loss: 0.6342081427574158\n",
            "Iteration: 8386; loss: 0.6640031933784485\n",
            "Iteration: 8387; loss: 0.6259198188781738\n",
            "Iteration: 8388; loss: 0.6423191428184509\n",
            "Iteration: 8389; loss: 0.6722397804260254\n",
            "Iteration: 8390; loss: 0.6363005638122559\n",
            "Iteration: 8391; loss: 0.635353684425354\n",
            "Iteration: 8392; loss: 0.6635060906410217\n",
            "Iteration: 8393; loss: 0.6747456789016724\n",
            "Iteration: 8394; loss: 0.6335286498069763\n",
            "Iteration: 8395; loss: 0.6887097358703613\n",
            "Iteration: 8396; loss: 0.6484872698783875\n",
            "Iteration: 8397; loss: 0.6541969180107117\n",
            "Iteration: 8398; loss: 0.6418276429176331\n",
            "Iteration: 8399; loss: 0.672189474105835\n",
            "Iteration: 8400; loss: 0.6534574031829834\n",
            "Iteration: 8401; loss: 0.6390005350112915\n",
            "Iteration: 8402; loss: 0.6485322117805481\n",
            "Iteration: 8403; loss: 0.6287120580673218\n",
            "Iteration: 8404; loss: 0.6266542673110962\n",
            "Iteration: 8405; loss: 0.6485117077827454\n",
            "Iteration: 8406; loss: 0.6136462688446045\n",
            "Iteration: 8407; loss: 0.6414594650268555\n",
            "Iteration: 8408; loss: 0.6401990056037903\n",
            "Iteration: 8409; loss: 0.6165010929107666\n",
            "Iteration: 8410; loss: 0.6561070084571838\n",
            "Iteration: 8411; loss: 0.6891771554946899\n",
            "Iteration: 8412; loss: 0.6590796709060669\n",
            "Iteration: 8413; loss: 0.6001487374305725\n",
            "Iteration: 8414; loss: 0.641965925693512\n",
            "Iteration: 8415; loss: 0.6644302606582642\n",
            "Iteration: 8416; loss: 0.6602367162704468\n",
            "Iteration: 8417; loss: 0.6643158197402954\n",
            "Iteration: 8418; loss: 0.6340084671974182\n",
            "Iteration: 8419; loss: 0.6633107662200928\n",
            "Iteration: 8420; loss: 0.6407461166381836\n",
            "Iteration: 8421; loss: 0.6315763592720032\n",
            "Iteration: 8422; loss: 0.6689726710319519\n",
            "Iteration: 8423; loss: 0.6432279348373413\n",
            "Iteration: 8424; loss: 0.6373295187950134\n",
            "Iteration: 8425; loss: 0.6396787762641907\n",
            "Iteration: 8426; loss: 0.615463137626648\n",
            "Iteration: 8427; loss: 0.6094717383384705\n",
            "Iteration: 8428; loss: 0.6275180578231812\n",
            "Iteration: 8429; loss: 0.6288582682609558\n",
            "Iteration: 8430; loss: 0.627195417881012\n",
            "Iteration: 8431; loss: 0.6672762632369995\n",
            "Iteration: 8432; loss: 0.6380770802497864\n",
            "Iteration: 8433; loss: 0.6263033747673035\n",
            "Iteration: 8434; loss: 0.5990626811981201\n",
            "Iteration: 8435; loss: 0.626123309135437\n",
            "Iteration: 8436; loss: 0.640501856803894\n",
            "Iteration: 8437; loss: 0.6214492917060852\n",
            "Iteration: 8438; loss: 0.6081670522689819\n",
            "Iteration: 8439; loss: 0.6445153951644897\n",
            "Iteration: 8440; loss: 0.5777783989906311\n",
            "Iteration: 8441; loss: 0.5919222235679626\n",
            "Iteration: 8442; loss: 0.641635000705719\n",
            "Iteration: 8443; loss: 0.5719140768051147\n",
            "Iteration: 8444; loss: 0.6233281493186951\n",
            "Iteration: 8445; loss: 0.5801968574523926\n",
            "Iteration: 8446; loss: 0.6249653100967407\n",
            "Iteration: 8447; loss: 0.6306602358818054\n",
            "Iteration: 8448; loss: 0.6647117137908936\n",
            "Iteration: 8449; loss: 0.6351386308670044\n",
            "Iteration: 8450; loss: 0.6864432692527771\n",
            "Iteration: 8451; loss: 0.6550648212432861\n",
            "Iteration: 8452; loss: 0.6491739153862\n",
            "Iteration: 8453; loss: 0.642835259437561\n",
            "Iteration: 8454; loss: 0.6211162209510803\n",
            "Iteration: 8455; loss: 0.6343337893486023\n",
            "Iteration: 8456; loss: 0.5976830720901489\n",
            "Iteration: 8457; loss: 0.6063243746757507\n",
            "Iteration: 8458; loss: 0.5782789587974548\n",
            "Iteration: 8459; loss: 0.6451417803764343\n",
            "Iteration: 8460; loss: 0.5855205059051514\n",
            "Iteration: 8461; loss: 0.6103119850158691\n",
            "Iteration: 8462; loss: 0.5819429755210876\n",
            "Iteration: 8463; loss: 0.6481950283050537\n",
            "Iteration: 8464; loss: 0.6206722259521484\n",
            "Iteration: 8465; loss: 0.6402430534362793\n",
            "Iteration: 8466; loss: 0.5939486622810364\n",
            "Iteration: 8467; loss: 0.608009397983551\n",
            "Iteration: 8468; loss: 0.627683162689209\n",
            "Iteration: 8469; loss: 0.6017598509788513\n",
            "Iteration: 8470; loss: 0.6239504218101501\n",
            "Iteration: 8471; loss: 0.622652530670166\n",
            "Iteration: 8472; loss: 0.6044172048568726\n",
            "Iteration: 8473; loss: 0.6160537600517273\n",
            "Iteration: 8474; loss: 0.6234122514724731\n",
            "Iteration: 8475; loss: 0.6500504612922668\n",
            "Iteration: 8476; loss: 0.6064221262931824\n",
            "Iteration: 8477; loss: 0.6176987886428833\n",
            "Iteration: 8478; loss: 0.5861353874206543\n",
            "Iteration: 8479; loss: 0.6429926156997681\n",
            "Iteration: 8480; loss: 0.5980902314186096\n",
            "Iteration: 8481; loss: 0.6340759992599487\n",
            "Iteration: 8482; loss: 0.6192961931228638\n",
            "Iteration: 8483; loss: 0.590593695640564\n",
            "Iteration: 8484; loss: 0.5753838419914246\n",
            "Iteration: 8485; loss: 0.6463441252708435\n",
            "Iteration: 8486; loss: 0.6639843583106995\n",
            "Iteration: 8487; loss: 0.6015039682388306\n",
            "Iteration: 8488; loss: 0.6006813049316406\n",
            "Iteration: 8489; loss: 0.6219276189804077\n",
            "Iteration: 8490; loss: 0.6097909212112427\n",
            "Iteration: 8491; loss: 0.587277889251709\n",
            "Iteration: 8492; loss: 0.599867582321167\n",
            "Iteration: 8493; loss: 0.5587999224662781\n",
            "Iteration: 8494; loss: 0.6411536931991577\n",
            "Iteration: 8495; loss: 0.5810174345970154\n",
            "Iteration: 8496; loss: 0.5798532962799072\n",
            "Iteration: 8497; loss: 0.5935669541358948\n",
            "Iteration: 8498; loss: 0.6355072855949402\n",
            "Iteration: 8499; loss: 0.5825449824333191\n",
            "Iteration: 8500; loss: 0.6083806753158569\n",
            "Iteration: 8501; loss: 0.5831494331359863\n",
            "Iteration: 8502; loss: 0.6149017810821533\n",
            "Iteration: 8503; loss: 0.5999602675437927\n",
            "Iteration: 8504; loss: 0.6040459871292114\n",
            "Iteration: 8505; loss: 0.5781400203704834\n",
            "Iteration: 8506; loss: 0.6085959672927856\n",
            "Iteration: 8507; loss: 0.5862370729446411\n",
            "Iteration: 8508; loss: 0.5805827975273132\n",
            "Iteration: 8509; loss: 0.5772073864936829\n",
            "Iteration: 8510; loss: 0.5873284935951233\n",
            "Iteration: 8511; loss: 0.5837650299072266\n",
            "Iteration: 8512; loss: 0.5719386339187622\n",
            "Iteration: 8513; loss: 0.6130510568618774\n",
            "Iteration: 8514; loss: 0.6058810353279114\n",
            "Iteration: 8515; loss: 0.6135636568069458\n",
            "Iteration: 8516; loss: 0.5576648712158203\n",
            "Iteration: 8517; loss: 0.5540817975997925\n",
            "Iteration: 8518; loss: 0.5782080292701721\n",
            "Iteration: 8519; loss: 0.6178576946258545\n",
            "Iteration: 8520; loss: 0.5461634993553162\n",
            "Iteration: 8521; loss: 0.5725123286247253\n",
            "Iteration: 8522; loss: 0.6447968482971191\n",
            "Iteration: 8523; loss: 0.6040712594985962\n",
            "Iteration: 8524; loss: 0.6266376376152039\n",
            "Iteration: 8525; loss: 0.5684632062911987\n",
            "Iteration: 8526; loss: 0.5970932245254517\n",
            "Iteration: 8527; loss: 0.5371776819229126\n",
            "Iteration: 8528; loss: 0.5934029817581177\n",
            "Iteration: 8529; loss: 0.6067067980766296\n",
            "Iteration: 8530; loss: 0.5406926870346069\n",
            "Iteration: 8531; loss: 0.5469238758087158\n",
            "Iteration: 8532; loss: 0.5789506435394287\n",
            "Iteration: 8533; loss: 0.6053338050842285\n",
            "Iteration: 8534; loss: 0.5976607799530029\n",
            "Iteration: 8535; loss: 0.5726475715637207\n",
            "Iteration: 8536; loss: 0.6077395081520081\n",
            "Iteration: 8537; loss: 0.6444210410118103\n",
            "Iteration: 8538; loss: 0.564538300037384\n",
            "Iteration: 8539; loss: 0.5236042737960815\n",
            "Iteration: 8540; loss: 0.6121418476104736\n",
            "Iteration: 8541; loss: 0.5611352920532227\n",
            "Iteration: 8542; loss: 0.5872128009796143\n",
            "Iteration: 8543; loss: 0.5671446323394775\n",
            "Iteration: 8544; loss: 0.578942060470581\n",
            "Iteration: 8545; loss: 0.5837235450744629\n",
            "Iteration: 8546; loss: 0.5823991894721985\n",
            "Iteration: 8547; loss: 0.6003731489181519\n",
            "Iteration: 8548; loss: 0.63320392370224\n",
            "Iteration: 8549; loss: 0.5334946513175964\n",
            "Iteration: 8550; loss: 0.5366262793540955\n",
            "Iteration: 8551; loss: 0.5983518362045288\n",
            "Iteration: 8552; loss: 0.56241774559021\n",
            "Iteration: 8553; loss: 0.5746461749076843\n",
            "Iteration: 8554; loss: 0.5925198793411255\n",
            "Iteration: 8555; loss: 0.618542492389679\n",
            "Iteration: 8556; loss: 0.563622236251831\n",
            "Iteration: 8557; loss: 0.5821288824081421\n",
            "Iteration: 8558; loss: 0.5719703435897827\n",
            "Iteration: 8559; loss: 0.5348370671272278\n",
            "Iteration: 8560; loss: 0.5639247298240662\n",
            "Iteration: 8561; loss: 0.5922649502754211\n",
            "Iteration: 8562; loss: 0.5717805027961731\n",
            "Iteration: 8563; loss: 0.5710241198539734\n",
            "Iteration: 8564; loss: 0.5066235661506653\n",
            "Iteration: 8565; loss: 0.5670631527900696\n",
            "Iteration: 8566; loss: 0.5841430425643921\n",
            "Iteration: 8567; loss: 0.5845690965652466\n",
            "Iteration: 8568; loss: 0.573100745677948\n",
            "Iteration: 8569; loss: 0.6225241422653198\n",
            "Iteration: 8570; loss: 0.5849580764770508\n",
            "Iteration: 8571; loss: 0.5585260987281799\n",
            "Iteration: 8572; loss: 0.5705321431159973\n",
            "Iteration: 8573; loss: 0.5795760750770569\n",
            "Iteration: 8574; loss: 0.5381762981414795\n",
            "Iteration: 8575; loss: 0.5900300741195679\n",
            "Iteration: 8576; loss: 0.5738086700439453\n",
            "Iteration: 8577; loss: 0.5632691383361816\n",
            "Iteration: 8578; loss: 0.5504164695739746\n",
            "Iteration: 8579; loss: 0.5536748766899109\n",
            "Iteration: 8580; loss: 0.5387393236160278\n",
            "Iteration: 8581; loss: 0.5137217044830322\n",
            "Iteration: 8582; loss: 0.5054846405982971\n",
            "Iteration: 8583; loss: 0.5820385217666626\n",
            "Iteration: 8584; loss: 0.572381854057312\n",
            "Iteration: 8585; loss: 0.5863783955574036\n",
            "Iteration: 8586; loss: 0.5853948593139648\n",
            "Iteration: 8587; loss: 0.5587437152862549\n",
            "Iteration: 8588; loss: 0.5815953016281128\n",
            "Iteration: 8589; loss: 0.6118741631507874\n",
            "Iteration: 8590; loss: 0.5460137724876404\n",
            "Iteration: 8591; loss: 0.5374928116798401\n",
            "Iteration: 8592; loss: 0.5813559293746948\n",
            "Iteration: 8593; loss: 0.5909781455993652\n",
            "Iteration: 8594; loss: 0.5779340267181396\n",
            "Iteration: 8595; loss: 0.5517857074737549\n",
            "Iteration: 8596; loss: 0.5457135438919067\n",
            "Iteration: 8597; loss: 0.5655049085617065\n",
            "Iteration: 8598; loss: 0.5876573324203491\n",
            "Iteration: 8599; loss: 0.5489422082901001\n",
            "Iteration: 8600; loss: 0.5641220808029175\n",
            "Iteration: 8601; loss: 0.621476948261261\n",
            "Iteration: 8602; loss: 0.5277722477912903\n",
            "Iteration: 8603; loss: 0.5475816130638123\n",
            "Iteration: 8604; loss: 0.5505331754684448\n",
            "Iteration: 8605; loss: 0.5820433497428894\n",
            "Iteration: 8606; loss: 0.6193298101425171\n",
            "Iteration: 8607; loss: 0.5580316781997681\n",
            "Iteration: 8608; loss: 0.5500635504722595\n",
            "Iteration: 8609; loss: 0.5232601165771484\n",
            "Iteration: 8610; loss: 0.5260308980941772\n",
            "Iteration: 8611; loss: 0.5625070929527283\n",
            "Iteration: 8612; loss: 0.5901492238044739\n",
            "Iteration: 8613; loss: 0.5618937611579895\n",
            "Iteration: 8614; loss: 0.5614625811576843\n",
            "Iteration: 8615; loss: 0.4935862421989441\n",
            "Iteration: 8616; loss: 0.47781622409820557\n",
            "Iteration: 8617; loss: 0.5177077651023865\n",
            "Iteration: 8618; loss: 0.512096643447876\n",
            "Iteration: 8619; loss: 0.5579612255096436\n",
            "Iteration: 8620; loss: 0.5702481269836426\n",
            "Iteration: 8621; loss: 0.5512799024581909\n",
            "Iteration: 8622; loss: 0.5000343918800354\n",
            "Iteration: 8623; loss: 0.5058767199516296\n",
            "Iteration: 8624; loss: 0.5422526597976685\n",
            "Iteration: 8625; loss: 0.5803108215332031\n",
            "Iteration: 8626; loss: 0.5429823398590088\n",
            "Iteration: 8627; loss: 0.5612874031066895\n",
            "Iteration: 8628; loss: 0.5235191583633423\n",
            "Iteration: 8629; loss: 0.5721129179000854\n",
            "Iteration: 8630; loss: 0.5129737854003906\n",
            "Iteration: 8631; loss: 0.5661754012107849\n",
            "Iteration: 8632; loss: 0.4835001230239868\n",
            "Iteration: 8633; loss: 0.5512046813964844\n",
            "Iteration: 8634; loss: 0.5077178478240967\n",
            "Iteration: 8635; loss: 0.5400545597076416\n",
            "Iteration: 8636; loss: 0.5612119436264038\n",
            "Iteration: 8637; loss: 0.5232722163200378\n",
            "Iteration: 8638; loss: 0.5142629146575928\n",
            "Iteration: 8639; loss: 0.5472599267959595\n",
            "Iteration: 8640; loss: 0.49891772866249084\n",
            "Iteration: 8641; loss: 0.5473790764808655\n",
            "Iteration: 8642; loss: 0.5302671790122986\n",
            "Iteration: 8643; loss: 0.4656168818473816\n",
            "Iteration: 8644; loss: 0.5410711169242859\n",
            "Iteration: 8645; loss: 0.48485255241394043\n",
            "Iteration: 8646; loss: 0.5657406449317932\n",
            "Iteration: 8647; loss: 0.5863798260688782\n",
            "Iteration: 8648; loss: 0.5681867003440857\n",
            "Iteration: 8649; loss: 0.5015053749084473\n",
            "Iteration: 8650; loss: 0.49664947390556335\n",
            "Iteration: 8651; loss: 0.5431417226791382\n",
            "Iteration: 8652; loss: 0.5208017826080322\n",
            "Iteration: 8653; loss: 0.48912686109542847\n",
            "Iteration: 8654; loss: 0.47718381881713867\n",
            "Iteration: 8655; loss: 0.4752853512763977\n",
            "Iteration: 8656; loss: 0.4494518041610718\n",
            "Iteration: 8657; loss: 0.5420424342155457\n",
            "Iteration: 8658; loss: 0.5124507546424866\n",
            "Iteration: 8659; loss: 0.5063220858573914\n",
            "Iteration: 8660; loss: 0.5351759195327759\n",
            "Iteration: 8661; loss: 0.5081345438957214\n",
            "Iteration: 8662; loss: 0.4970889091491699\n",
            "Iteration: 8663; loss: 0.5361714363098145\n",
            "Iteration: 8664; loss: 0.4840973913669586\n",
            "Iteration: 8665; loss: 0.42580145597457886\n",
            "Iteration: 8666; loss: 0.5119261741638184\n",
            "Iteration: 8667; loss: 0.5151345729827881\n",
            "Iteration: 8668; loss: 0.40768638253211975\n",
            "Iteration: 8669; loss: 0.4618709981441498\n",
            "Iteration: 8670; loss: 0.5755401849746704\n",
            "Iteration: 8671; loss: 0.523247480392456\n",
            "Iteration: 8672; loss: 0.46945720911026\n",
            "Iteration: 8673; loss: 0.5000717639923096\n",
            "Iteration: 8674; loss: 0.5085587501525879\n",
            "Iteration: 8675; loss: 0.5489360094070435\n",
            "Iteration: 8676; loss: 0.532298743724823\n",
            "Iteration: 8677; loss: 0.44393444061279297\n",
            "Iteration: 8678; loss: 0.5298553705215454\n",
            "Iteration: 8679; loss: 0.4913795292377472\n",
            "Iteration: 8680; loss: 0.4792139232158661\n",
            "Iteration: 8681; loss: 0.5180872082710266\n",
            "Iteration: 8682; loss: 0.5250200033187866\n",
            "Iteration: 8683; loss: 0.4790095090866089\n",
            "Iteration: 8684; loss: 0.5393630862236023\n",
            "Iteration: 8685; loss: 0.48742008209228516\n",
            "Iteration: 8686; loss: 0.5084179043769836\n",
            "Iteration: 8687; loss: 0.486440509557724\n",
            "Iteration: 8688; loss: 0.4651928246021271\n",
            "Iteration: 8689; loss: 0.5074227452278137\n",
            "Iteration: 8690; loss: 0.4508541226387024\n",
            "Iteration: 8691; loss: 0.4874498248100281\n",
            "Iteration: 8692; loss: 0.47677433490753174\n",
            "Iteration: 8693; loss: 0.47904717922210693\n",
            "Iteration: 8694; loss: 0.4526093006134033\n",
            "Iteration: 8695; loss: 0.5107228755950928\n",
            "Iteration: 8696; loss: 0.534572184085846\n",
            "Iteration: 8697; loss: 0.4796954393386841\n",
            "Iteration: 8698; loss: 0.48238229751586914\n",
            "Iteration: 8699; loss: 0.5106586217880249\n",
            "Iteration: 8700; loss: 0.5108532905578613\n",
            "Iteration: 8701; loss: 0.49160417914390564\n",
            "Iteration: 8702; loss: 0.43546509742736816\n",
            "Iteration: 8703; loss: 0.49122700095176697\n",
            "Iteration: 8704; loss: 0.49842727184295654\n",
            "Iteration: 8705; loss: 0.47090038657188416\n",
            "Iteration: 8706; loss: 0.4654974341392517\n",
            "Iteration: 8707; loss: 0.4830942153930664\n",
            "Iteration: 8708; loss: 0.46803000569343567\n",
            "Iteration: 8709; loss: 0.44932785630226135\n",
            "Iteration: 8710; loss: 0.4735744893550873\n",
            "Iteration: 8711; loss: 0.4860488772392273\n",
            "Iteration: 8712; loss: 0.46840742230415344\n",
            "Iteration: 8713; loss: 0.4565505385398865\n",
            "Iteration: 8714; loss: 0.43310898542404175\n",
            "Iteration: 8715; loss: 0.46135395765304565\n",
            "Iteration: 8716; loss: 0.4741494655609131\n",
            "Iteration: 8717; loss: 0.43595337867736816\n",
            "Iteration: 8718; loss: 0.5241926908493042\n",
            "Iteration: 8719; loss: 0.478132426738739\n",
            "Iteration: 8720; loss: 0.4970870614051819\n",
            "Iteration: 8721; loss: 0.4925556778907776\n",
            "Iteration: 8722; loss: 0.4089067876338959\n",
            "Iteration: 8723; loss: 0.4467398524284363\n",
            "Iteration: 8724; loss: 0.4233071804046631\n",
            "Iteration: 8725; loss: 0.45430290699005127\n",
            "Iteration: 8726; loss: 0.5023391842842102\n",
            "Iteration: 8727; loss: 0.5226201415061951\n",
            "Iteration: 8728; loss: 0.4881664514541626\n",
            "Iteration: 8729; loss: 0.4664705693721771\n",
            "Iteration: 8730; loss: 0.4396902918815613\n",
            "Iteration: 8731; loss: 0.5049490332603455\n",
            "Iteration: 8732; loss: 0.4948722720146179\n",
            "Iteration: 8733; loss: 0.4405735433101654\n",
            "Iteration: 8734; loss: 0.506628692150116\n",
            "Iteration: 8735; loss: 0.4768544137477875\n",
            "Iteration: 8736; loss: 0.48559415340423584\n",
            "Iteration: 8737; loss: 0.4973362684249878\n",
            "Iteration: 8738; loss: 0.4710913896560669\n",
            "Iteration: 8739; loss: 0.4235931634902954\n",
            "Iteration: 8740; loss: 0.49238914251327515\n",
            "Iteration: 8741; loss: 0.4733927249908447\n",
            "Iteration: 8742; loss: 0.45943963527679443\n",
            "Iteration: 8743; loss: 0.4302312135696411\n",
            "Iteration: 8744; loss: 0.4811917245388031\n",
            "Iteration: 8745; loss: 0.39926213026046753\n",
            "Iteration: 8746; loss: 0.4383618235588074\n",
            "Iteration: 8747; loss: 0.4721628427505493\n",
            "Iteration: 8748; loss: 0.521874189376831\n",
            "Iteration: 8749; loss: 0.5112541913986206\n",
            "Iteration: 8750; loss: 0.43723201751708984\n",
            "Iteration: 8751; loss: 0.41827380657196045\n",
            "Iteration: 8752; loss: 0.5094305276870728\n",
            "Iteration: 8753; loss: 0.446885347366333\n",
            "Iteration: 8754; loss: 0.47164636850357056\n",
            "Iteration: 8755; loss: 0.4940134286880493\n",
            "Iteration: 8756; loss: 0.419286847114563\n",
            "Iteration: 8757; loss: 0.43058836460113525\n",
            "Iteration: 8758; loss: 0.34891316294670105\n",
            "Iteration: 8759; loss: 0.4278416037559509\n",
            "Iteration: 8760; loss: 0.4697139263153076\n",
            "Iteration: 8761; loss: 0.4640014171600342\n",
            "Iteration: 8762; loss: 0.43326497077941895\n",
            "Iteration: 8763; loss: 0.4659944176673889\n",
            "Iteration: 8764; loss: 0.4112965166568756\n",
            "Iteration: 8765; loss: 0.4314166307449341\n",
            "Iteration: 8766; loss: 0.4584352970123291\n",
            "Iteration: 8767; loss: 0.4355793595314026\n",
            "Iteration: 8768; loss: 0.45308104157447815\n",
            "Iteration: 8769; loss: 0.4263046085834503\n",
            "Iteration: 8770; loss: 0.4919596314430237\n",
            "Iteration: 8771; loss: 0.42434653639793396\n",
            "Iteration: 8772; loss: 0.4094344973564148\n",
            "Iteration: 8773; loss: 0.40380480885505676\n",
            "Iteration: 8774; loss: 0.4151829183101654\n",
            "Iteration: 8775; loss: 0.4338478744029999\n",
            "Iteration: 8776; loss: 0.4009426236152649\n",
            "Iteration: 8777; loss: 0.41032958030700684\n",
            "Iteration: 8778; loss: 0.4204356074333191\n",
            "Iteration: 8779; loss: 0.38791078329086304\n",
            "Iteration: 8780; loss: 0.4972689747810364\n",
            "Iteration: 8781; loss: 0.4128322899341583\n",
            "Iteration: 8782; loss: 0.3810390830039978\n",
            "Iteration: 8783; loss: 0.3645506203174591\n",
            "Iteration: 8784; loss: 0.39618566632270813\n",
            "Iteration: 8785; loss: 0.3989800810813904\n",
            "Iteration: 8786; loss: 0.4025006890296936\n",
            "Iteration: 8787; loss: 0.41562938690185547\n",
            "Iteration: 8788; loss: 0.4257393777370453\n",
            "Iteration: 8789; loss: 0.4135173261165619\n",
            "Iteration: 8790; loss: 0.4166087806224823\n",
            "Iteration: 8791; loss: 0.4023691415786743\n",
            "Iteration: 8792; loss: 0.3887665569782257\n",
            "Iteration: 8793; loss: 0.371904194355011\n",
            "Iteration: 8794; loss: 0.38673242926597595\n",
            "Iteration: 8795; loss: 0.3726632297039032\n",
            "Iteration: 8796; loss: 0.37730371952056885\n",
            "Iteration: 8797; loss: 0.41623455286026\n",
            "Iteration: 8798; loss: 0.33238649368286133\n",
            "Iteration: 8799; loss: 0.41199684143066406\n",
            "Iteration: 8800; loss: 0.41384005546569824\n",
            "Iteration: 8801; loss: 0.4041616916656494\n",
            "Iteration: 8802; loss: 0.39992567896842957\n",
            "Iteration: 8803; loss: 0.34565508365631104\n",
            "Iteration: 8804; loss: 0.3908114433288574\n",
            "Iteration: 8805; loss: 0.4264233708381653\n",
            "Iteration: 8806; loss: 0.46579083800315857\n",
            "Iteration: 8807; loss: 0.3407163619995117\n",
            "Iteration: 8808; loss: 0.4308052062988281\n",
            "Iteration: 8809; loss: 0.36034154891967773\n",
            "Iteration: 8810; loss: 0.3829415738582611\n",
            "Iteration: 8811; loss: 0.4039362967014313\n",
            "Iteration: 8812; loss: 0.4164476692676544\n",
            "Iteration: 8813; loss: 0.3179677724838257\n",
            "Iteration: 8814; loss: 0.45977652072906494\n",
            "Iteration: 8815; loss: 0.36536288261413574\n",
            "Iteration: 8816; loss: 0.3401496708393097\n",
            "Iteration: 8817; loss: 0.3856678307056427\n",
            "Iteration: 8818; loss: 0.3916316032409668\n",
            "Iteration: 8819; loss: 0.34654173254966736\n",
            "Iteration: 8820; loss: 0.4061828553676605\n",
            "Iteration: 8821; loss: 0.4025210440158844\n",
            "Iteration: 8822; loss: 0.41184118390083313\n",
            "Iteration: 8823; loss: 0.40785840153694153\n",
            "Iteration: 8824; loss: 0.3461120128631592\n",
            "Iteration: 8825; loss: 0.37979140877723694\n",
            "Iteration: 8826; loss: 0.42763298749923706\n",
            "Iteration: 8827; loss: 0.43108299374580383\n",
            "Iteration: 8828; loss: 0.44672977924346924\n",
            "Iteration: 8829; loss: 0.33763569593429565\n",
            "Iteration: 8830; loss: 0.4073958396911621\n",
            "Iteration: 8831; loss: 0.43641436100006104\n",
            "Iteration: 8832; loss: 0.3638656735420227\n",
            "Iteration: 8833; loss: 0.4256293475627899\n",
            "Iteration: 8834; loss: 0.34564536809921265\n",
            "Iteration: 8835; loss: 0.43163245916366577\n",
            "Iteration: 8836; loss: 0.3695795238018036\n",
            "Iteration: 8837; loss: 0.3093920946121216\n",
            "Iteration: 8838; loss: 0.3195137679576874\n",
            "Iteration: 8839; loss: 0.34172523021698\n",
            "Iteration: 8840; loss: 0.3195490539073944\n",
            "Iteration: 8841; loss: 0.3710407316684723\n",
            "Iteration: 8842; loss: 0.42906445264816284\n",
            "Iteration: 8843; loss: 0.3744860887527466\n",
            "Iteration: 8844; loss: 0.3363639712333679\n",
            "Iteration: 8845; loss: 0.3669690489768982\n",
            "Iteration: 8846; loss: 0.3390572965145111\n",
            "Iteration: 8847; loss: 0.36074498295783997\n",
            "Iteration: 8848; loss: 0.3707845211029053\n",
            "Iteration: 8849; loss: 0.34900784492492676\n",
            "Iteration: 8850; loss: 0.44884029030799866\n",
            "Iteration: 8851; loss: 0.34847885370254517\n",
            "Iteration: 8852; loss: 0.342418372631073\n",
            "Iteration: 8853; loss: 0.49693238735198975\n",
            "Iteration: 8854; loss: 0.36926960945129395\n",
            "Iteration: 8855; loss: 0.40477603673934937\n",
            "Iteration: 8856; loss: 0.3819675147533417\n",
            "Iteration: 8857; loss: 0.3970069885253906\n",
            "Iteration: 8858; loss: 0.35377901792526245\n",
            "Iteration: 8859; loss: 0.3932846486568451\n",
            "Iteration: 8860; loss: 0.34853285551071167\n",
            "Iteration: 8861; loss: 0.29482483863830566\n",
            "Iteration: 8862; loss: 0.35425278544425964\n",
            "Iteration: 8863; loss: 0.3384075462818146\n",
            "Iteration: 8864; loss: 0.44254401326179504\n",
            "Iteration: 8865; loss: 0.39547985792160034\n",
            "Iteration: 8866; loss: 0.33042046427726746\n",
            "Iteration: 8867; loss: 0.3737167716026306\n",
            "Iteration: 8868; loss: 0.35236939787864685\n",
            "Iteration: 8869; loss: 0.3353002071380615\n",
            "Iteration: 8870; loss: 0.38232171535491943\n",
            "Iteration: 8871; loss: 0.43269646167755127\n",
            "Iteration: 8872; loss: 0.3688332140445709\n",
            "Iteration: 8873; loss: 0.36288946866989136\n",
            "Iteration: 8874; loss: 0.3459547162055969\n",
            "Iteration: 8875; loss: 0.3978023827075958\n",
            "Iteration: 8876; loss: 0.3367176949977875\n",
            "Iteration: 8877; loss: 0.3026074767112732\n",
            "Iteration: 8878; loss: 0.36792683601379395\n",
            "Iteration: 8879; loss: 0.4072955250740051\n",
            "Iteration: 8880; loss: 0.29484525322914124\n",
            "Iteration: 8881; loss: 0.39905890822410583\n",
            "Iteration: 8882; loss: 0.35008329153060913\n",
            "Iteration: 8883; loss: 0.4268704056739807\n",
            "Iteration: 8884; loss: 0.37482911348342896\n",
            "Iteration: 8885; loss: 0.3087385892868042\n",
            "Iteration: 8886; loss: 0.3184123635292053\n",
            "Iteration: 8887; loss: 0.3256157636642456\n",
            "Iteration: 8888; loss: 0.4031534492969513\n",
            "Iteration: 8889; loss: 0.4318567216396332\n",
            "Iteration: 8890; loss: 0.3567045331001282\n",
            "Iteration: 8891; loss: 0.3744930624961853\n",
            "Iteration: 8892; loss: 0.31747201085090637\n",
            "Iteration: 8893; loss: 0.3268827795982361\n",
            "Iteration: 8894; loss: 0.3792763352394104\n",
            "Iteration: 8895; loss: 0.3223159909248352\n",
            "Iteration: 8896; loss: 0.33298957347869873\n",
            "Iteration: 8897; loss: 0.31293126940727234\n",
            "Iteration: 8898; loss: 0.33709433674812317\n",
            "Iteration: 8899; loss: 0.2823399007320404\n",
            "Iteration: 8900; loss: 0.30290737748146057\n",
            "Iteration: 8901; loss: 0.3778379261493683\n",
            "Iteration: 8902; loss: 0.380530446767807\n",
            "Iteration: 8903; loss: 0.3463801443576813\n",
            "Iteration: 8904; loss: 0.2729226052761078\n",
            "Iteration: 8905; loss: 0.33914312720298767\n",
            "Iteration: 8906; loss: 0.29945051670074463\n",
            "Iteration: 8907; loss: 0.3538854718208313\n",
            "Iteration: 8908; loss: 0.37452611327171326\n",
            "Iteration: 8909; loss: 0.37407323718070984\n",
            "Iteration: 8910; loss: 0.3130963146686554\n",
            "Iteration: 8911; loss: 0.32771363854408264\n",
            "Iteration: 8912; loss: 0.3376375138759613\n",
            "Iteration: 8913; loss: 0.343791127204895\n",
            "Iteration: 8914; loss: 0.2934485375881195\n",
            "Iteration: 8915; loss: 0.3700420558452606\n",
            "Iteration: 8916; loss: 0.31979119777679443\n",
            "Iteration: 8917; loss: 0.32192882895469666\n",
            "Iteration: 8918; loss: 0.3320978879928589\n",
            "Iteration: 8919; loss: 0.29389306902885437\n",
            "Iteration: 8920; loss: 0.3800872266292572\n",
            "Iteration: 8921; loss: 0.37998533248901367\n",
            "Iteration: 8922; loss: 0.32879430055618286\n",
            "Iteration: 8923; loss: 0.30419525504112244\n",
            "Iteration: 8924; loss: 0.31088918447494507\n",
            "Iteration: 8925; loss: 0.3196566700935364\n",
            "Iteration: 8926; loss: 0.4031084179878235\n",
            "Iteration: 8927; loss: 0.3112742304801941\n",
            "Iteration: 8928; loss: 0.32293272018432617\n",
            "Iteration: 8929; loss: 0.2671922445297241\n",
            "Iteration: 8930; loss: 0.2887166142463684\n",
            "Iteration: 8931; loss: 0.3733798563480377\n",
            "Iteration: 8932; loss: 0.32984402775764465\n",
            "Iteration: 8933; loss: 0.326007604598999\n",
            "Iteration: 8934; loss: 0.33468079566955566\n",
            "Iteration: 8935; loss: 0.329303115606308\n",
            "Iteration: 8936; loss: 0.34018710255622864\n",
            "Iteration: 8937; loss: 0.3343257009983063\n",
            "Iteration: 8938; loss: 0.33216577768325806\n",
            "Iteration: 8939; loss: 0.311225950717926\n",
            "Iteration: 8940; loss: 0.3583330512046814\n",
            "Iteration: 8941; loss: 0.31868666410446167\n",
            "Iteration: 8942; loss: 0.3161647915840149\n",
            "Iteration: 8943; loss: 0.3422708511352539\n",
            "Iteration: 8944; loss: 0.2920420467853546\n",
            "Iteration: 8945; loss: 0.31666675209999084\n",
            "Iteration: 8946; loss: 0.25300249457359314\n",
            "Iteration: 8947; loss: 0.2719627618789673\n",
            "Iteration: 8948; loss: 0.24580639600753784\n",
            "Iteration: 8949; loss: 0.3305976986885071\n",
            "Iteration: 8950; loss: 0.2896682024002075\n",
            "Iteration: 8951; loss: 0.35980719327926636\n",
            "Iteration: 8952; loss: 0.30806729197502136\n",
            "Iteration: 8953; loss: 0.21421144902706146\n",
            "Iteration: 8954; loss: 0.3302261531352997\n",
            "Iteration: 8955; loss: 0.24694174528121948\n",
            "Iteration: 8956; loss: 0.25942113995552063\n",
            "Iteration: 8957; loss: 0.32048845291137695\n",
            "Iteration: 8958; loss: 0.3596692681312561\n",
            "Iteration: 8959; loss: 0.29970040917396545\n",
            "Iteration: 8960; loss: 0.2712368071079254\n",
            "Iteration: 8961; loss: 0.27581682801246643\n",
            "Iteration: 8962; loss: 0.25160646438598633\n",
            "Iteration: 8963; loss: 0.3536997437477112\n",
            "Iteration: 8964; loss: 0.28035494685173035\n",
            "Iteration: 8965; loss: 0.3309578001499176\n",
            "Iteration: 8966; loss: 0.44099974632263184\n",
            "Iteration: 8967; loss: 0.2601633667945862\n",
            "Iteration: 8968; loss: 0.2947254180908203\n",
            "Iteration: 8969; loss: 0.27186521887779236\n",
            "Iteration: 8970; loss: 0.2089925855398178\n",
            "Iteration: 8971; loss: 0.3763982653617859\n",
            "Iteration: 8972; loss: 0.30547744035720825\n",
            "Iteration: 8973; loss: 0.22526389360427856\n",
            "Iteration: 8974; loss: 0.29168710112571716\n",
            "Iteration: 8975; loss: 0.33787867426872253\n",
            "Iteration: 8976; loss: 0.2776011526584625\n",
            "Iteration: 8977; loss: 0.31400007009506226\n",
            "Iteration: 8978; loss: 0.24234409630298615\n",
            "Iteration: 8979; loss: 0.29203101992607117\n",
            "Iteration: 8980; loss: 0.21182310581207275\n",
            "Iteration: 8981; loss: 0.30600935220718384\n",
            "Iteration: 8982; loss: 0.38489389419555664\n",
            "Iteration: 8983; loss: 0.3264368772506714\n",
            "Iteration: 8984; loss: 0.30327004194259644\n",
            "Iteration: 8985; loss: 0.32140326499938965\n",
            "Iteration: 8986; loss: 0.3193247616291046\n",
            "Iteration: 8987; loss: 0.3370584547519684\n",
            "Iteration: 8988; loss: 0.23790308833122253\n",
            "Iteration: 8989; loss: 0.32757899165153503\n",
            "Iteration: 8990; loss: 0.33170533180236816\n",
            "Iteration: 8991; loss: 0.3143283724784851\n",
            "Iteration: 8992; loss: 0.2436145842075348\n",
            "Iteration: 8993; loss: 0.2515385150909424\n",
            "Iteration: 8994; loss: 0.24325662851333618\n",
            "Iteration: 8995; loss: 0.24369478225708008\n",
            "Iteration: 8996; loss: 0.27879858016967773\n",
            "Iteration: 8997; loss: 0.26242002844810486\n",
            "Iteration: 8998; loss: 0.31874334812164307\n",
            "Iteration: 8999; loss: 0.28445202112197876\n",
            "Iteration: 9000; loss: 0.2844086289405823\n",
            "Iteration: 9001; loss: 0.25104039907455444\n",
            "Iteration: 9002; loss: 0.28061047196388245\n",
            "Iteration: 9003; loss: 0.31483232975006104\n",
            "Iteration: 9004; loss: 0.29357820749282837\n",
            "Iteration: 9005; loss: 0.30756276845932007\n",
            "Iteration: 9006; loss: 0.28653448820114136\n",
            "Iteration: 9007; loss: 0.2539597749710083\n",
            "Iteration: 9008; loss: 0.2501209080219269\n",
            "Iteration: 9009; loss: 0.2842975854873657\n",
            "Iteration: 9010; loss: 0.2169342190027237\n",
            "Iteration: 9011; loss: 0.26675909757614136\n",
            "Iteration: 9012; loss: 0.28845521807670593\n",
            "Iteration: 9013; loss: 0.23438045382499695\n",
            "Iteration: 9014; loss: 0.27036231756210327\n",
            "Iteration: 9015; loss: 0.286792129278183\n",
            "Iteration: 9016; loss: 0.2823975086212158\n",
            "Iteration: 9017; loss: 0.34067365527153015\n",
            "Iteration: 9018; loss: 0.2251415103673935\n",
            "Iteration: 9019; loss: 0.24721570312976837\n",
            "Iteration: 9020; loss: 0.22783374786376953\n",
            "Iteration: 9021; loss: 0.21747714281082153\n",
            "Iteration: 9022; loss: 0.31040579080581665\n",
            "Iteration: 9023; loss: 0.23470118641853333\n",
            "Iteration: 9024; loss: 0.2700081169605255\n",
            "Iteration: 9025; loss: 0.2736009359359741\n",
            "Iteration: 9026; loss: 0.3048476278781891\n",
            "Iteration: 9027; loss: 0.1855718195438385\n",
            "Iteration: 9028; loss: 0.27101340889930725\n",
            "Iteration: 9029; loss: 0.19756333529949188\n",
            "Iteration: 9030; loss: 0.20406723022460938\n",
            "Iteration: 9031; loss: 0.21012268960475922\n",
            "Iteration: 9032; loss: 0.16473761200904846\n",
            "Iteration: 9033; loss: 0.25207841396331787\n",
            "Iteration: 9034; loss: 0.2576388418674469\n",
            "Iteration: 9035; loss: 0.276908814907074\n",
            "Iteration: 9036; loss: 0.26571279764175415\n",
            "Iteration: 9037; loss: 0.2802542448043823\n",
            "Iteration: 9038; loss: 0.27408719062805176\n",
            "Iteration: 9039; loss: 0.20961377024650574\n",
            "Iteration: 9040; loss: 0.2973916530609131\n",
            "Iteration: 9041; loss: 0.18571151793003082\n",
            "Iteration: 9042; loss: 0.24744129180908203\n",
            "Iteration: 9043; loss: 0.2701375484466553\n",
            "Iteration: 9044; loss: 0.18569615483283997\n",
            "Iteration: 9045; loss: 0.22746823728084564\n",
            "Iteration: 9046; loss: 0.2910049855709076\n",
            "Iteration: 9047; loss: 0.30337774753570557\n",
            "Iteration: 9048; loss: 0.26338696479797363\n",
            "Iteration: 9049; loss: 0.20907308161258698\n",
            "Iteration: 9050; loss: 0.19419246912002563\n",
            "Iteration: 9051; loss: 0.17892427742481232\n",
            "Iteration: 9052; loss: 0.22705909609794617\n",
            "Iteration: 9053; loss: 0.2226240038871765\n",
            "Iteration: 9054; loss: 0.14675001800060272\n",
            "Iteration: 9055; loss: 0.2519245445728302\n",
            "Iteration: 9056; loss: 0.23298119008541107\n",
            "Iteration: 9057; loss: 0.20993053913116455\n",
            "Iteration: 9058; loss: 0.20572371780872345\n",
            "Iteration: 9059; loss: 0.2041960060596466\n",
            "Iteration: 9060; loss: 0.2114308774471283\n",
            "Iteration: 9061; loss: 0.23242807388305664\n",
            "Iteration: 9062; loss: 0.2447933554649353\n",
            "Iteration: 9063; loss: 0.2549680471420288\n",
            "Iteration: 9064; loss: 0.22794829308986664\n",
            "Iteration: 9065; loss: 0.1461557149887085\n",
            "Iteration: 9066; loss: 0.21993392705917358\n",
            "Iteration: 9067; loss: 0.23140713572502136\n",
            "Iteration: 9068; loss: 0.19659611582756042\n",
            "Iteration: 9069; loss: 0.2609338164329529\n",
            "Iteration: 9070; loss: 0.28160953521728516\n",
            "Iteration: 9071; loss: 0.24142660200595856\n",
            "Iteration: 9072; loss: 0.2231203019618988\n",
            "Iteration: 9073; loss: 0.2317250370979309\n",
            "Iteration: 9074; loss: 0.3049360513687134\n",
            "Iteration: 9075; loss: 0.16792424023151398\n",
            "Iteration: 9076; loss: 0.2410755306482315\n",
            "Iteration: 9077; loss: 0.21994461119174957\n",
            "Iteration: 9078; loss: 0.1994338035583496\n",
            "Iteration: 9079; loss: 0.2194577306509018\n",
            "Iteration: 9080; loss: 0.2622687518596649\n",
            "Iteration: 9081; loss: 0.2023155689239502\n",
            "Iteration: 9082; loss: 0.23442891240119934\n",
            "Iteration: 9083; loss: 0.2534824013710022\n",
            "Iteration: 9084; loss: 0.235336035490036\n",
            "Iteration: 9085; loss: 0.19857607781887054\n",
            "Iteration: 9086; loss: 0.19162821769714355\n",
            "Iteration: 9087; loss: 0.1836627572774887\n",
            "Iteration: 9088; loss: 0.18582920730113983\n",
            "Iteration: 9089; loss: 0.1822947859764099\n",
            "Iteration: 9090; loss: 0.16938266158103943\n",
            "Iteration: 9091; loss: 0.200674906373024\n",
            "Iteration: 9092; loss: 0.18972258269786835\n",
            "Iteration: 9093; loss: 0.1674768626689911\n",
            "Iteration: 9094; loss: 0.14950525760650635\n",
            "Iteration: 9095; loss: 0.20035149157047272\n",
            "Iteration: 9096; loss: 0.18261109292507172\n",
            "Iteration: 9097; loss: 0.20554623007774353\n",
            "Iteration: 9098; loss: 0.22086991369724274\n",
            "Iteration: 9099; loss: 0.20969834923744202\n",
            "Iteration: 9100; loss: 0.12413958460092545\n",
            "Iteration: 9101; loss: 0.1899857074022293\n",
            "Iteration: 9102; loss: 0.25580355525016785\n",
            "Iteration: 9103; loss: 0.15397919714450836\n",
            "Iteration: 9104; loss: 0.1570722907781601\n",
            "Iteration: 9105; loss: 0.2662177085876465\n",
            "Iteration: 9106; loss: 0.24348431825637817\n",
            "Iteration: 9107; loss: 0.16908501088619232\n",
            "Iteration: 9108; loss: 0.24019663035869598\n",
            "Iteration: 9109; loss: 0.23139765858650208\n",
            "Iteration: 9110; loss: 0.1588938683271408\n",
            "Iteration: 9111; loss: 0.12882894277572632\n",
            "Iteration: 9112; loss: 0.20564565062522888\n",
            "Iteration: 9113; loss: 0.20984816551208496\n",
            "Iteration: 9114; loss: 0.16230720281600952\n",
            "Iteration: 9115; loss: 0.17752046883106232\n",
            "Iteration: 9116; loss: 0.2592361569404602\n",
            "Iteration: 9117; loss: 0.17963044345378876\n",
            "Iteration: 9118; loss: 0.20637518167495728\n",
            "Iteration: 9119; loss: 0.22090426087379456\n",
            "Iteration: 9120; loss: 0.16647174954414368\n",
            "Iteration: 9121; loss: 0.19473615288734436\n",
            "Iteration: 9122; loss: 0.15856696665287018\n",
            "Iteration: 9123; loss: 0.1726154386997223\n",
            "Iteration: 9124; loss: 0.17321375012397766\n",
            "Iteration: 9125; loss: 0.22789883613586426\n",
            "Iteration: 9126; loss: 0.20191645622253418\n",
            "Iteration: 9127; loss: 0.1582304686307907\n",
            "Iteration: 9128; loss: 0.21759924292564392\n",
            "Iteration: 9129; loss: 0.20107930898666382\n",
            "Iteration: 9130; loss: 0.21275606751441956\n",
            "Iteration: 9131; loss: 0.16932712495326996\n",
            "Iteration: 9132; loss: 0.17196333408355713\n",
            "Iteration: 9133; loss: 0.24005386233329773\n",
            "Iteration: 9134; loss: 0.17871586978435516\n",
            "Iteration: 9135; loss: 0.13349032402038574\n",
            "Iteration: 9136; loss: 0.14857377111911774\n",
            "Iteration: 9137; loss: 0.23022852838039398\n",
            "Iteration: 9138; loss: 0.2187064290046692\n",
            "Iteration: 9139; loss: 0.1716034710407257\n",
            "Iteration: 9140; loss: 0.15579548478126526\n",
            "Iteration: 9141; loss: 0.16050544381141663\n",
            "Iteration: 9142; loss: 0.1350477933883667\n",
            "Iteration: 9143; loss: 0.18012897670269012\n",
            "Iteration: 9144; loss: 0.17748816311359406\n",
            "Iteration: 9145; loss: 0.19559475779533386\n",
            "Iteration: 9146; loss: 0.2030445635318756\n",
            "Iteration: 9147; loss: 0.18933089077472687\n",
            "Iteration: 9148; loss: 0.18204396963119507\n",
            "Iteration: 9149; loss: 0.15610787272453308\n",
            "Iteration: 9150; loss: 0.1693994402885437\n",
            "Iteration: 9151; loss: 0.18004988133907318\n",
            "Iteration: 9152; loss: 0.16663691401481628\n",
            "Iteration: 9153; loss: 0.1930515468120575\n",
            "Iteration: 9154; loss: 0.16782015562057495\n",
            "Iteration: 9155; loss: 0.10837863385677338\n",
            "Iteration: 9156; loss: 0.2317051887512207\n",
            "Iteration: 9157; loss: 0.1330665498971939\n",
            "Iteration: 9158; loss: 0.1521058976650238\n",
            "Iteration: 9159; loss: 0.17416912317276\n",
            "Iteration: 9160; loss: 0.15950757265090942\n",
            "Iteration: 9161; loss: 0.21167908608913422\n",
            "Iteration: 9162; loss: 0.1695462167263031\n",
            "Iteration: 9163; loss: 0.1789565086364746\n",
            "Iteration: 9164; loss: 0.19852644205093384\n",
            "Iteration: 9165; loss: 0.1946280300617218\n",
            "Iteration: 9166; loss: 0.16114291548728943\n",
            "Iteration: 9167; loss: 0.1575947254896164\n",
            "Iteration: 9168; loss: 0.19398091733455658\n",
            "Iteration: 9169; loss: 0.2681669592857361\n",
            "Iteration: 9170; loss: 0.18579666316509247\n",
            "Iteration: 9171; loss: 0.18779712915420532\n",
            "Iteration: 9172; loss: 0.19444791972637177\n",
            "Iteration: 9173; loss: 0.17896045744419098\n",
            "Iteration: 9174; loss: 0.12291868031024933\n",
            "Iteration: 9175; loss: 0.18265105783939362\n",
            "Iteration: 9176; loss: 0.14303110539913177\n",
            "Iteration: 9177; loss: 0.22805801033973694\n",
            "Iteration: 9178; loss: 0.16898761689662933\n",
            "Iteration: 9179; loss: 0.16187310218811035\n",
            "Iteration: 9180; loss: 0.22913438081741333\n",
            "Iteration: 9181; loss: 0.2026013433933258\n",
            "Iteration: 9182; loss: 0.23834389448165894\n",
            "Iteration: 9183; loss: 0.16434814035892487\n",
            "Iteration: 9184; loss: 0.13309426605701447\n",
            "Iteration: 9185; loss: 0.17479027807712555\n",
            "Iteration: 9186; loss: 0.18141742050647736\n",
            "Iteration: 9187; loss: 0.20494791865348816\n",
            "Iteration: 9188; loss: 0.30354535579681396\n",
            "Iteration: 9189; loss: 0.1393030732870102\n",
            "Iteration: 9190; loss: 0.18152114748954773\n",
            "Iteration: 9191; loss: 0.16830627620220184\n",
            "Iteration: 9192; loss: 0.13810372352600098\n",
            "Iteration: 9193; loss: 0.15444453060626984\n",
            "Iteration: 9194; loss: 0.18679216504096985\n",
            "Iteration: 9195; loss: 0.1431071013212204\n",
            "Iteration: 9196; loss: 0.1972728669643402\n",
            "Iteration: 9197; loss: 0.1549130231142044\n",
            "Iteration: 9198; loss: 0.13142302632331848\n",
            "Iteration: 9199; loss: 0.22500501573085785\n",
            "Iteration: 9200; loss: 0.21952253580093384\n",
            "Iteration: 9201; loss: 0.142438143491745\n",
            "Iteration: 9202; loss: 0.24477054178714752\n",
            "Iteration: 9203; loss: 0.2147805392742157\n",
            "Iteration: 9204; loss: 0.170762836933136\n",
            "Iteration: 9205; loss: 0.1613144427537918\n",
            "Iteration: 9206; loss: 0.17239728569984436\n",
            "Iteration: 9207; loss: 0.1610257625579834\n",
            "Iteration: 9208; loss: 0.1426805555820465\n",
            "Iteration: 9209; loss: 0.15739119052886963\n",
            "Iteration: 9210; loss: 0.13734477758407593\n",
            "Iteration: 9211; loss: 0.14842170476913452\n",
            "Iteration: 9212; loss: 0.16392934322357178\n",
            "Iteration: 9213; loss: 0.1546865850687027\n",
            "Iteration: 9214; loss: 0.10383138805627823\n",
            "Iteration: 9215; loss: 0.11431730538606644\n",
            "Iteration: 9216; loss: 0.11816473305225372\n",
            "Iteration: 9217; loss: 0.1013268530368805\n",
            "Iteration: 9218; loss: 0.1278851181268692\n",
            "Iteration: 9219; loss: 0.11344040930271149\n",
            "Iteration: 9220; loss: 0.11906313896179199\n",
            "Iteration: 9221; loss: 0.10657484829425812\n",
            "Iteration: 9222; loss: 0.15607675909996033\n",
            "Iteration: 9223; loss: 0.15872488915920258\n",
            "Iteration: 9224; loss: 0.0870908796787262\n",
            "Iteration: 9225; loss: 0.15589101612567902\n",
            "Iteration: 9226; loss: 0.13552220165729523\n",
            "Iteration: 9227; loss: 0.1498524397611618\n",
            "Iteration: 9228; loss: 0.1631316840648651\n",
            "Iteration: 9229; loss: 0.17333616316318512\n",
            "Iteration: 9230; loss: 0.13024410605430603\n",
            "Iteration: 9231; loss: 0.10944356769323349\n",
            "Iteration: 9232; loss: 0.13052117824554443\n",
            "Iteration: 9233; loss: 0.13539175689220428\n",
            "Iteration: 9234; loss: 0.08562951534986496\n",
            "Iteration: 9235; loss: 0.09432832151651382\n",
            "Iteration: 9236; loss: 0.1426512748003006\n",
            "Iteration: 9237; loss: 0.11899925023317337\n",
            "Iteration: 9238; loss: 0.16224369406700134\n",
            "Iteration: 9239; loss: 0.1493835598230362\n",
            "Iteration: 9240; loss: 0.18462656438350677\n",
            "Iteration: 9241; loss: 0.12287408858537674\n",
            "Iteration: 9242; loss: 0.13522139191627502\n",
            "Iteration: 9243; loss: 0.17744038999080658\n",
            "Iteration: 9244; loss: 0.16923612356185913\n",
            "Iteration: 9245; loss: 0.2238936424255371\n",
            "Iteration: 9246; loss: 0.21589617431163788\n",
            "Iteration: 9247; loss: 0.1053607240319252\n",
            "Iteration: 9248; loss: 0.13728253543376923\n",
            "Iteration: 9249; loss: 0.2272154539823532\n",
            "Iteration: 9250; loss: 0.13749271631240845\n",
            "Iteration: 9251; loss: 0.1481315642595291\n",
            "Iteration: 9252; loss: 0.13067667186260223\n",
            "Iteration: 9253; loss: 0.12183639407157898\n",
            "Iteration: 9254; loss: 0.11956360191106796\n",
            "Iteration: 9255; loss: 0.15931357443332672\n",
            "Iteration: 9256; loss: 0.1746494024991989\n",
            "Iteration: 9257; loss: 0.09956352412700653\n",
            "Iteration: 9258; loss: 0.12580925226211548\n",
            "Iteration: 9259; loss: 0.1389499455690384\n",
            "Iteration: 9260; loss: 0.09272465854883194\n",
            "Iteration: 9261; loss: 0.10929246246814728\n",
            "Iteration: 9262; loss: 0.09836046397686005\n",
            "Iteration: 9263; loss: 0.1343303918838501\n",
            "Iteration: 9264; loss: 0.11102505028247833\n",
            "Iteration: 9265; loss: 0.09805548936128616\n",
            "Iteration: 9266; loss: 0.08801392465829849\n",
            "Iteration: 9267; loss: 0.1138034462928772\n",
            "Iteration: 9268; loss: 0.12451547384262085\n",
            "Iteration: 9269; loss: 0.11843990534543991\n",
            "Iteration: 9270; loss: 0.11491495370864868\n",
            "Iteration: 9271; loss: 0.12822678685188293\n",
            "Iteration: 9272; loss: 0.07911021262407303\n",
            "Iteration: 9273; loss: 0.053187593817710876\n",
            "Iteration: 9274; loss: 0.08740610629320145\n",
            "Iteration: 9275; loss: 0.10488755255937576\n",
            "Iteration: 9276; loss: 0.10242606699466705\n",
            "Iteration: 9277; loss: 0.10478220134973526\n",
            "Iteration: 9278; loss: 0.08700726181268692\n",
            "Iteration: 9279; loss: 0.13164891302585602\n",
            "Iteration: 9280; loss: 0.08532284200191498\n",
            "Iteration: 9281; loss: 0.09267807751893997\n",
            "Iteration: 9282; loss: 0.10835227370262146\n",
            "Iteration: 9283; loss: 0.08265893161296844\n",
            "Iteration: 9284; loss: 0.09016921371221542\n",
            "Iteration: 9285; loss: 0.11471758782863617\n",
            "Iteration: 9286; loss: 0.10609667003154755\n",
            "Iteration: 9287; loss: 0.1048424020409584\n",
            "Iteration: 9288; loss: 0.1310187429189682\n",
            "Iteration: 9289; loss: 0.09101958572864532\n",
            "Iteration: 9290; loss: 0.10196274518966675\n",
            "Iteration: 9291; loss: 0.18816964328289032\n",
            "Iteration: 9292; loss: 0.11575860530138016\n",
            "Iteration: 9293; loss: 0.13692796230316162\n",
            "Iteration: 9294; loss: 0.07729578018188477\n",
            "Iteration: 9295; loss: 0.08615356683731079\n",
            "Iteration: 9296; loss: 0.14566576480865479\n",
            "Iteration: 9297; loss: 0.1766546219587326\n",
            "Iteration: 9298; loss: 0.10914342105388641\n",
            "Iteration: 9299; loss: 0.08078087866306305\n",
            "Iteration: 9300; loss: 0.07080022990703583\n",
            "Iteration: 9301; loss: 0.06751278042793274\n",
            "Iteration: 9302; loss: 0.09316623210906982\n",
            "Iteration: 9303; loss: 0.10568586736917496\n",
            "Iteration: 9304; loss: 0.07293897867202759\n",
            "Iteration: 9305; loss: 0.13829770684242249\n",
            "Iteration: 9306; loss: 0.1647404432296753\n",
            "Iteration: 9307; loss: 0.07164421677589417\n",
            "Iteration: 9308; loss: 0.11236672103404999\n",
            "Iteration: 9309; loss: 0.12489953637123108\n",
            "Iteration: 9310; loss: 0.08166523277759552\n",
            "Iteration: 9311; loss: 0.10986906290054321\n",
            "Iteration: 9312; loss: 0.08418511599302292\n",
            "Iteration: 9313; loss: 0.11799094080924988\n",
            "Iteration: 9314; loss: 0.0829804465174675\n",
            "Iteration: 9315; loss: 0.11372075974941254\n",
            "Iteration: 9316; loss: 0.0674331858754158\n",
            "Iteration: 9317; loss: 0.1254015862941742\n",
            "Iteration: 9318; loss: 0.11067158728837967\n",
            "Iteration: 9319; loss: 0.06488107144832611\n",
            "Iteration: 9320; loss: 0.12605762481689453\n",
            "Iteration: 9321; loss: 0.11016329377889633\n",
            "Iteration: 9322; loss: 0.19510644674301147\n",
            "Iteration: 9323; loss: 0.12216754257678986\n",
            "Iteration: 9324; loss: 0.12094926834106445\n",
            "Iteration: 9325; loss: 0.10379981994628906\n",
            "Iteration: 9326; loss: 0.12157642096281052\n",
            "Iteration: 9327; loss: 0.054770998656749725\n",
            "Iteration: 9328; loss: 0.12390724569559097\n",
            "Iteration: 9329; loss: 0.1007179468870163\n",
            "Iteration: 9330; loss: 0.06855615973472595\n",
            "Iteration: 9331; loss: 0.1550425887107849\n",
            "Iteration: 9332; loss: 0.07510128617286682\n",
            "Iteration: 9333; loss: 0.10136531293392181\n",
            "Iteration: 9334; loss: 0.07669688761234283\n",
            "Iteration: 9335; loss: 0.10383529961109161\n",
            "Iteration: 9336; loss: 0.08392748236656189\n",
            "Iteration: 9337; loss: 0.10505351424217224\n",
            "Iteration: 9338; loss: 0.09721039235591888\n",
            "Iteration: 9339; loss: 0.07899624109268188\n",
            "Iteration: 9340; loss: 0.10740703344345093\n",
            "Iteration: 9341; loss: 0.07224442809820175\n",
            "Iteration: 9342; loss: 0.07147502154111862\n",
            "Iteration: 9343; loss: 0.0534844771027565\n",
            "Iteration: 9344; loss: 0.07662371546030045\n",
            "Iteration: 9345; loss: 0.07692733407020569\n",
            "Iteration: 9346; loss: 0.07743235677480698\n",
            "Iteration: 9347; loss: 0.11354529112577438\n",
            "Iteration: 9348; loss: 0.06570203602313995\n",
            "Iteration: 9349; loss: 0.06734064966440201\n",
            "Iteration: 9350; loss: 0.095161572098732\n",
            "Iteration: 9351; loss: 0.09817961603403091\n",
            "Iteration: 9352; loss: 0.11564457416534424\n",
            "Iteration: 9353; loss: 0.09921558946371078\n",
            "Iteration: 9354; loss: 0.09947873651981354\n",
            "Iteration: 9355; loss: 0.114841528236866\n",
            "Iteration: 9356; loss: 0.09066362679004669\n",
            "Iteration: 9357; loss: 0.06604564189910889\n",
            "Iteration: 9358; loss: 0.1419525444507599\n",
            "Iteration: 9359; loss: 0.0914074033498764\n",
            "Iteration: 9360; loss: 0.08612357825040817\n",
            "Iteration: 9361; loss: 0.11190839856863022\n",
            "Iteration: 9362; loss: 0.07278110831975937\n",
            "Iteration: 9363; loss: 0.1373385787010193\n",
            "Iteration: 9364; loss: 0.10899287462234497\n",
            "Iteration: 9365; loss: 0.1282763034105301\n",
            "Iteration: 9366; loss: 0.05961288511753082\n",
            "Iteration: 9367; loss: 0.06000044569373131\n",
            "Iteration: 9368; loss: 0.06982378661632538\n",
            "Iteration: 9369; loss: 0.10592079162597656\n",
            "Iteration: 9370; loss: 0.06381173431873322\n",
            "Iteration: 9371; loss: 0.08449431508779526\n",
            "Iteration: 9372; loss: 0.06909503787755966\n",
            "Iteration: 9373; loss: 0.0481850765645504\n",
            "Iteration: 9374; loss: 0.0695786401629448\n",
            "Iteration: 9375; loss: 0.05454270914196968\n",
            "Iteration: 9376; loss: 0.11152918636798859\n",
            "Iteration: 9377; loss: 0.0916755348443985\n",
            "Iteration: 9378; loss: 0.07848799228668213\n",
            "Iteration: 9379; loss: 0.06706629693508148\n",
            "Iteration: 9380; loss: 0.058153800666332245\n",
            "Iteration: 9381; loss: 0.11113443970680237\n",
            "Iteration: 9382; loss: 0.12314329296350479\n",
            "Iteration: 9383; loss: 0.07637729495763779\n",
            "Iteration: 9384; loss: 0.06015970930457115\n",
            "Iteration: 9385; loss: 0.05838071554899216\n",
            "Iteration: 9386; loss: 0.05913831293582916\n",
            "Iteration: 9387; loss: 0.07313396036624908\n",
            "Iteration: 9388; loss: 0.11383012682199478\n",
            "Iteration: 9389; loss: 0.07093143463134766\n",
            "Iteration: 9390; loss: 0.08524326980113983\n",
            "Iteration: 9391; loss: 0.043738704174757004\n",
            "Iteration: 9392; loss: 0.07626291364431381\n",
            "Iteration: 9393; loss: 0.1190582662820816\n",
            "Iteration: 9394; loss: 0.11297671496868134\n",
            "Iteration: 9395; loss: 0.029414154589176178\n",
            "Iteration: 9396; loss: 0.06292402744293213\n",
            "Iteration: 9397; loss: 0.07907671481370926\n",
            "Iteration: 9398; loss: 0.028509607538580894\n",
            "Iteration: 9399; loss: 0.12136703729629517\n",
            "Iteration: 9400; loss: 0.08420629799365997\n",
            "Iteration: 9401; loss: 0.07911422103643417\n",
            "Iteration: 9402; loss: 0.08733345568180084\n",
            "Iteration: 9403; loss: 0.0980796292424202\n",
            "Iteration: 9404; loss: 0.06815832853317261\n",
            "Iteration: 9405; loss: 0.07424222677946091\n",
            "Iteration: 9406; loss: 0.058814581483602524\n",
            "Iteration: 9407; loss: 0.06794145703315735\n",
            "Iteration: 9408; loss: 0.08387932181358337\n",
            "Iteration: 9409; loss: 0.0460454598069191\n",
            "Iteration: 9410; loss: 0.058577973395586014\n",
            "Iteration: 9411; loss: 0.10793638229370117\n",
            "Iteration: 9412; loss: 0.051448918879032135\n",
            "Iteration: 9413; loss: 0.07600659132003784\n",
            "Iteration: 9414; loss: 0.07039283215999603\n",
            "Iteration: 9415; loss: 0.10181823372840881\n",
            "Iteration: 9416; loss: 0.09161533415317535\n",
            "Iteration: 9417; loss: 0.0939326137304306\n",
            "Iteration: 9418; loss: 0.07446372509002686\n",
            "Iteration: 9419; loss: 0.0979481041431427\n",
            "Iteration: 9420; loss: 0.0929340124130249\n",
            "Iteration: 9421; loss: 0.09098006784915924\n",
            "Iteration: 9422; loss: 0.13295763731002808\n",
            "Iteration: 9423; loss: 0.057215772569179535\n",
            "Iteration: 9424; loss: 0.05504726618528366\n",
            "Iteration: 9425; loss: 0.07129740715026855\n",
            "Iteration: 9426; loss: 0.050597745925188065\n",
            "Iteration: 9427; loss: 0.10881301760673523\n",
            "Iteration: 9428; loss: 0.05953235924243927\n",
            "Iteration: 9429; loss: 0.05827246233820915\n",
            "Iteration: 9430; loss: 0.10371848940849304\n",
            "Iteration: 9431; loss: 0.07473836094141006\n",
            "Iteration: 9432; loss: 0.11517883837223053\n",
            "Iteration: 9433; loss: 0.08240474760532379\n",
            "Iteration: 9434; loss: 0.05270650237798691\n",
            "Iteration: 9435; loss: 0.06088817119598389\n",
            "Iteration: 9436; loss: 0.06459061056375504\n",
            "Iteration: 9437; loss: 0.08772306889295578\n",
            "Iteration: 9438; loss: 0.12309882789850235\n",
            "Iteration: 9439; loss: 0.06794210523366928\n",
            "Iteration: 9440; loss: 0.046130817383527756\n",
            "Iteration: 9441; loss: 0.09332957863807678\n",
            "Iteration: 9442; loss: 0.09028889238834381\n",
            "Iteration: 9443; loss: 0.08551458269357681\n",
            "Iteration: 9444; loss: 0.12597741186618805\n",
            "Iteration: 9445; loss: 0.10408273339271545\n",
            "Iteration: 9446; loss: 0.11798462271690369\n",
            "Iteration: 9447; loss: 0.06051654368638992\n",
            "Iteration: 9448; loss: 0.07345632463693619\n",
            "Iteration: 9449; loss: 0.07962961494922638\n",
            "Iteration: 9450; loss: 0.09940600395202637\n",
            "Iteration: 9451; loss: 0.06910629570484161\n",
            "Iteration: 9452; loss: 0.07116788625717163\n",
            "Iteration: 9453; loss: 0.06779473274946213\n",
            "Iteration: 9454; loss: 0.109613798558712\n",
            "Iteration: 9455; loss: 0.06611695885658264\n",
            "Iteration: 9456; loss: 0.09847482293844223\n",
            "Iteration: 9457; loss: 0.04840799793601036\n",
            "Iteration: 9458; loss: 0.02798265963792801\n",
            "Iteration: 9459; loss: 0.13048289716243744\n",
            "Iteration: 9460; loss: 0.09077933430671692\n",
            "Iteration: 9461; loss: 0.043399594724178314\n",
            "Iteration: 9462; loss: 0.09835204482078552\n",
            "Iteration: 9463; loss: 0.04391422122716904\n",
            "Iteration: 9464; loss: 0.05114912614226341\n",
            "Iteration: 9465; loss: 0.053978949785232544\n",
            "Iteration: 9466; loss: 0.050506867468357086\n",
            "Iteration: 9467; loss: 0.05979946628212929\n",
            "Iteration: 9468; loss: 0.06911434233188629\n",
            "Iteration: 9469; loss: 0.11370454728603363\n",
            "Iteration: 9470; loss: 0.0661403238773346\n",
            "Iteration: 9471; loss: 0.042423129081726074\n",
            "Iteration: 9472; loss: 0.06349438428878784\n",
            "Iteration: 9473; loss: 0.06918573379516602\n",
            "Iteration: 9474; loss: 0.037833016365766525\n",
            "Iteration: 9475; loss: 0.04628776013851166\n",
            "Iteration: 9476; loss: 0.06224406510591507\n",
            "Iteration: 9477; loss: 0.10411140322685242\n",
            "Iteration: 9478; loss: 0.031287048012018204\n",
            "Iteration: 9479; loss: 0.07961455732584\n",
            "Iteration: 9480; loss: 0.05336654186248779\n",
            "Iteration: 9481; loss: 0.052799567580223083\n",
            "Iteration: 9482; loss: 0.1034175381064415\n",
            "Iteration: 9483; loss: 0.06751271337270737\n",
            "Iteration: 9484; loss: 0.08795834332704544\n",
            "Iteration: 9485; loss: 0.06979132443666458\n",
            "Iteration: 9486; loss: 0.08240222930908203\n",
            "Iteration: 9487; loss: 0.06587512791156769\n",
            "Iteration: 9488; loss: 0.09848030656576157\n",
            "Iteration: 9489; loss: 0.03462734445929527\n",
            "Iteration: 9490; loss: 0.04606543481349945\n",
            "Iteration: 9491; loss: 0.034415118396282196\n",
            "Iteration: 9492; loss: 0.04142679274082184\n",
            "Iteration: 9493; loss: 0.05886330083012581\n",
            "Iteration: 9494; loss: 0.04147927835583687\n",
            "Iteration: 9495; loss: 0.05741216614842415\n",
            "Iteration: 9496; loss: 0.10808764398097992\n",
            "Iteration: 9497; loss: 0.05239533632993698\n",
            "Iteration: 9498; loss: 0.09231752157211304\n",
            "Iteration: 9499; loss: 0.05402718484401703\n",
            "Iteration: 9500; loss: 0.05640455707907677\n",
            "Iteration: 9501; loss: 0.033537350594997406\n",
            "Iteration: 9502; loss: 0.1123942881822586\n",
            "Iteration: 9503; loss: 0.035631392151117325\n",
            "Iteration: 9504; loss: 0.06785811483860016\n",
            "Iteration: 9505; loss: 0.1189834401011467\n",
            "Iteration: 9506; loss: 0.08471211045980453\n",
            "Iteration: 9507; loss: 0.09132447838783264\n",
            "Iteration: 9508; loss: 0.06513147056102753\n",
            "Iteration: 9509; loss: 0.08327757567167282\n",
            "Iteration: 9510; loss: 0.10088638961315155\n",
            "Iteration: 9511; loss: 0.09725867211818695\n",
            "Iteration: 9512; loss: 0.03269796818494797\n",
            "Iteration: 9513; loss: 0.09035409986972809\n",
            "Iteration: 9514; loss: 0.0321904793381691\n",
            "Iteration: 9515; loss: 0.0682835653424263\n",
            "Iteration: 9516; loss: 0.08693724870681763\n",
            "Iteration: 9517; loss: 0.055531371384859085\n",
            "Iteration: 9518; loss: 0.12170767039060593\n",
            "Iteration: 9519; loss: 0.06034620478749275\n",
            "Iteration: 9520; loss: 0.06075605750083923\n",
            "Iteration: 9521; loss: 0.10617071390151978\n",
            "Iteration: 9522; loss: 0.08300952613353729\n",
            "Iteration: 9523; loss: 0.06539148837327957\n",
            "Iteration: 9524; loss: 0.07885530591011047\n",
            "Iteration: 9525; loss: 0.06549931317567825\n",
            "Iteration: 9526; loss: 0.048761747777462006\n",
            "Iteration: 9527; loss: 0.08937140554189682\n",
            "Iteration: 9528; loss: 0.0514848455786705\n",
            "Iteration: 9529; loss: 0.052559833973646164\n",
            "Iteration: 9530; loss: 0.04272238537669182\n",
            "Iteration: 9531; loss: 0.044182851910591125\n",
            "Iteration: 9532; loss: 0.06982942670583725\n",
            "Iteration: 9533; loss: 0.12780705094337463\n",
            "Iteration: 9534; loss: 0.061184801161289215\n",
            "Iteration: 9535; loss: 0.03993833437561989\n",
            "Iteration: 9536; loss: 0.042526111006736755\n",
            "Iteration: 9537; loss: 0.06220293790102005\n",
            "Iteration: 9538; loss: 0.10746387392282486\n",
            "Iteration: 9539; loss: 0.05744347721338272\n",
            "Iteration: 9540; loss: 0.028519369661808014\n",
            "Iteration: 9541; loss: 0.0499444417655468\n",
            "Iteration: 9542; loss: 0.056191135197877884\n",
            "Iteration: 9543; loss: 0.06542972475290298\n",
            "Iteration: 9544; loss: 0.05236796289682388\n",
            "Iteration: 9545; loss: 0.06250283122062683\n",
            "Iteration: 9546; loss: 0.03998427838087082\n",
            "Iteration: 9547; loss: 0.029501713812351227\n",
            "Iteration: 9548; loss: 0.07168450206518173\n",
            "Iteration: 9549; loss: 0.0772707611322403\n",
            "Iteration: 9550; loss: 0.033720239996910095\n",
            "Iteration: 9551; loss: 0.05893036723136902\n",
            "Iteration: 9552; loss: 0.03220421075820923\n",
            "Iteration: 9553; loss: 0.1353171318769455\n",
            "Iteration: 9554; loss: 0.060668960213661194\n",
            "Iteration: 9555; loss: 0.04321658983826637\n",
            "Iteration: 9556; loss: 0.08004465699195862\n",
            "Iteration: 9557; loss: 0.038423363119363785\n",
            "Iteration: 9558; loss: 0.059063658118247986\n",
            "Iteration: 9559; loss: 0.026860451325774193\n",
            "Iteration: 9560; loss: 0.08307847380638123\n",
            "Iteration: 9561; loss: 0.03731560707092285\n",
            "Iteration: 9562; loss: 0.031213367357850075\n",
            "Iteration: 9563; loss: 0.034981414675712585\n",
            "Iteration: 9564; loss: 0.04838598892092705\n",
            "Iteration: 9565; loss: 0.04391670227050781\n",
            "Iteration: 9566; loss: 0.035813912749290466\n",
            "Iteration: 9567; loss: 0.05435784533619881\n",
            "Iteration: 9568; loss: 0.04626115411520004\n",
            "Iteration: 9569; loss: 0.056153714656829834\n",
            "Iteration: 9570; loss: 0.1517697125673294\n",
            "Iteration: 9571; loss: 0.05566919222474098\n",
            "Iteration: 9572; loss: 0.024349380284547806\n",
            "Iteration: 9573; loss: 0.0794449970126152\n",
            "Iteration: 9574; loss: 0.028346996754407883\n",
            "Iteration: 9575; loss: 0.02717818319797516\n",
            "Iteration: 9576; loss: 0.05741192027926445\n",
            "Iteration: 9577; loss: 0.03587869554758072\n",
            "Iteration: 9578; loss: 0.03260456770658493\n",
            "Iteration: 9579; loss: 0.04030398651957512\n",
            "Iteration: 9580; loss: 0.040710389614105225\n",
            "Iteration: 9581; loss: 0.04580041393637657\n",
            "Iteration: 9582; loss: 0.032884690910577774\n",
            "Iteration: 9583; loss: 0.05122927576303482\n",
            "Iteration: 9584; loss: 0.05775341019034386\n",
            "Iteration: 9585; loss: 0.04866676777601242\n",
            "Iteration: 9586; loss: 0.05627156049013138\n",
            "Iteration: 9587; loss: 0.05078204721212387\n",
            "Iteration: 9588; loss: 0.035137638449668884\n",
            "Iteration: 9589; loss: 0.08686695247888565\n",
            "Iteration: 9590; loss: 0.02781164087355137\n",
            "Iteration: 9591; loss: 0.058249086141586304\n",
            "Iteration: 9592; loss: 0.09123007953166962\n",
            "Iteration: 9593; loss: 0.05216130241751671\n",
            "Iteration: 9594; loss: 0.08249995857477188\n",
            "Iteration: 9595; loss: 0.027045367285609245\n",
            "Iteration: 9596; loss: 0.12925201654434204\n",
            "Iteration: 9597; loss: 0.06407374888658524\n",
            "Iteration: 9598; loss: 0.1357707679271698\n",
            "Iteration: 9599; loss: 0.0651819258928299\n",
            "Iteration: 9600; loss: 0.053682904690504074\n",
            "Iteration: 9601; loss: 0.06587203592061996\n",
            "Iteration: 9602; loss: 0.03494783863425255\n",
            "Iteration: 9603; loss: 0.0730644091963768\n",
            "Iteration: 9604; loss: 0.07413637638092041\n",
            "Iteration: 9605; loss: 0.08827322721481323\n",
            "Iteration: 9606; loss: 0.05614810064435005\n",
            "Iteration: 9607; loss: 0.07913418114185333\n",
            "Iteration: 9608; loss: 0.04749911278486252\n",
            "Iteration: 9609; loss: 0.04837627708911896\n",
            "Iteration: 9610; loss: 0.05174059420824051\n",
            "Iteration: 9611; loss: 0.03378576040267944\n",
            "Iteration: 9612; loss: 0.0740855485200882\n",
            "Iteration: 9613; loss: 0.07160023599863052\n",
            "Iteration: 9614; loss: 0.029009435325860977\n",
            "Iteration: 9615; loss: 0.0325346440076828\n",
            "Iteration: 9616; loss: 0.058380126953125\n",
            "Iteration: 9617; loss: 0.04268888011574745\n",
            "Iteration: 9618; loss: 0.04396183043718338\n",
            "Iteration: 9619; loss: 0.06785772740840912\n",
            "Iteration: 9620; loss: 0.02954508736729622\n",
            "Iteration: 9621; loss: 0.036468543112277985\n",
            "Iteration: 9622; loss: 0.0651547983288765\n",
            "Iteration: 9623; loss: 0.04376395791769028\n",
            "Iteration: 9624; loss: 0.10004576295614243\n",
            "Iteration: 9625; loss: 0.035141803324222565\n",
            "Iteration: 9626; loss: 0.1682075709104538\n",
            "Iteration: 9627; loss: 0.057887423783540726\n",
            "Iteration: 9628; loss: 0.07000989466905594\n",
            "Iteration: 9629; loss: 0.08155881613492966\n",
            "Iteration: 9630; loss: 0.0786275714635849\n",
            "Iteration: 9631; loss: 0.025593740865588188\n",
            "Iteration: 9632; loss: 0.07787953317165375\n",
            "Iteration: 9633; loss: 0.051225777715444565\n",
            "Iteration: 9634; loss: 0.049215465784072876\n",
            "Iteration: 9635; loss: 0.04240782558917999\n",
            "Iteration: 9636; loss: 0.02414269745349884\n",
            "Iteration: 9637; loss: 0.08331260085105896\n",
            "Iteration: 9638; loss: 0.03475438803434372\n",
            "Iteration: 9639; loss: 0.08621757477521896\n",
            "Iteration: 9640; loss: 0.10685186088085175\n",
            "Iteration: 9641; loss: 0.051258232444524765\n",
            "Iteration: 9642; loss: 0.045989930629730225\n",
            "Iteration: 9643; loss: 0.08312344551086426\n",
            "Iteration: 9644; loss: 0.018081869930028915\n",
            "Iteration: 9645; loss: 0.08071495592594147\n",
            "Iteration: 9646; loss: 0.08701249957084656\n",
            "Iteration: 9647; loss: 0.031053025275468826\n",
            "Iteration: 9648; loss: 0.019684532657265663\n",
            "Iteration: 9649; loss: 0.04000177979469299\n",
            "Iteration: 9650; loss: 0.06661534309387207\n",
            "Iteration: 9651; loss: 0.06138480827212334\n",
            "Iteration: 9652; loss: 0.057826656848192215\n",
            "Iteration: 9653; loss: 0.09676803648471832\n",
            "Iteration: 9654; loss: 0.08094829320907593\n",
            "Iteration: 9655; loss: 0.024346135556697845\n",
            "Iteration: 9656; loss: 0.0854051485657692\n",
            "Iteration: 9657; loss: 0.044482193887233734\n",
            "Iteration: 9658; loss: 0.05001835525035858\n",
            "Iteration: 9659; loss: 0.0593622587621212\n",
            "Iteration: 9660; loss: 0.0208806823939085\n",
            "Iteration: 9661; loss: 0.03244785964488983\n",
            "Iteration: 9662; loss: 0.08279287070035934\n",
            "Iteration: 9663; loss: 0.09450911730527878\n",
            "Iteration: 9664; loss: 0.02610689587891102\n",
            "Iteration: 9665; loss: 0.049478888511657715\n",
            "Iteration: 9666; loss: 0.019204478710889816\n",
            "Iteration: 9667; loss: 0.04086332395672798\n",
            "Iteration: 9668; loss: 0.03599056228995323\n",
            "Iteration: 9669; loss: 0.08670508861541748\n",
            "Iteration: 9670; loss: 0.07348563522100449\n",
            "Iteration: 9671; loss: 0.025408707559108734\n",
            "Iteration: 9672; loss: 0.05174694210290909\n",
            "Iteration: 9673; loss: 0.03380589932203293\n",
            "Iteration: 9674; loss: 0.04112383723258972\n",
            "Iteration: 9675; loss: 0.06395755708217621\n",
            "Iteration: 9676; loss: 0.04165441542863846\n",
            "Iteration: 9677; loss: 0.04405587911605835\n",
            "Iteration: 9678; loss: 0.15940335392951965\n",
            "Iteration: 9679; loss: 0.021017251536250114\n",
            "Iteration: 9680; loss: 0.049333054572343826\n",
            "Iteration: 9681; loss: 0.08687050640583038\n",
            "Iteration: 9682; loss: 0.03445381671190262\n",
            "Iteration: 9683; loss: 0.02732604369521141\n",
            "Iteration: 9684; loss: 0.10207206010818481\n",
            "Iteration: 9685; loss: 0.0362870991230011\n",
            "Iteration: 9686; loss: 0.060112498700618744\n",
            "Iteration: 9687; loss: 0.06407085061073303\n",
            "Iteration: 9688; loss: 0.11334143579006195\n",
            "Iteration: 9689; loss: 0.047025781124830246\n",
            "Iteration: 9690; loss: 0.08606457710266113\n",
            "Iteration: 9691; loss: 0.0463949553668499\n",
            "Iteration: 9692; loss: 0.02052401192486286\n",
            "Iteration: 9693; loss: 0.051327791064977646\n",
            "Iteration: 9694; loss: 0.025647535920143127\n",
            "Iteration: 9695; loss: 0.03643282130360603\n",
            "Iteration: 9696; loss: 0.02658892050385475\n",
            "Iteration: 9697; loss: 0.0489775650203228\n",
            "Iteration: 9698; loss: 0.04251004382967949\n",
            "Iteration: 9699; loss: 0.08223387598991394\n",
            "Iteration: 9700; loss: 0.07554122060537338\n",
            "Iteration: 9701; loss: 0.04643494263291359\n",
            "Iteration: 9702; loss: 0.03991466760635376\n",
            "Iteration: 9703; loss: 0.019683852791786194\n",
            "Iteration: 9704; loss: 0.042154740542173386\n",
            "Iteration: 9705; loss: 0.09421581774950027\n",
            "Iteration: 9706; loss: 0.052328139543533325\n",
            "Iteration: 9707; loss: 0.12838271260261536\n",
            "Iteration: 9708; loss: 0.08612953126430511\n",
            "Iteration: 9709; loss: 0.05916844308376312\n",
            "Iteration: 9710; loss: 0.041831232607364655\n",
            "Iteration: 9711; loss: 0.07243809849023819\n",
            "Iteration: 9712; loss: 0.04092145711183548\n",
            "Iteration: 9713; loss: 0.019477207213640213\n",
            "Iteration: 9714; loss: 0.025581251829862595\n",
            "Iteration: 9715; loss: 0.046842847019433975\n",
            "Iteration: 9716; loss: 0.025903906673192978\n",
            "Iteration: 9717; loss: 0.06301678717136383\n",
            "Iteration: 9718; loss: 0.0345441997051239\n",
            "Iteration: 9719; loss: 0.0738379955291748\n",
            "Iteration: 9720; loss: 0.062324877828359604\n",
            "Iteration: 9721; loss: 0.07012706995010376\n",
            "Iteration: 9722; loss: 0.023291822522878647\n",
            "Iteration: 9723; loss: 0.05034768953919411\n",
            "Iteration: 9724; loss: 0.04966295138001442\n",
            "Iteration: 9725; loss: 0.024289369583129883\n",
            "Iteration: 9726; loss: 0.04943856596946716\n",
            "Iteration: 9727; loss: 0.04918583482503891\n",
            "Iteration: 9728; loss: 0.04399653896689415\n",
            "Iteration: 9729; loss: 0.034619737416505814\n",
            "Iteration: 9730; loss: 0.03446696698665619\n",
            "Iteration: 9731; loss: 0.025187231600284576\n",
            "Iteration: 9732; loss: 0.018056929111480713\n",
            "Iteration: 9733; loss: 0.0642165094614029\n",
            "Iteration: 9734; loss: 0.034191880375146866\n",
            "Iteration: 9735; loss: 0.06188296154141426\n",
            "Iteration: 9736; loss: 0.060032084584236145\n",
            "Iteration: 9737; loss: 0.022729938849806786\n",
            "Iteration: 9738; loss: 0.07911760360002518\n",
            "Iteration: 9739; loss: 0.02690405212342739\n",
            "Iteration: 9740; loss: 0.049317486584186554\n",
            "Iteration: 9741; loss: 0.12380264699459076\n",
            "Iteration: 9742; loss: 0.03292860835790634\n",
            "Iteration: 9743; loss: 0.026420071721076965\n",
            "Iteration: 9744; loss: 0.024240463972091675\n",
            "Iteration: 9745; loss: 0.042273592203855515\n",
            "Iteration: 9746; loss: 0.04787161573767662\n",
            "Iteration: 9747; loss: 0.048571955412626266\n",
            "Iteration: 9748; loss: 0.0262129008769989\n",
            "Iteration: 9749; loss: 0.055556535720825195\n",
            "Iteration: 9750; loss: 0.10999033600091934\n",
            "Iteration: 9751; loss: 0.0387243926525116\n",
            "Iteration: 9752; loss: 0.04035680741071701\n",
            "Iteration: 9753; loss: 0.044514451175928116\n",
            "Iteration: 9754; loss: 0.043666452169418335\n",
            "Iteration: 9755; loss: 0.07090913504362106\n",
            "Iteration: 9756; loss: 0.05177140608429909\n",
            "Iteration: 9757; loss: 0.043485622853040695\n",
            "Iteration: 9758; loss: 0.040480464696884155\n",
            "Iteration: 9759; loss: 0.02772187441587448\n",
            "Iteration: 9760; loss: 0.0720868781208992\n",
            "Iteration: 9761; loss: 0.023624368011951447\n",
            "Iteration: 9762; loss: 0.030071666464209557\n",
            "Iteration: 9763; loss: 0.03719790652394295\n",
            "Iteration: 9764; loss: 0.07684561610221863\n",
            "Iteration: 9765; loss: 0.01682792790234089\n",
            "Iteration: 9766; loss: 0.023560870438814163\n",
            "Iteration: 9767; loss: 0.06568468362092972\n",
            "Iteration: 9768; loss: 0.05856902897357941\n",
            "Iteration: 9769; loss: 0.03273651748895645\n",
            "Iteration: 9770; loss: 0.03227060288190842\n",
            "Iteration: 9771; loss: 0.029131511226296425\n",
            "Iteration: 9772; loss: 0.0257747620344162\n",
            "Iteration: 9773; loss: 0.01917140744626522\n",
            "Iteration: 9774; loss: 0.02200424112379551\n",
            "Iteration: 9775; loss: 0.03236265480518341\n",
            "Iteration: 9776; loss: 0.029268058016896248\n",
            "Iteration: 9777; loss: 0.01631658710539341\n",
            "Iteration: 9778; loss: 0.0322205051779747\n",
            "Iteration: 9779; loss: 0.03665652126073837\n",
            "Iteration: 9780; loss: 0.02634335123002529\n",
            "Iteration: 9781; loss: 0.057532038539648056\n",
            "Iteration: 9782; loss: 0.07677239924669266\n",
            "Iteration: 9783; loss: 0.021640116348862648\n",
            "Iteration: 9784; loss: 0.04101532697677612\n",
            "Iteration: 9785; loss: 0.029230697080492973\n",
            "Iteration: 9786; loss: 0.06638822704553604\n",
            "Iteration: 9787; loss: 0.07258129119873047\n",
            "Iteration: 9788; loss: 0.024899829179048538\n",
            "Iteration: 9789; loss: 0.02514544501900673\n",
            "Iteration: 9790; loss: 0.12969069182872772\n",
            "Iteration: 9791; loss: 0.01930026151239872\n",
            "Iteration: 9792; loss: 0.02617487870156765\n",
            "Iteration: 9793; loss: 0.0353146456182003\n",
            "Iteration: 9794; loss: 0.02210269495844841\n",
            "Iteration: 9795; loss: 0.07406938076019287\n",
            "Iteration: 9796; loss: 0.06793195754289627\n",
            "Iteration: 9797; loss: 0.009918689727783203\n",
            "Iteration: 9798; loss: 0.015393612906336784\n",
            "Iteration: 9799; loss: 0.05141737312078476\n",
            "Iteration: 9800; loss: 0.07372186332941055\n",
            "Iteration: 9801; loss: 0.029955219477415085\n",
            "Iteration: 9802; loss: 0.07508067786693573\n",
            "Iteration: 9803; loss: 0.058535777032375336\n",
            "Iteration: 9804; loss: 0.040694236755371094\n",
            "Iteration: 9805; loss: 0.0802278220653534\n",
            "Iteration: 9806; loss: 0.027710964903235435\n",
            "Iteration: 9807; loss: 0.03625844046473503\n",
            "Iteration: 9808; loss: 0.05002780631184578\n",
            "Iteration: 9809; loss: 0.012698342092335224\n",
            "Iteration: 9810; loss: 0.08590378612279892\n",
            "Iteration: 9811; loss: 0.0396665558218956\n",
            "Iteration: 9812; loss: 0.017465418204665184\n",
            "Iteration: 9813; loss: 0.041654884815216064\n",
            "Iteration: 9814; loss: 0.026237737387418747\n",
            "Iteration: 9815; loss: 0.01402963511645794\n",
            "Iteration: 9816; loss: 0.014808579348027706\n",
            "Iteration: 9817; loss: 0.021944822743535042\n",
            "Iteration: 9818; loss: 0.07609564065933228\n",
            "Iteration: 9819; loss: 0.03370850533246994\n",
            "Iteration: 9820; loss: 0.021424125880002975\n",
            "Iteration: 9821; loss: 0.03141072019934654\n",
            "Iteration: 9822; loss: 0.018546342849731445\n",
            "Iteration: 9823; loss: 0.024052372202277184\n",
            "Iteration: 9824; loss: 0.018404515460133553\n",
            "Iteration: 9825; loss: 0.018428796902298927\n",
            "Iteration: 9826; loss: 0.04161819815635681\n",
            "Iteration: 9827; loss: 0.013678493909537792\n",
            "Iteration: 9828; loss: 0.07593576610088348\n",
            "Iteration: 9829; loss: 0.03940263018012047\n",
            "Iteration: 9830; loss: 0.03665808588266373\n",
            "Iteration: 9831; loss: 0.023914378136396408\n",
            "Iteration: 9832; loss: 0.019340766593813896\n",
            "Iteration: 9833; loss: 0.03835050016641617\n",
            "Iteration: 9834; loss: 0.049362894147634506\n",
            "Iteration: 9835; loss: 0.07657285034656525\n",
            "Iteration: 9836; loss: 0.05538473278284073\n",
            "Iteration: 9837; loss: 0.022070810198783875\n",
            "Iteration: 9838; loss: 0.07543616741895676\n",
            "Iteration: 9839; loss: 0.0365937277674675\n",
            "Iteration: 9840; loss: 0.02075127325952053\n",
            "Iteration: 9841; loss: 0.03488701581954956\n",
            "Iteration: 9842; loss: 0.05418998748064041\n",
            "Iteration: 9843; loss: 0.026218632236123085\n",
            "Iteration: 9844; loss: 0.0214970912784338\n",
            "Iteration: 9845; loss: 0.03487458825111389\n",
            "Iteration: 9846; loss: 0.058419663459062576\n",
            "Iteration: 9847; loss: 0.048334751278162\n",
            "Iteration: 9848; loss: 0.08296343684196472\n",
            "Iteration: 9849; loss: 0.015274638310074806\n",
            "Iteration: 9850; loss: 0.051215458661317825\n",
            "Iteration: 9851; loss: 0.02092718705534935\n",
            "Iteration: 9852; loss: 0.08016644418239594\n",
            "Iteration: 9853; loss: 0.06709782034158707\n",
            "Iteration: 9854; loss: 0.01684393547475338\n",
            "Iteration: 9855; loss: 0.02056773006916046\n",
            "Iteration: 9856; loss: 0.046579476445913315\n",
            "Iteration: 9857; loss: 0.08172393590211868\n",
            "Iteration: 9858; loss: 0.016675913706421852\n",
            "Iteration: 9859; loss: 0.07502438873052597\n",
            "Iteration: 9860; loss: 0.022928450256586075\n",
            "Iteration: 9861; loss: 0.05220692977309227\n",
            "Iteration: 9862; loss: 0.02434436045587063\n",
            "Iteration: 9863; loss: 0.0817851722240448\n",
            "Iteration: 9864; loss: 0.038459137082099915\n",
            "Iteration: 9865; loss: 0.03520157188177109\n",
            "Iteration: 9866; loss: 0.01912793330848217\n",
            "Iteration: 9867; loss: 0.040512196719646454\n",
            "Iteration: 9868; loss: 0.05651923641562462\n",
            "Iteration: 9869; loss: 0.03237861767411232\n",
            "Iteration: 9870; loss: 0.0720721110701561\n",
            "Iteration: 9871; loss: 0.0341942273080349\n",
            "Iteration: 9872; loss: 0.0426335334777832\n",
            "Iteration: 9873; loss: 0.06868117302656174\n",
            "Iteration: 9874; loss: 0.04433954507112503\n",
            "Iteration: 9875; loss: 0.0795876681804657\n",
            "Iteration: 9876; loss: 0.03548619896173477\n",
            "Iteration: 9877; loss: 0.02155924402177334\n",
            "Iteration: 9878; loss: 0.04996823891997337\n",
            "Iteration: 9879; loss: 0.05796463042497635\n",
            "Iteration: 9880; loss: 0.028466731309890747\n",
            "Iteration: 9881; loss: 0.039311353117227554\n",
            "Iteration: 9882; loss: 0.044475115835666656\n",
            "Iteration: 9883; loss: 0.01608540117740631\n",
            "Iteration: 9884; loss: 0.08982027322053909\n",
            "Iteration: 9885; loss: 0.07997046411037445\n",
            "Iteration: 9886; loss: 0.13628056645393372\n",
            "Iteration: 9887; loss: 0.02041453868150711\n",
            "Iteration: 9888; loss: 0.018454307690262794\n",
            "Iteration: 9889; loss: 0.07095277309417725\n",
            "Iteration: 9890; loss: 0.0623011514544487\n",
            "Iteration: 9891; loss: 0.02436855249106884\n",
            "Iteration: 9892; loss: 0.024269185960292816\n",
            "Iteration: 9893; loss: 0.06386763602495193\n",
            "Iteration: 9894; loss: 0.045142363756895065\n",
            "Iteration: 9895; loss: 0.027012530714273453\n",
            "Iteration: 9896; loss: 0.03384819254279137\n",
            "Iteration: 9897; loss: 0.06686284393072128\n",
            "Iteration: 9898; loss: 0.022226911038160324\n",
            "Iteration: 9899; loss: 0.11735588312149048\n",
            "Iteration: 9900; loss: 0.03290533646941185\n",
            "Iteration: 9901; loss: 0.04112197086215019\n",
            "Iteration: 9902; loss: 0.029228415340185165\n",
            "Iteration: 9903; loss: 0.07552879303693771\n",
            "Iteration: 9904; loss: 0.0337417908012867\n",
            "Iteration: 9905; loss: 0.030184604227542877\n",
            "Iteration: 9906; loss: 0.026635942980647087\n",
            "Iteration: 9907; loss: 0.01993294060230255\n",
            "Iteration: 9908; loss: 0.028900329023599625\n",
            "Iteration: 9909; loss: 0.04352033510804176\n",
            "Iteration: 9910; loss: 0.02075359597802162\n",
            "Iteration: 9911; loss: 0.014796856790781021\n",
            "Iteration: 9912; loss: 0.027046581730246544\n",
            "Iteration: 9913; loss: 0.024289917200803757\n",
            "Iteration: 9914; loss: 0.03844258561730385\n",
            "Iteration: 9915; loss: 0.01824878342449665\n",
            "Iteration: 9916; loss: 0.023681487888097763\n",
            "Iteration: 9917; loss: 0.014963934198021889\n",
            "Iteration: 9918; loss: 0.04708053544163704\n",
            "Iteration: 9919; loss: 0.04785909131169319\n",
            "Iteration: 9920; loss: 0.028651507571339607\n",
            "Iteration: 9921; loss: 0.028189288452267647\n",
            "Iteration: 9922; loss: 0.04588812589645386\n",
            "Iteration: 9923; loss: 0.021297017112374306\n",
            "Iteration: 9924; loss: 0.09088116139173508\n",
            "Iteration: 9925; loss: 0.031025391072034836\n",
            "Iteration: 9926; loss: 0.06179343909025192\n",
            "Iteration: 9927; loss: 0.033606115728616714\n",
            "Iteration: 9928; loss: 0.062301624566316605\n",
            "Iteration: 9929; loss: 0.05944468080997467\n",
            "Iteration: 9930; loss: 0.06530061364173889\n",
            "Iteration: 9931; loss: 0.046319350600242615\n",
            "Iteration: 9932; loss: 0.0277956984937191\n",
            "Iteration: 9933; loss: 0.1140558198094368\n",
            "Iteration: 9934; loss: 0.03437355160713196\n",
            "Iteration: 9935; loss: 0.02502446621656418\n",
            "Iteration: 9936; loss: 0.02371853031218052\n",
            "Iteration: 9937; loss: 0.02722649648785591\n",
            "Iteration: 9938; loss: 0.028075847774744034\n",
            "Iteration: 9939; loss: 0.01669178158044815\n",
            "Iteration: 9940; loss: 0.028222886845469475\n",
            "Iteration: 9941; loss: 0.03991882875561714\n",
            "Iteration: 9942; loss: 0.07265332341194153\n",
            "Iteration: 9943; loss: 0.030260199680924416\n",
            "Iteration: 9944; loss: 0.0703793540596962\n",
            "Iteration: 9945; loss: 0.039355337619781494\n",
            "Iteration: 9946; loss: 0.041577890515327454\n",
            "Iteration: 9947; loss: 0.05979703366756439\n",
            "Iteration: 9948; loss: 0.07384231686592102\n",
            "Iteration: 9949; loss: 0.022164393216371536\n",
            "Iteration: 9950; loss: 0.03868719935417175\n",
            "Iteration: 9951; loss: 0.06663373112678528\n",
            "Iteration: 9952; loss: 0.02513979747891426\n",
            "Iteration: 9953; loss: 0.02355808950960636\n",
            "Iteration: 9954; loss: 0.02167792059481144\n",
            "Iteration: 9955; loss: 0.09090021252632141\n",
            "Iteration: 9956; loss: 0.06634268164634705\n",
            "Iteration: 9957; loss: 0.0386153906583786\n",
            "Iteration: 9958; loss: 0.03769129514694214\n",
            "Iteration: 9959; loss: 0.04862430319190025\n",
            "Iteration: 9960; loss: 0.06138962507247925\n",
            "Iteration: 9961; loss: 0.02510054036974907\n",
            "Iteration: 9962; loss: 0.018100567162036896\n",
            "Iteration: 9963; loss: 0.07045136392116547\n",
            "Iteration: 9964; loss: 0.019331667572259903\n",
            "Iteration: 9965; loss: 0.023115642368793488\n",
            "Iteration: 9966; loss: 0.01381669845432043\n",
            "Iteration: 9967; loss: 0.05048048123717308\n",
            "Iteration: 9968; loss: 0.031140757724642754\n",
            "Iteration: 9969; loss: 0.020053712651133537\n",
            "Iteration: 9970; loss: 0.014243911020457745\n",
            "Iteration: 9971; loss: 0.018186435103416443\n",
            "Iteration: 9972; loss: 0.03449749946594238\n",
            "Iteration: 9973; loss: 0.07400649785995483\n",
            "Iteration: 9974; loss: 0.012453676201403141\n",
            "Iteration: 9975; loss: 0.032029859721660614\n",
            "Iteration: 9976; loss: 0.04788685217499733\n",
            "Iteration: 9977; loss: 0.02754206210374832\n",
            "Iteration: 9978; loss: 0.01872710883617401\n",
            "Iteration: 9979; loss: 0.019876154139637947\n",
            "Iteration: 9980; loss: 0.03846703842282295\n",
            "Iteration: 9981; loss: 0.015514438971877098\n",
            "Iteration: 9982; loss: 0.045087285339832306\n",
            "Iteration: 9983; loss: 0.03148084878921509\n",
            "Iteration: 9984; loss: 0.027225568890571594\n",
            "Iteration: 9985; loss: 0.015649929642677307\n",
            "Iteration: 9986; loss: 0.020747123286128044\n",
            "Iteration: 9987; loss: 0.030103864148259163\n",
            "Iteration: 9988; loss: 0.0281609445810318\n",
            "Iteration: 9989; loss: 0.030107207596302032\n",
            "Iteration: 9990; loss: 0.022747481241822243\n",
            "Iteration: 9991; loss: 0.018807509914040565\n",
            "Iteration: 9992; loss: 0.01809319108724594\n",
            "Iteration: 9993; loss: 0.02590429224073887\n",
            "Iteration: 9994; loss: 0.013305862434208393\n",
            "Iteration: 9995; loss: 0.0418723002076149\n",
            "Iteration: 9996; loss: 0.013979995623230934\n",
            "Iteration: 9997; loss: 0.015125597827136517\n",
            "Iteration: 9998; loss: 0.017207343131303787\n",
            "Iteration: 9999; loss: 0.008721093647181988\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9c0007ef20>]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title RNN network with the classic delayed XOR problem. Standard directional derivative \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "# import preconditioned_stochastic_gradient_descent as psgd\n",
        "\n",
        "device = torch.device('cpu')\n",
        "batch_size, seq_len = 128, 32          # increasing sequence_length or decreasing dimension_hidden_layer will make learning harder;\n",
        "dim_in, dim_hidden, dim_out = 2, 30, 1  # current setting can solve seq len 80 ~ 90 reliably without the help of momentum \n",
        "\n",
        "def generate_train_data():\n",
        "    x = np.zeros([batch_size, seq_len, dim_in], dtype=np.float32)\n",
        "    y = np.zeros([batch_size, dim_out], dtype=np.float32)\n",
        "    for i in range(batch_size):\n",
        "        x[i, :, 0] = np.random.choice([-1.0, 1.0], seq_len)\n",
        "\n",
        "        i1 = int(np.floor(np.random.rand() * 0.1 * seq_len))\n",
        "        i2 = int(np.floor(np.random.rand() * 0.4 * seq_len + 0.1 * seq_len))\n",
        "        x[i, i1, 1] = 1.0\n",
        "        x[i, i2, 1] = 1.0\n",
        "        if x[i, i1, 0] == x[i, i2, 0]:  # XOR\n",
        "            y[i] = -1.0  # lable 0\n",
        "        else:\n",
        "            y[i] = 1.0  # lable 1\n",
        "\n",
        "    # tranpose x to format (sequence_length, batch_size, dimension_of_input)\n",
        "    return [torch.tensor(np.transpose(x, [1, 0, 2])).to(device),\n",
        "            torch.tensor(y).to(device)]\n",
        "\n",
        "# generate a random orthogonal matrix for recurrent matrix initialization\n",
        "def get_rand_orth(dim):\n",
        "    temp = np.random.normal(size=[dim, dim])\n",
        "    q, _ = np.linalg.qr(temp)\n",
        "    return torch.tensor(q, dtype=torch.float32).to(device)\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.W1x = torch.nn.Parameter(0.1 * torch.randn(dim_in, dim_hidden))\n",
        "        self.W1h = torch.nn.Parameter(get_rand_orth(dim_hidden))\n",
        "        self.b1 = torch.nn.Parameter(torch.zeros(dim_hidden))\n",
        "        self.W2 = torch.nn.Parameter(0.1 * torch.randn(dim_hidden, dim_out))\n",
        "        self.b2 = torch.nn.Parameter(torch.zeros([]))\n",
        "\n",
        "    def forward(self, xs):\n",
        "        h = torch.zeros(batch_size, dim_hidden, device=device)\n",
        "        for x in torch.unbind(xs):\n",
        "            h = torch.tanh(x @ self.W1x + h @ self.W1h + self.b1)\n",
        "        return h @ self.W2 + self.b2\n",
        "\n",
        "model = Model().to(device)\n",
        "# initialize the PSGD optimizer with the low-rank approximation preconditioner \n",
        "opt = UVd(model.parameters(),\n",
        "               # rank_of_approximation=10, preconditioner_init_scale=1.0,\n",
        "               # lr_params=0.01, lr_preconditioner=0.01, momentum=0.9,\n",
        "               grad_clip_max_norm=1.0, # preconditioner_update_probability=1.0,\n",
        "               exact_hessian_vector_product=False,\n",
        "               momentum=0.9,\n",
        "               directional = 1\n",
        "               )\n",
        "\n",
        "\n",
        "def train_loss(xy_pair):  # logistic loss\n",
        "    return -torch.mean(torch.log(torch.sigmoid(xy_pair[1] * model(xy_pair[0]))))\n",
        "\n",
        "Losses = []\n",
        "for num_iter in range(10000):\n",
        "    train_data = generate_train_data()\n",
        "\n",
        "    rng_state = torch.get_rng_state()\n",
        "    # rng_cuda_state = torch.cuda.get_rng_state()\n",
        "    def closure(): \n",
        "        # If exact_hessian_vector_product=False and rng is used inside closure, \n",
        "        # make sure rng starts from the same state; otherwise, doesn't matter. \n",
        "        torch.set_rng_state(rng_state)\n",
        "        # torch.cuda.set_rng_state(rng_cuda_state)\n",
        "        return train_loss(train_data) # return a loss\n",
        "\n",
        "    loss = opt.step(closure)\n",
        "    Losses.append(loss.item())\n",
        "    print('Iteration: {}; loss: {}'.format(num_iter, Losses[-1]))\n",
        "\n",
        "plt.plot(Losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_kyniHT-mAVt",
        "outputId": "257c0b50-d237-41fc-9823-76601d75a0fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration: 5000; loss: 0.6902387738227844\n",
            "Iteration: 5001; loss: 0.6918805837631226\n",
            "Iteration: 5002; loss: 0.6921588182449341\n",
            "Iteration: 5003; loss: 0.6937887072563171\n",
            "Iteration: 5004; loss: 0.6920034289360046\n",
            "Iteration: 5005; loss: 0.6938259601593018\n",
            "Iteration: 5006; loss: 0.6929646730422974\n",
            "Iteration: 5007; loss: 0.6949727535247803\n",
            "Iteration: 5008; loss: 0.6930686235427856\n",
            "Iteration: 5009; loss: 0.6940971612930298\n",
            "Iteration: 5010; loss: 0.6929494142532349\n",
            "Iteration: 5011; loss: 0.6967167854309082\n",
            "Iteration: 5012; loss: 0.6926057934761047\n",
            "Iteration: 5013; loss: 0.6938834190368652\n",
            "Iteration: 5014; loss: 0.6947521567344666\n",
            "Iteration: 5015; loss: 0.6932332515716553\n",
            "Iteration: 5016; loss: 0.6920724511146545\n",
            "Iteration: 5017; loss: 0.6916383504867554\n",
            "Iteration: 5018; loss: 0.6922935843467712\n",
            "Iteration: 5019; loss: 0.6946487426757812\n",
            "Iteration: 5020; loss: 0.6924275755882263\n",
            "Iteration: 5021; loss: 0.6899878978729248\n",
            "Iteration: 5022; loss: 0.695713996887207\n",
            "Iteration: 5023; loss: 0.6928800940513611\n",
            "Iteration: 5024; loss: 0.6913423538208008\n",
            "Iteration: 5025; loss: 0.6936554908752441\n",
            "Iteration: 5026; loss: 0.6906473636627197\n",
            "Iteration: 5027; loss: 0.6908701062202454\n",
            "Iteration: 5028; loss: 0.6922637820243835\n",
            "Iteration: 5029; loss: 0.6943365931510925\n",
            "Iteration: 5030; loss: 0.692548394203186\n",
            "Iteration: 5031; loss: 0.6931653022766113\n",
            "Iteration: 5032; loss: 0.6921069025993347\n",
            "Iteration: 5033; loss: 0.691778302192688\n",
            "Iteration: 5034; loss: 0.6903672218322754\n",
            "Iteration: 5035; loss: 0.6951153874397278\n",
            "Iteration: 5036; loss: 0.6912813186645508\n",
            "Iteration: 5037; loss: 0.6937152147293091\n",
            "Iteration: 5038; loss: 0.695313036441803\n",
            "Iteration: 5039; loss: 0.6914244294166565\n",
            "Iteration: 5040; loss: 0.6952090859413147\n",
            "Iteration: 5041; loss: 0.6951967477798462\n",
            "Iteration: 5042; loss: 0.6916675567626953\n",
            "Iteration: 5043; loss: 0.6948449611663818\n",
            "Iteration: 5044; loss: 0.6899482607841492\n",
            "Iteration: 5045; loss: 0.6956689953804016\n",
            "Iteration: 5046; loss: 0.6911846995353699\n",
            "Iteration: 5047; loss: 0.6954415440559387\n",
            "Iteration: 5048; loss: 0.6921917796134949\n",
            "Iteration: 5049; loss: 0.6914990544319153\n",
            "Iteration: 5050; loss: 0.6937078237533569\n",
            "Iteration: 5051; loss: 0.6941996812820435\n",
            "Iteration: 5052; loss: 0.6933162212371826\n",
            "Iteration: 5053; loss: 0.6938217878341675\n",
            "Iteration: 5054; loss: 0.6934221982955933\n",
            "Iteration: 5055; loss: 0.6947835087776184\n",
            "Iteration: 5056; loss: 0.69107985496521\n",
            "Iteration: 5057; loss: 0.692050039768219\n",
            "Iteration: 5058; loss: 0.6939049959182739\n",
            "Iteration: 5059; loss: 0.6952390670776367\n",
            "Iteration: 5060; loss: 0.69347083568573\n",
            "Iteration: 5061; loss: 0.6954789161682129\n",
            "Iteration: 5062; loss: 0.6940256953239441\n",
            "Iteration: 5063; loss: 0.6936798691749573\n",
            "Iteration: 5064; loss: 0.6929506659507751\n",
            "Iteration: 5065; loss: 0.693661093711853\n",
            "Iteration: 5066; loss: 0.6944863200187683\n",
            "Iteration: 5067; loss: 0.6931241750717163\n",
            "Iteration: 5068; loss: 0.6926769018173218\n",
            "Iteration: 5069; loss: 0.6932100057601929\n",
            "Iteration: 5070; loss: 0.6928882002830505\n",
            "Iteration: 5071; loss: 0.6908856630325317\n",
            "Iteration: 5072; loss: 0.69122314453125\n",
            "Iteration: 5073; loss: 0.6915274858474731\n",
            "Iteration: 5074; loss: 0.6922813057899475\n",
            "Iteration: 5075; loss: 0.6920479536056519\n",
            "Iteration: 5076; loss: 0.6919349431991577\n",
            "Iteration: 5077; loss: 0.6928139925003052\n",
            "Iteration: 5078; loss: 0.6938295960426331\n",
            "Iteration: 5079; loss: 0.6904011368751526\n",
            "Iteration: 5080; loss: 0.6933073997497559\n",
            "Iteration: 5081; loss: 0.6928392648696899\n",
            "Iteration: 5082; loss: 0.6940262317657471\n",
            "Iteration: 5083; loss: 0.693263590335846\n",
            "Iteration: 5084; loss: 0.6918267607688904\n",
            "Iteration: 5085; loss: 0.6932640671730042\n",
            "Iteration: 5086; loss: 0.6921922564506531\n",
            "Iteration: 5087; loss: 0.6930180788040161\n",
            "Iteration: 5088; loss: 0.6910061836242676\n",
            "Iteration: 5089; loss: 0.6964638829231262\n",
            "Iteration: 5090; loss: 0.6922896504402161\n",
            "Iteration: 5091; loss: 0.6923967599868774\n",
            "Iteration: 5092; loss: 0.6910388469696045\n",
            "Iteration: 5093; loss: 0.6944993734359741\n",
            "Iteration: 5094; loss: 0.694632887840271\n",
            "Iteration: 5095; loss: 0.6934928894042969\n",
            "Iteration: 5096; loss: 0.6929976940155029\n",
            "Iteration: 5097; loss: 0.6955273747444153\n",
            "Iteration: 5098; loss: 0.6904555559158325\n",
            "Iteration: 5099; loss: 0.6949793696403503\n",
            "Iteration: 5100; loss: 0.6951209902763367\n",
            "Iteration: 5101; loss: 0.694209098815918\n",
            "Iteration: 5102; loss: 0.6923620700836182\n",
            "Iteration: 5103; loss: 0.6946740746498108\n",
            "Iteration: 5104; loss: 0.6947487592697144\n",
            "Iteration: 5105; loss: 0.6925386190414429\n",
            "Iteration: 5106; loss: 0.6941033005714417\n",
            "Iteration: 5107; loss: 0.6917548775672913\n",
            "Iteration: 5108; loss: 0.6936364769935608\n",
            "Iteration: 5109; loss: 0.6950831413269043\n",
            "Iteration: 5110; loss: 0.6946947574615479\n",
            "Iteration: 5111; loss: 0.6953591704368591\n",
            "Iteration: 5112; loss: 0.6928994655609131\n",
            "Iteration: 5113; loss: 0.6947240829467773\n",
            "Iteration: 5114; loss: 0.6916674971580505\n",
            "Iteration: 5115; loss: 0.692959725856781\n",
            "Iteration: 5116; loss: 0.6915449500083923\n",
            "Iteration: 5117; loss: 0.6930099129676819\n",
            "Iteration: 5118; loss: 0.6941778063774109\n",
            "Iteration: 5119; loss: 0.6937627792358398\n",
            "Iteration: 5120; loss: 0.692833662033081\n",
            "Iteration: 5121; loss: 0.6892738342285156\n",
            "Iteration: 5122; loss: 0.6905734539031982\n",
            "Iteration: 5123; loss: 0.6929711103439331\n",
            "Iteration: 5124; loss: 0.6941101551055908\n",
            "Iteration: 5125; loss: 0.6917449831962585\n",
            "Iteration: 5126; loss: 0.6944103240966797\n",
            "Iteration: 5127; loss: 0.6954804062843323\n",
            "Iteration: 5128; loss: 0.6943572759628296\n",
            "Iteration: 5129; loss: 0.6921993494033813\n",
            "Iteration: 5130; loss: 0.6907228827476501\n",
            "Iteration: 5131; loss: 0.6955980062484741\n",
            "Iteration: 5132; loss: 0.6934874653816223\n",
            "Iteration: 5133; loss: 0.694432258605957\n",
            "Iteration: 5134; loss: 0.6901057362556458\n",
            "Iteration: 5135; loss: 0.693448543548584\n",
            "Iteration: 5136; loss: 0.6919872760772705\n",
            "Iteration: 5137; loss: 0.6909671425819397\n",
            "Iteration: 5138; loss: 0.6949154138565063\n",
            "Iteration: 5139; loss: 0.6925825476646423\n",
            "Iteration: 5140; loss: 0.692665696144104\n",
            "Iteration: 5141; loss: 0.6916714906692505\n",
            "Iteration: 5142; loss: 0.6933483481407166\n",
            "Iteration: 5143; loss: 0.6974380016326904\n",
            "Iteration: 5144; loss: 0.6926532983779907\n",
            "Iteration: 5145; loss: 0.6928851008415222\n",
            "Iteration: 5146; loss: 0.6961026787757874\n",
            "Iteration: 5147; loss: 0.6932225227355957\n",
            "Iteration: 5148; loss: 0.6921700835227966\n",
            "Iteration: 5149; loss: 0.6980305314064026\n",
            "Iteration: 5150; loss: 0.6941813230514526\n",
            "Iteration: 5151; loss: 0.6963734030723572\n",
            "Iteration: 5152; loss: 0.6913056373596191\n",
            "Iteration: 5153; loss: 0.693946361541748\n",
            "Iteration: 5154; loss: 0.6929113864898682\n",
            "Iteration: 5155; loss: 0.693000078201294\n",
            "Iteration: 5156; loss: 0.6948733329772949\n",
            "Iteration: 5157; loss: 0.6917084455490112\n",
            "Iteration: 5158; loss: 0.6935036778450012\n",
            "Iteration: 5159; loss: 0.6930819749832153\n",
            "Iteration: 5160; loss: 0.6942673921585083\n",
            "Iteration: 5161; loss: 0.6926454901695251\n",
            "Iteration: 5162; loss: 0.6921727061271667\n",
            "Iteration: 5163; loss: 0.6944997906684875\n",
            "Iteration: 5164; loss: 0.6949692964553833\n",
            "Iteration: 5165; loss: 0.6910605430603027\n",
            "Iteration: 5166; loss: 0.6901447772979736\n",
            "Iteration: 5167; loss: 0.6924077868461609\n",
            "Iteration: 5168; loss: 0.6933572292327881\n",
            "Iteration: 5169; loss: 0.6939910650253296\n",
            "Iteration: 5170; loss: 0.6913703680038452\n",
            "Iteration: 5171; loss: 0.6952012181282043\n",
            "Iteration: 5172; loss: 0.6905418634414673\n",
            "Iteration: 5173; loss: 0.6965539455413818\n",
            "Iteration: 5174; loss: 0.6928892731666565\n",
            "Iteration: 5175; loss: 0.6951039433479309\n",
            "Iteration: 5176; loss: 0.6943817138671875\n",
            "Iteration: 5177; loss: 0.6922014355659485\n",
            "Iteration: 5178; loss: 0.6918272972106934\n",
            "Iteration: 5179; loss: 0.6967373490333557\n",
            "Iteration: 5180; loss: 0.6953716278076172\n",
            "Iteration: 5181; loss: 0.6934632658958435\n",
            "Iteration: 5182; loss: 0.6936017274856567\n",
            "Iteration: 5183; loss: 0.6938488483428955\n",
            "Iteration: 5184; loss: 0.6919646263122559\n",
            "Iteration: 5185; loss: 0.6925666332244873\n",
            "Iteration: 5186; loss: 0.692682147026062\n",
            "Iteration: 5187; loss: 0.6934882402420044\n",
            "Iteration: 5188; loss: 0.6915671825408936\n",
            "Iteration: 5189; loss: 0.6941136717796326\n",
            "Iteration: 5190; loss: 0.6914364695549011\n",
            "Iteration: 5191; loss: 0.6912136673927307\n",
            "Iteration: 5192; loss: 0.6927880644798279\n",
            "Iteration: 5193; loss: 0.6941163539886475\n",
            "Iteration: 5194; loss: 0.6931602358818054\n",
            "Iteration: 5195; loss: 0.6919759511947632\n",
            "Iteration: 5196; loss: 0.6927811503410339\n",
            "Iteration: 5197; loss: 0.69332355260849\n",
            "Iteration: 5198; loss: 0.694017231464386\n",
            "Iteration: 5199; loss: 0.6936863660812378\n",
            "Iteration: 5200; loss: 0.694603681564331\n",
            "Iteration: 5201; loss: 0.6933069229125977\n",
            "Iteration: 5202; loss: 0.6956085562705994\n",
            "Iteration: 5203; loss: 0.6911752820014954\n",
            "Iteration: 5204; loss: 0.6924176812171936\n",
            "Iteration: 5205; loss: 0.6939924955368042\n",
            "Iteration: 5206; loss: 0.6922114491462708\n",
            "Iteration: 5207; loss: 0.693717360496521\n",
            "Iteration: 5208; loss: 0.6963231563568115\n",
            "Iteration: 5209; loss: 0.6942588686943054\n",
            "Iteration: 5210; loss: 0.6936615705490112\n",
            "Iteration: 5211; loss: 0.6941593885421753\n",
            "Iteration: 5212; loss: 0.6896455883979797\n",
            "Iteration: 5213; loss: 0.6921361684799194\n",
            "Iteration: 5214; loss: 0.6939802169799805\n",
            "Iteration: 5215; loss: 0.6917366981506348\n",
            "Iteration: 5216; loss: 0.6934805512428284\n",
            "Iteration: 5217; loss: 0.6932945847511292\n",
            "Iteration: 5218; loss: 0.6975647211074829\n",
            "Iteration: 5219; loss: 0.6907459497451782\n",
            "Iteration: 5220; loss: 0.6930781602859497\n",
            "Iteration: 5221; loss: 0.6935234665870667\n",
            "Iteration: 5222; loss: 0.6932217478752136\n",
            "Iteration: 5223; loss: 0.692234456539154\n",
            "Iteration: 5224; loss: 0.6922226548194885\n",
            "Iteration: 5225; loss: 0.6921834349632263\n",
            "Iteration: 5226; loss: 0.6935675740242004\n",
            "Iteration: 5227; loss: 0.692989706993103\n",
            "Iteration: 5228; loss: 0.6916143894195557\n",
            "Iteration: 5229; loss: 0.6941002011299133\n",
            "Iteration: 5230; loss: 0.6924492120742798\n",
            "Iteration: 5231; loss: 0.6943399310112\n",
            "Iteration: 5232; loss: 0.6934077739715576\n",
            "Iteration: 5233; loss: 0.6943315267562866\n",
            "Iteration: 5234; loss: 0.6933871507644653\n",
            "Iteration: 5235; loss: 0.694728672504425\n",
            "Iteration: 5236; loss: 0.6933822631835938\n",
            "Iteration: 5237; loss: 0.6935713887214661\n",
            "Iteration: 5238; loss: 0.6914645433425903\n",
            "Iteration: 5239; loss: 0.6942138671875\n",
            "Iteration: 5240; loss: 0.6924432516098022\n",
            "Iteration: 5241; loss: 0.6964048147201538\n",
            "Iteration: 5242; loss: 0.6927276253700256\n",
            "Iteration: 5243; loss: 0.6929781436920166\n",
            "Iteration: 5244; loss: 0.6959888935089111\n",
            "Iteration: 5245; loss: 0.6923652291297913\n",
            "Iteration: 5246; loss: 0.6915631294250488\n",
            "Iteration: 5247; loss: 0.692627489566803\n",
            "Iteration: 5248; loss: 0.6911879181861877\n",
            "Iteration: 5249; loss: 0.6938179731369019\n",
            "Iteration: 5250; loss: 0.6930692195892334\n",
            "Iteration: 5251; loss: 0.6942743062973022\n",
            "Iteration: 5252; loss: 0.6932845115661621\n",
            "Iteration: 5253; loss: 0.6928390860557556\n",
            "Iteration: 5254; loss: 0.6914345026016235\n",
            "Iteration: 5255; loss: 0.694415271282196\n",
            "Iteration: 5256; loss: 0.6923632025718689\n",
            "Iteration: 5257; loss: 0.6922974586486816\n",
            "Iteration: 5258; loss: 0.692213237285614\n",
            "Iteration: 5259; loss: 0.6945606470108032\n",
            "Iteration: 5260; loss: 0.6917608976364136\n",
            "Iteration: 5261; loss: 0.6933623552322388\n",
            "Iteration: 5262; loss: 0.6946461200714111\n",
            "Iteration: 5263; loss: 0.6928081512451172\n",
            "Iteration: 5264; loss: 0.6943172812461853\n",
            "Iteration: 5265; loss: 0.6925650238990784\n",
            "Iteration: 5266; loss: 0.6968812942504883\n",
            "Iteration: 5267; loss: 0.6941011548042297\n",
            "Iteration: 5268; loss: 0.6910557746887207\n",
            "Iteration: 5269; loss: 0.6915985345840454\n",
            "Iteration: 5270; loss: 0.6942106485366821\n",
            "Iteration: 5271; loss: 0.6949468851089478\n",
            "Iteration: 5272; loss: 0.6919317841529846\n",
            "Iteration: 5273; loss: 0.6929057836532593\n",
            "Iteration: 5274; loss: 0.6926093697547913\n",
            "Iteration: 5275; loss: 0.6938909888267517\n",
            "Iteration: 5276; loss: 0.6947529315948486\n",
            "Iteration: 5277; loss: 0.6910104751586914\n",
            "Iteration: 5278; loss: 0.6949834823608398\n",
            "Iteration: 5279; loss: 0.6921675801277161\n",
            "Iteration: 5280; loss: 0.6931613683700562\n",
            "Iteration: 5281; loss: 0.6927850842475891\n",
            "Iteration: 5282; loss: 0.6918290853500366\n",
            "Iteration: 5283; loss: 0.6947034001350403\n",
            "Iteration: 5284; loss: 0.691653311252594\n",
            "Iteration: 5285; loss: 0.692890465259552\n",
            "Iteration: 5286; loss: 0.6954603791236877\n",
            "Iteration: 5287; loss: 0.6942830085754395\n",
            "Iteration: 5288; loss: 0.6910452246665955\n",
            "Iteration: 5289; loss: 0.6954885125160217\n",
            "Iteration: 5290; loss: 0.6939685344696045\n",
            "Iteration: 5291; loss: 0.6910427212715149\n",
            "Iteration: 5292; loss: 0.6929008960723877\n",
            "Iteration: 5293; loss: 0.6933544874191284\n",
            "Iteration: 5294; loss: 0.6934058666229248\n",
            "Iteration: 5295; loss: 0.6901832222938538\n",
            "Iteration: 5296; loss: 0.6896514296531677\n",
            "Iteration: 5297; loss: 0.6943179368972778\n",
            "Iteration: 5298; loss: 0.6930618286132812\n",
            "Iteration: 5299; loss: 0.6913585662841797\n",
            "Iteration: 5300; loss: 0.6936073899269104\n",
            "Iteration: 5301; loss: 0.6948423385620117\n",
            "Iteration: 5302; loss: 0.6923776268959045\n",
            "Iteration: 5303; loss: 0.6937081813812256\n",
            "Iteration: 5304; loss: 0.69081050157547\n",
            "Iteration: 5305; loss: 0.6930184364318848\n",
            "Iteration: 5306; loss: 0.6945884227752686\n",
            "Iteration: 5307; loss: 0.6929585337638855\n",
            "Iteration: 5308; loss: 0.6952542662620544\n",
            "Iteration: 5309; loss: 0.6936197876930237\n",
            "Iteration: 5310; loss: 0.6925339698791504\n",
            "Iteration: 5311; loss: 0.6932591199874878\n",
            "Iteration: 5312; loss: 0.6952630281448364\n",
            "Iteration: 5313; loss: 0.6916928887367249\n",
            "Iteration: 5314; loss: 0.6927963495254517\n",
            "Iteration: 5315; loss: 0.6918174028396606\n",
            "Iteration: 5316; loss: 0.6930494904518127\n",
            "Iteration: 5317; loss: 0.6942294836044312\n",
            "Iteration: 5318; loss: 0.694085955619812\n",
            "Iteration: 5319; loss: 0.6916277408599854\n",
            "Iteration: 5320; loss: 0.6937738656997681\n",
            "Iteration: 5321; loss: 0.6926487684249878\n",
            "Iteration: 5322; loss: 0.6924052834510803\n",
            "Iteration: 5323; loss: 0.6939883232116699\n",
            "Iteration: 5324; loss: 0.6929758191108704\n",
            "Iteration: 5325; loss: 0.6924838423728943\n",
            "Iteration: 5326; loss: 0.6927779316902161\n",
            "Iteration: 5327; loss: 0.6898541450500488\n",
            "Iteration: 5328; loss: 0.6948203444480896\n",
            "Iteration: 5329; loss: 0.6919112205505371\n",
            "Iteration: 5330; loss: 0.6940553188323975\n",
            "Iteration: 5331; loss: 0.6935097575187683\n",
            "Iteration: 5332; loss: 0.6921124458312988\n",
            "Iteration: 5333; loss: 0.6929083466529846\n",
            "Iteration: 5334; loss: 0.693474292755127\n",
            "Iteration: 5335; loss: 0.6933885216712952\n",
            "Iteration: 5336; loss: 0.692314624786377\n",
            "Iteration: 5337; loss: 0.6987897753715515\n",
            "Iteration: 5338; loss: 0.691472053527832\n",
            "Iteration: 5339; loss: 0.6943435668945312\n",
            "Iteration: 5340; loss: 0.6928689479827881\n",
            "Iteration: 5341; loss: 0.6936982274055481\n",
            "Iteration: 5342; loss: 0.691638171672821\n",
            "Iteration: 5343; loss: 0.6960317492485046\n",
            "Iteration: 5344; loss: 0.6931459307670593\n",
            "Iteration: 5345; loss: 0.6927027702331543\n",
            "Iteration: 5346; loss: 0.6929479241371155\n",
            "Iteration: 5347; loss: 0.6928943395614624\n",
            "Iteration: 5348; loss: 0.6926695704460144\n",
            "Iteration: 5349; loss: 0.6950263381004333\n",
            "Iteration: 5350; loss: 0.6931433081626892\n",
            "Iteration: 5351; loss: 0.6980268359184265\n",
            "Iteration: 5352; loss: 0.6945679783821106\n",
            "Iteration: 5353; loss: 0.6929656267166138\n",
            "Iteration: 5354; loss: 0.6915003061294556\n",
            "Iteration: 5355; loss: 0.6949095129966736\n",
            "Iteration: 5356; loss: 0.69366455078125\n",
            "Iteration: 5357; loss: 0.6927564144134521\n",
            "Iteration: 5358; loss: 0.6919652223587036\n",
            "Iteration: 5359; loss: 0.6929686665534973\n",
            "Iteration: 5360; loss: 0.6914405226707458\n",
            "Iteration: 5361; loss: 0.695561408996582\n",
            "Iteration: 5362; loss: 0.6926632523536682\n",
            "Iteration: 5363; loss: 0.6903873085975647\n",
            "Iteration: 5364; loss: 0.6930142641067505\n",
            "Iteration: 5365; loss: 0.6933978796005249\n",
            "Iteration: 5366; loss: 0.6937421560287476\n",
            "Iteration: 5367; loss: 0.6933960914611816\n",
            "Iteration: 5368; loss: 0.6933431625366211\n",
            "Iteration: 5369; loss: 0.693260669708252\n",
            "Iteration: 5370; loss: 0.6902871131896973\n",
            "Iteration: 5371; loss: 0.693419337272644\n",
            "Iteration: 5372; loss: 0.6922493577003479\n",
            "Iteration: 5373; loss: 0.692565381526947\n",
            "Iteration: 5374; loss: 0.69399493932724\n",
            "Iteration: 5375; loss: 0.6962928175926208\n",
            "Iteration: 5376; loss: 0.6910790205001831\n",
            "Iteration: 5377; loss: 0.692699670791626\n",
            "Iteration: 5378; loss: 0.6920273303985596\n",
            "Iteration: 5379; loss: 0.6933514475822449\n",
            "Iteration: 5380; loss: 0.6928558349609375\n",
            "Iteration: 5381; loss: 0.6940644979476929\n",
            "Iteration: 5382; loss: 0.6923249959945679\n",
            "Iteration: 5383; loss: 0.6965951919555664\n",
            "Iteration: 5384; loss: 0.6916488409042358\n",
            "Iteration: 5385; loss: 0.6917061805725098\n",
            "Iteration: 5386; loss: 0.6908183693885803\n",
            "Iteration: 5387; loss: 0.6932664513587952\n",
            "Iteration: 5388; loss: 0.6929678320884705\n",
            "Iteration: 5389; loss: 0.6957650780677795\n",
            "Iteration: 5390; loss: 0.6919949650764465\n",
            "Iteration: 5391; loss: 0.6913250088691711\n",
            "Iteration: 5392; loss: 0.6923317909240723\n",
            "Iteration: 5393; loss: 0.693764328956604\n",
            "Iteration: 5394; loss: 0.6923995018005371\n",
            "Iteration: 5395; loss: 0.6919686794281006\n",
            "Iteration: 5396; loss: 0.695817232131958\n",
            "Iteration: 5397; loss: 0.6918265223503113\n",
            "Iteration: 5398; loss: 0.694377064704895\n",
            "Iteration: 5399; loss: 0.6927365064620972\n",
            "Iteration: 5400; loss: 0.6908969283103943\n",
            "Iteration: 5401; loss: 0.6927570104598999\n",
            "Iteration: 5402; loss: 0.6962151527404785\n",
            "Iteration: 5403; loss: 0.6929140686988831\n",
            "Iteration: 5404; loss: 0.6933488845825195\n",
            "Iteration: 5405; loss: 0.6939138770103455\n",
            "Iteration: 5406; loss: 0.6919296979904175\n",
            "Iteration: 5407; loss: 0.6927034258842468\n",
            "Iteration: 5408; loss: 0.6926966905593872\n",
            "Iteration: 5409; loss: 0.691415011882782\n",
            "Iteration: 5410; loss: 0.6934332847595215\n",
            "Iteration: 5411; loss: 0.6909795999526978\n",
            "Iteration: 5412; loss: 0.6929446458816528\n",
            "Iteration: 5413; loss: 0.6950001120567322\n",
            "Iteration: 5414; loss: 0.6904430389404297\n",
            "Iteration: 5415; loss: 0.6938570737838745\n",
            "Iteration: 5416; loss: 0.6907082200050354\n",
            "Iteration: 5417; loss: 0.6953974366188049\n",
            "Iteration: 5418; loss: 0.6916812658309937\n",
            "Iteration: 5419; loss: 0.6927585005760193\n",
            "Iteration: 5420; loss: 0.6936715245246887\n",
            "Iteration: 5421; loss: 0.6946498155593872\n",
            "Iteration: 5422; loss: 0.6935208439826965\n",
            "Iteration: 5423; loss: 0.6962333917617798\n",
            "Iteration: 5424; loss: 0.6951108574867249\n",
            "Iteration: 5425; loss: 0.693688154220581\n",
            "Iteration: 5426; loss: 0.6920145750045776\n",
            "Iteration: 5427; loss: 0.6904576420783997\n",
            "Iteration: 5428; loss: 0.694542646408081\n",
            "Iteration: 5429; loss: 0.6943829655647278\n",
            "Iteration: 5430; loss: 0.6934051513671875\n",
            "Iteration: 5431; loss: 0.6914366483688354\n",
            "Iteration: 5432; loss: 0.6939550042152405\n",
            "Iteration: 5433; loss: 0.6921246647834778\n",
            "Iteration: 5434; loss: 0.691274106502533\n",
            "Iteration: 5435; loss: 0.6965600848197937\n",
            "Iteration: 5436; loss: 0.6938714385032654\n",
            "Iteration: 5437; loss: 0.6916297078132629\n",
            "Iteration: 5438; loss: 0.6946105360984802\n",
            "Iteration: 5439; loss: 0.6929985880851746\n",
            "Iteration: 5440; loss: 0.6910295486450195\n",
            "Iteration: 5441; loss: 0.6956868171691895\n",
            "Iteration: 5442; loss: 0.6899039149284363\n",
            "Iteration: 5443; loss: 0.692671000957489\n",
            "Iteration: 5444; loss: 0.6951878070831299\n",
            "Iteration: 5445; loss: 0.6914287209510803\n",
            "Iteration: 5446; loss: 0.6936028599739075\n",
            "Iteration: 5447; loss: 0.6911576390266418\n",
            "Iteration: 5448; loss: 0.6922534704208374\n",
            "Iteration: 5449; loss: 0.6958675384521484\n",
            "Iteration: 5450; loss: 0.6956092715263367\n",
            "Iteration: 5451; loss: 0.6905016899108887\n",
            "Iteration: 5452; loss: 0.6928385496139526\n",
            "Iteration: 5453; loss: 0.6922248601913452\n",
            "Iteration: 5454; loss: 0.6899336576461792\n",
            "Iteration: 5455; loss: 0.6919004321098328\n",
            "Iteration: 5456; loss: 0.6921984553337097\n",
            "Iteration: 5457; loss: 0.6942154169082642\n",
            "Iteration: 5458; loss: 0.695871114730835\n",
            "Iteration: 5459; loss: 0.6925554871559143\n",
            "Iteration: 5460; loss: 0.6937022805213928\n",
            "Iteration: 5461; loss: 0.6915255188941956\n",
            "Iteration: 5462; loss: 0.6933634877204895\n",
            "Iteration: 5463; loss: 0.6950815320014954\n",
            "Iteration: 5464; loss: 0.6947179436683655\n",
            "Iteration: 5465; loss: 0.6926759481430054\n",
            "Iteration: 5466; loss: 0.691125214099884\n",
            "Iteration: 5467; loss: 0.6930482983589172\n",
            "Iteration: 5468; loss: 0.6956671476364136\n",
            "Iteration: 5469; loss: 0.691616415977478\n",
            "Iteration: 5470; loss: 0.6950904130935669\n",
            "Iteration: 5471; loss: 0.6959166526794434\n",
            "Iteration: 5472; loss: 0.6934230923652649\n",
            "Iteration: 5473; loss: 0.6949435472488403\n",
            "Iteration: 5474; loss: 0.6921675801277161\n",
            "Iteration: 5475; loss: 0.6905440092086792\n",
            "Iteration: 5476; loss: 0.6947619915008545\n",
            "Iteration: 5477; loss: 0.69526207447052\n",
            "Iteration: 5478; loss: 0.6894434094429016\n",
            "Iteration: 5479; loss: 0.6933645606040955\n",
            "Iteration: 5480; loss: 0.6916236877441406\n",
            "Iteration: 5481; loss: 0.6937925219535828\n",
            "Iteration: 5482; loss: 0.6894235014915466\n",
            "Iteration: 5483; loss: 0.6913967132568359\n",
            "Iteration: 5484; loss: 0.6909817457199097\n",
            "Iteration: 5485; loss: 0.6919224858283997\n",
            "Iteration: 5486; loss: 0.6921426057815552\n",
            "Iteration: 5487; loss: 0.6906524300575256\n",
            "Iteration: 5488; loss: 0.6925959587097168\n",
            "Iteration: 5489; loss: 0.6946032643318176\n",
            "Iteration: 5490; loss: 0.6915303468704224\n",
            "Iteration: 5491; loss: 0.6965987086296082\n",
            "Iteration: 5492; loss: 0.691421627998352\n",
            "Iteration: 5493; loss: 0.6932829022407532\n",
            "Iteration: 5494; loss: 0.6948690414428711\n",
            "Iteration: 5495; loss: 0.695627748966217\n",
            "Iteration: 5496; loss: 0.6897928714752197\n",
            "Iteration: 5497; loss: 0.694650411605835\n",
            "Iteration: 5498; loss: 0.6921393871307373\n",
            "Iteration: 5499; loss: 0.6936317682266235\n",
            "Iteration: 5500; loss: 0.6954030990600586\n",
            "Iteration: 5501; loss: 0.6931703686714172\n",
            "Iteration: 5502; loss: 0.6929810047149658\n",
            "Iteration: 5503; loss: 0.6921820044517517\n",
            "Iteration: 5504; loss: 0.6951367855072021\n",
            "Iteration: 5505; loss: 0.6916813850402832\n",
            "Iteration: 5506; loss: 0.6932249069213867\n",
            "Iteration: 5507; loss: 0.6905021071434021\n",
            "Iteration: 5508; loss: 0.6906781792640686\n",
            "Iteration: 5509; loss: 0.6929446458816528\n",
            "Iteration: 5510; loss: 0.6944592595100403\n",
            "Iteration: 5511; loss: 0.692256510257721\n",
            "Iteration: 5512; loss: 0.6937465667724609\n",
            "Iteration: 5513; loss: 0.6901326179504395\n",
            "Iteration: 5514; loss: 0.6965380907058716\n",
            "Iteration: 5515; loss: 0.6952305436134338\n",
            "Iteration: 5516; loss: 0.6941941976547241\n",
            "Iteration: 5517; loss: 0.6939418315887451\n",
            "Iteration: 5518; loss: 0.6960574388504028\n",
            "Iteration: 5519; loss: 0.6951196193695068\n",
            "Iteration: 5520; loss: 0.6930456161499023\n",
            "Iteration: 5521; loss: 0.6904854774475098\n",
            "Iteration: 5522; loss: 0.6919195652008057\n",
            "Iteration: 5523; loss: 0.6932149529457092\n",
            "Iteration: 5524; loss: 0.6919540166854858\n",
            "Iteration: 5525; loss: 0.6947500705718994\n",
            "Iteration: 5526; loss: 0.6901296377182007\n",
            "Iteration: 5527; loss: 0.6913231611251831\n",
            "Iteration: 5528; loss: 0.6935741901397705\n",
            "Iteration: 5529; loss: 0.6941310167312622\n",
            "Iteration: 5530; loss: 0.6922450661659241\n",
            "Iteration: 5531; loss: 0.6930689811706543\n",
            "Iteration: 5532; loss: 0.6944262981414795\n",
            "Iteration: 5533; loss: 0.6962037682533264\n",
            "Iteration: 5534; loss: 0.6934248208999634\n",
            "Iteration: 5535; loss: 0.6942646503448486\n",
            "Iteration: 5536; loss: 0.6933709979057312\n",
            "Iteration: 5537; loss: 0.6896644830703735\n",
            "Iteration: 5538; loss: 0.6926610469818115\n",
            "Iteration: 5539; loss: 0.6922298669815063\n",
            "Iteration: 5540; loss: 0.6946099400520325\n",
            "Iteration: 5541; loss: 0.6948598623275757\n",
            "Iteration: 5542; loss: 0.6924461126327515\n",
            "Iteration: 5543; loss: 0.6931674480438232\n",
            "Iteration: 5544; loss: 0.6959602236747742\n",
            "Iteration: 5545; loss: 0.6912758350372314\n",
            "Iteration: 5546; loss: 0.6934698224067688\n",
            "Iteration: 5547; loss: 0.6920815706253052\n",
            "Iteration: 5548; loss: 0.6946898698806763\n",
            "Iteration: 5549; loss: 0.695968508720398\n",
            "Iteration: 5550; loss: 0.6931964159011841\n",
            "Iteration: 5551; loss: 0.690494179725647\n",
            "Iteration: 5552; loss: 0.696094810962677\n",
            "Iteration: 5553; loss: 0.692304790019989\n",
            "Iteration: 5554; loss: 0.6912967562675476\n",
            "Iteration: 5555; loss: 0.6932511329650879\n",
            "Iteration: 5556; loss: 0.6925376653671265\n",
            "Iteration: 5557; loss: 0.69597327709198\n",
            "Iteration: 5558; loss: 0.6907291412353516\n",
            "Iteration: 5559; loss: 0.6915138959884644\n",
            "Iteration: 5560; loss: 0.693812370300293\n",
            "Iteration: 5561; loss: 0.6908495426177979\n",
            "Iteration: 5562; loss: 0.6941176056861877\n",
            "Iteration: 5563; loss: 0.6942474842071533\n",
            "Iteration: 5564; loss: 0.6918984055519104\n",
            "Iteration: 5565; loss: 0.689374566078186\n",
            "Iteration: 5566; loss: 0.694318950176239\n",
            "Iteration: 5567; loss: 0.6960101127624512\n",
            "Iteration: 5568; loss: 0.6938109993934631\n",
            "Iteration: 5569; loss: 0.6910701394081116\n",
            "Iteration: 5570; loss: 0.6968897581100464\n",
            "Iteration: 5571; loss: 0.6926592588424683\n",
            "Iteration: 5572; loss: 0.6915320754051208\n",
            "Iteration: 5573; loss: 0.6880953907966614\n",
            "Iteration: 5574; loss: 0.6906322240829468\n",
            "Iteration: 5575; loss: 0.6937593817710876\n",
            "Iteration: 5576; loss: 0.6937582492828369\n",
            "Iteration: 5577; loss: 0.6940138339996338\n",
            "Iteration: 5578; loss: 0.688670814037323\n",
            "Iteration: 5579; loss: 0.6940892338752747\n",
            "Iteration: 5580; loss: 0.6949917078018188\n",
            "Iteration: 5581; loss: 0.6933773756027222\n",
            "Iteration: 5582; loss: 0.6916974782943726\n",
            "Iteration: 5583; loss: 0.6900477409362793\n",
            "Iteration: 5584; loss: 0.6916316747665405\n",
            "Iteration: 5585; loss: 0.6928996443748474\n",
            "Iteration: 5586; loss: 0.6960226893424988\n",
            "Iteration: 5587; loss: 0.694260835647583\n",
            "Iteration: 5588; loss: 0.6883251667022705\n",
            "Iteration: 5589; loss: 0.6907310485839844\n",
            "Iteration: 5590; loss: 0.6922349333763123\n",
            "Iteration: 5591; loss: 0.6964225769042969\n",
            "Iteration: 5592; loss: 0.692668616771698\n",
            "Iteration: 5593; loss: 0.6922683119773865\n",
            "Iteration: 5594; loss: 0.6953867077827454\n",
            "Iteration: 5595; loss: 0.6923564672470093\n",
            "Iteration: 5596; loss: 0.6973887085914612\n",
            "Iteration: 5597; loss: 0.6912041902542114\n",
            "Iteration: 5598; loss: 0.692340075969696\n",
            "Iteration: 5599; loss: 0.6916905641555786\n",
            "Iteration: 5600; loss: 0.6923758387565613\n",
            "Iteration: 5601; loss: 0.6926568746566772\n",
            "Iteration: 5602; loss: 0.6877573728561401\n",
            "Iteration: 5603; loss: 0.6892994046211243\n",
            "Iteration: 5604; loss: 0.698397159576416\n",
            "Iteration: 5605; loss: 0.6934769153594971\n",
            "Iteration: 5606; loss: 0.6940774321556091\n",
            "Iteration: 5607; loss: 0.6957238912582397\n",
            "Iteration: 5608; loss: 0.6954411864280701\n",
            "Iteration: 5609; loss: 0.693351149559021\n",
            "Iteration: 5610; loss: 0.6942464113235474\n",
            "Iteration: 5611; loss: 0.6909023523330688\n",
            "Iteration: 5612; loss: 0.6906014680862427\n",
            "Iteration: 5613; loss: 0.6934406161308289\n",
            "Iteration: 5614; loss: 0.6913637518882751\n",
            "Iteration: 5615; loss: 0.692833662033081\n",
            "Iteration: 5616; loss: 0.6925593018531799\n",
            "Iteration: 5617; loss: 0.6943366527557373\n",
            "Iteration: 5618; loss: 0.6920878887176514\n",
            "Iteration: 5619; loss: 0.6953690052032471\n",
            "Iteration: 5620; loss: 0.6967366337776184\n",
            "Iteration: 5621; loss: 0.6945696473121643\n",
            "Iteration: 5622; loss: 0.6952267289161682\n",
            "Iteration: 5623; loss: 0.6919969320297241\n",
            "Iteration: 5624; loss: 0.6929094195365906\n",
            "Iteration: 5625; loss: 0.694103479385376\n",
            "Iteration: 5626; loss: 0.6931614875793457\n",
            "Iteration: 5627; loss: 0.6951104402542114\n",
            "Iteration: 5628; loss: 0.6910312175750732\n",
            "Iteration: 5629; loss: 0.6948586106300354\n",
            "Iteration: 5630; loss: 0.6954236030578613\n",
            "Iteration: 5631; loss: 0.6926373839378357\n",
            "Iteration: 5632; loss: 0.6983435750007629\n",
            "Iteration: 5633; loss: 0.6914962530136108\n",
            "Iteration: 5634; loss: 0.6914548873901367\n",
            "Iteration: 5635; loss: 0.6938127279281616\n",
            "Iteration: 5636; loss: 0.6975470185279846\n",
            "Iteration: 5637; loss: 0.6939326524734497\n",
            "Iteration: 5638; loss: 0.6972637176513672\n",
            "Iteration: 5639; loss: 0.6955753564834595\n",
            "Iteration: 5640; loss: 0.6948193907737732\n",
            "Iteration: 5641; loss: 0.6948249936103821\n",
            "Iteration: 5642; loss: 0.6939018368721008\n",
            "Iteration: 5643; loss: 0.692127525806427\n",
            "Iteration: 5644; loss: 0.6939676403999329\n",
            "Iteration: 5645; loss: 0.6938710808753967\n",
            "Iteration: 5646; loss: 0.6937766671180725\n",
            "Iteration: 5647; loss: 0.6966333985328674\n",
            "Iteration: 5648; loss: 0.6909956336021423\n",
            "Iteration: 5649; loss: 0.6919633150100708\n",
            "Iteration: 5650; loss: 0.697311282157898\n",
            "Iteration: 5651; loss: 0.6934843063354492\n",
            "Iteration: 5652; loss: 0.6922545433044434\n",
            "Iteration: 5653; loss: 0.6952329277992249\n",
            "Iteration: 5654; loss: 0.6947035789489746\n",
            "Iteration: 5655; loss: 0.6962031126022339\n",
            "Iteration: 5656; loss: 0.6938655376434326\n",
            "Iteration: 5657; loss: 0.695295512676239\n",
            "Iteration: 5658; loss: 0.6926495432853699\n",
            "Iteration: 5659; loss: 0.693323016166687\n",
            "Iteration: 5660; loss: 0.6935409903526306\n",
            "Iteration: 5661; loss: 0.6917171478271484\n",
            "Iteration: 5662; loss: 0.695377767086029\n",
            "Iteration: 5663; loss: 0.6913489103317261\n",
            "Iteration: 5664; loss: 0.6929022073745728\n",
            "Iteration: 5665; loss: 0.6909351944923401\n",
            "Iteration: 5666; loss: 0.6904633641242981\n",
            "Iteration: 5667; loss: 0.6922723054885864\n",
            "Iteration: 5668; loss: 0.6926844716072083\n",
            "Iteration: 5669; loss: 0.6953914761543274\n",
            "Iteration: 5670; loss: 0.6953397989273071\n",
            "Iteration: 5671; loss: 0.6934797763824463\n",
            "Iteration: 5672; loss: 0.6927320957183838\n",
            "Iteration: 5673; loss: 0.6935057044029236\n",
            "Iteration: 5674; loss: 0.6937656402587891\n",
            "Iteration: 5675; loss: 0.6948405504226685\n",
            "Iteration: 5676; loss: 0.692115068435669\n",
            "Iteration: 5677; loss: 0.6916171908378601\n",
            "Iteration: 5678; loss: 0.6948703527450562\n",
            "Iteration: 5679; loss: 0.6942123770713806\n",
            "Iteration: 5680; loss: 0.6937559843063354\n",
            "Iteration: 5681; loss: 0.6929628849029541\n",
            "Iteration: 5682; loss: 0.6939764022827148\n",
            "Iteration: 5683; loss: 0.6929389238357544\n",
            "Iteration: 5684; loss: 0.6931093335151672\n",
            "Iteration: 5685; loss: 0.6920925378799438\n",
            "Iteration: 5686; loss: 0.6928300261497498\n",
            "Iteration: 5687; loss: 0.6927531957626343\n",
            "Iteration: 5688; loss: 0.692611038684845\n",
            "Iteration: 5689; loss: 0.6964884400367737\n",
            "Iteration: 5690; loss: 0.691470742225647\n",
            "Iteration: 5691; loss: 0.6940707564353943\n",
            "Iteration: 5692; loss: 0.6936489939689636\n",
            "Iteration: 5693; loss: 0.690751850605011\n",
            "Iteration: 5694; loss: 0.6946674585342407\n",
            "Iteration: 5695; loss: 0.6919900178909302\n",
            "Iteration: 5696; loss: 0.6926602125167847\n",
            "Iteration: 5697; loss: 0.6922928094863892\n",
            "Iteration: 5698; loss: 0.6921152472496033\n",
            "Iteration: 5699; loss: 0.6952375173568726\n",
            "Iteration: 5700; loss: 0.6923720836639404\n",
            "Iteration: 5701; loss: 0.6930149793624878\n",
            "Iteration: 5702; loss: 0.6921252608299255\n",
            "Iteration: 5703; loss: 0.6928466558456421\n",
            "Iteration: 5704; loss: 0.6934717893600464\n",
            "Iteration: 5705; loss: 0.6914581656455994\n",
            "Iteration: 5706; loss: 0.6945314407348633\n",
            "Iteration: 5707; loss: 0.6930670142173767\n",
            "Iteration: 5708; loss: 0.6915459632873535\n",
            "Iteration: 5709; loss: 0.692794919013977\n",
            "Iteration: 5710; loss: 0.6958044767379761\n",
            "Iteration: 5711; loss: 0.6920163035392761\n",
            "Iteration: 5712; loss: 0.6946028470993042\n",
            "Iteration: 5713; loss: 0.6931380033493042\n",
            "Iteration: 5714; loss: 0.6936501264572144\n",
            "Iteration: 5715; loss: 0.6931925415992737\n",
            "Iteration: 5716; loss: 0.6931343674659729\n",
            "Iteration: 5717; loss: 0.690895676612854\n",
            "Iteration: 5718; loss: 0.692497193813324\n",
            "Iteration: 5719; loss: 0.6925417184829712\n",
            "Iteration: 5720; loss: 0.6934802532196045\n",
            "Iteration: 5721; loss: 0.6951521039009094\n",
            "Iteration: 5722; loss: 0.6936227083206177\n",
            "Iteration: 5723; loss: 0.6921595931053162\n",
            "Iteration: 5724; loss: 0.6928204298019409\n",
            "Iteration: 5725; loss: 0.6922324895858765\n",
            "Iteration: 5726; loss: 0.691241979598999\n",
            "Iteration: 5727; loss: 0.6950576305389404\n",
            "Iteration: 5728; loss: 0.6932308077812195\n",
            "Iteration: 5729; loss: 0.6934008002281189\n",
            "Iteration: 5730; loss: 0.6938685774803162\n",
            "Iteration: 5731; loss: 0.6957815885543823\n",
            "Iteration: 5732; loss: 0.6915718913078308\n",
            "Iteration: 5733; loss: 0.6946951150894165\n",
            "Iteration: 5734; loss: 0.6940735578536987\n",
            "Iteration: 5735; loss: 0.694351077079773\n",
            "Iteration: 5736; loss: 0.6931729316711426\n",
            "Iteration: 5737; loss: 0.6927975416183472\n",
            "Iteration: 5738; loss: 0.6927198767662048\n",
            "Iteration: 5739; loss: 0.6914126873016357\n",
            "Iteration: 5740; loss: 0.6946417689323425\n",
            "Iteration: 5741; loss: 0.6941400766372681\n",
            "Iteration: 5742; loss: 0.6931399703025818\n",
            "Iteration: 5743; loss: 0.6903846263885498\n",
            "Iteration: 5744; loss: 0.6916868090629578\n",
            "Iteration: 5745; loss: 0.6943606734275818\n",
            "Iteration: 5746; loss: 0.6952061653137207\n",
            "Iteration: 5747; loss: 0.6950334906578064\n",
            "Iteration: 5748; loss: 0.6959355473518372\n",
            "Iteration: 5749; loss: 0.6930282115936279\n",
            "Iteration: 5750; loss: 0.6943241357803345\n",
            "Iteration: 5751; loss: 0.6941146850585938\n",
            "Iteration: 5752; loss: 0.6931307911872864\n",
            "Iteration: 5753; loss: 0.6926558613777161\n",
            "Iteration: 5754; loss: 0.6934873461723328\n",
            "Iteration: 5755; loss: 0.6947922706604004\n",
            "Iteration: 5756; loss: 0.690136730670929\n",
            "Iteration: 5757; loss: 0.6947376132011414\n",
            "Iteration: 5758; loss: 0.6930525302886963\n",
            "Iteration: 5759; loss: 0.692253828048706\n",
            "Iteration: 5760; loss: 0.6907033324241638\n",
            "Iteration: 5761; loss: 0.6930915117263794\n",
            "Iteration: 5762; loss: 0.6940226554870605\n",
            "Iteration: 5763; loss: 0.6932244300842285\n",
            "Iteration: 5764; loss: 0.6948018670082092\n",
            "Iteration: 5765; loss: 0.6950464248657227\n",
            "Iteration: 5766; loss: 0.6957189440727234\n",
            "Iteration: 5767; loss: 0.6934787631034851\n",
            "Iteration: 5768; loss: 0.691644549369812\n",
            "Iteration: 5769; loss: 0.6924740076065063\n",
            "Iteration: 5770; loss: 0.6935944557189941\n",
            "Iteration: 5771; loss: 0.6929191946983337\n",
            "Iteration: 5772; loss: 0.6943969130516052\n",
            "Iteration: 5773; loss: 0.6913604140281677\n",
            "Iteration: 5774; loss: 0.6939271688461304\n",
            "Iteration: 5775; loss: 0.6928727030754089\n",
            "Iteration: 5776; loss: 0.6917319297790527\n",
            "Iteration: 5777; loss: 0.6923664808273315\n",
            "Iteration: 5778; loss: 0.6928153038024902\n",
            "Iteration: 5779; loss: 0.6923109889030457\n",
            "Iteration: 5780; loss: 0.6937766671180725\n",
            "Iteration: 5781; loss: 0.6924329996109009\n",
            "Iteration: 5782; loss: 0.6937426924705505\n",
            "Iteration: 5783; loss: 0.6913270354270935\n",
            "Iteration: 5784; loss: 0.6901195049285889\n",
            "Iteration: 5785; loss: 0.6942782402038574\n",
            "Iteration: 5786; loss: 0.6919332146644592\n",
            "Iteration: 5787; loss: 0.6925642490386963\n",
            "Iteration: 5788; loss: 0.6946930289268494\n",
            "Iteration: 5789; loss: 0.69136643409729\n",
            "Iteration: 5790; loss: 0.6928325295448303\n",
            "Iteration: 5791; loss: 0.695418119430542\n",
            "Iteration: 5792; loss: 0.6902316212654114\n",
            "Iteration: 5793; loss: 0.6931089758872986\n",
            "Iteration: 5794; loss: 0.691260576248169\n",
            "Iteration: 5795; loss: 0.6919217109680176\n",
            "Iteration: 5796; loss: 0.6935969591140747\n",
            "Iteration: 5797; loss: 0.690754771232605\n",
            "Iteration: 5798; loss: 0.6930959224700928\n",
            "Iteration: 5799; loss: 0.6936859488487244\n",
            "Iteration: 5800; loss: 0.6934410333633423\n",
            "Iteration: 5801; loss: 0.6938331127166748\n",
            "Iteration: 5802; loss: 0.6936619281768799\n",
            "Iteration: 5803; loss: 0.6939515471458435\n",
            "Iteration: 5804; loss: 0.6926433444023132\n",
            "Iteration: 5805; loss: 0.6959254145622253\n",
            "Iteration: 5806; loss: 0.6952168941497803\n",
            "Iteration: 5807; loss: 0.6928531527519226\n",
            "Iteration: 5808; loss: 0.6942116022109985\n",
            "Iteration: 5809; loss: 0.6918899416923523\n",
            "Iteration: 5810; loss: 0.6932341456413269\n",
            "Iteration: 5811; loss: 0.6942347884178162\n",
            "Iteration: 5812; loss: 0.6912397146224976\n",
            "Iteration: 5813; loss: 0.6917182803153992\n",
            "Iteration: 5814; loss: 0.6945993304252625\n",
            "Iteration: 5815; loss: 0.6951200366020203\n",
            "Iteration: 5816; loss: 0.6929808259010315\n",
            "Iteration: 5817; loss: 0.6928433179855347\n",
            "Iteration: 5818; loss: 0.6914516687393188\n",
            "Iteration: 5819; loss: 0.6942480802536011\n",
            "Iteration: 5820; loss: 0.6920192241668701\n",
            "Iteration: 5821; loss: 0.6933040618896484\n",
            "Iteration: 5822; loss: 0.6939706206321716\n",
            "Iteration: 5823; loss: 0.6936655044555664\n",
            "Iteration: 5824; loss: 0.6913561820983887\n",
            "Iteration: 5825; loss: 0.6923677325248718\n",
            "Iteration: 5826; loss: 0.692859947681427\n",
            "Iteration: 5827; loss: 0.691899299621582\n",
            "Iteration: 5828; loss: 0.6918773651123047\n",
            "Iteration: 5829; loss: 0.6927555799484253\n",
            "Iteration: 5830; loss: 0.6927553415298462\n",
            "Iteration: 5831; loss: 0.6953082084655762\n",
            "Iteration: 5832; loss: 0.6946858763694763\n",
            "Iteration: 5833; loss: 0.6924477815628052\n",
            "Iteration: 5834; loss: 0.692548930644989\n",
            "Iteration: 5835; loss: 0.6952023506164551\n",
            "Iteration: 5836; loss: 0.692582368850708\n",
            "Iteration: 5837; loss: 0.6935220956802368\n",
            "Iteration: 5838; loss: 0.694583535194397\n",
            "Iteration: 5839; loss: 0.6951847076416016\n",
            "Iteration: 5840; loss: 0.6947147846221924\n",
            "Iteration: 5841; loss: 0.6914140582084656\n",
            "Iteration: 5842; loss: 0.6911572217941284\n",
            "Iteration: 5843; loss: 0.6925792694091797\n",
            "Iteration: 5844; loss: 0.6915222406387329\n",
            "Iteration: 5845; loss: 0.6910653710365295\n",
            "Iteration: 5846; loss: 0.6926623582839966\n",
            "Iteration: 5847; loss: 0.6940715909004211\n",
            "Iteration: 5848; loss: 0.6939057111740112\n",
            "Iteration: 5849; loss: 0.6945131421089172\n",
            "Iteration: 5850; loss: 0.6951022148132324\n",
            "Iteration: 5851; loss: 0.6940222382545471\n",
            "Iteration: 5852; loss: 0.6919225454330444\n",
            "Iteration: 5853; loss: 0.694496750831604\n",
            "Iteration: 5854; loss: 0.6959620714187622\n",
            "Iteration: 5855; loss: 0.6937175989151001\n",
            "Iteration: 5856; loss: 0.6930626630783081\n",
            "Iteration: 5857; loss: 0.6908498406410217\n",
            "Iteration: 5858; loss: 0.6918931007385254\n",
            "Iteration: 5859; loss: 0.6948682069778442\n",
            "Iteration: 5860; loss: 0.6917632222175598\n",
            "Iteration: 5861; loss: 0.695091724395752\n",
            "Iteration: 5862; loss: 0.6928207874298096\n",
            "Iteration: 5863; loss: 0.6937135457992554\n",
            "Iteration: 5864; loss: 0.692466676235199\n",
            "Iteration: 5865; loss: 0.6926095485687256\n",
            "Iteration: 5866; loss: 0.6933401226997375\n",
            "Iteration: 5867; loss: 0.6936686038970947\n",
            "Iteration: 5868; loss: 0.6947362422943115\n",
            "Iteration: 5869; loss: 0.6899285316467285\n",
            "Iteration: 5870; loss: 0.690873384475708\n",
            "Iteration: 5871; loss: 0.6926480531692505\n",
            "Iteration: 5872; loss: 0.6915266513824463\n",
            "Iteration: 5873; loss: 0.6932658553123474\n",
            "Iteration: 5874; loss: 0.6955217123031616\n",
            "Iteration: 5875; loss: 0.6922476291656494\n",
            "Iteration: 5876; loss: 0.6908725500106812\n",
            "Iteration: 5877; loss: 0.6956011056900024\n",
            "Iteration: 5878; loss: 0.6950590014457703\n",
            "Iteration: 5879; loss: 0.6912950873374939\n",
            "Iteration: 5880; loss: 0.6930550336837769\n",
            "Iteration: 5881; loss: 0.6951277256011963\n",
            "Iteration: 5882; loss: 0.6950582265853882\n",
            "Iteration: 5883; loss: 0.6933178305625916\n",
            "Iteration: 5884; loss: 0.6941023468971252\n",
            "Iteration: 5885; loss: 0.6937373876571655\n",
            "Iteration: 5886; loss: 0.694303035736084\n",
            "Iteration: 5887; loss: 0.6920419335365295\n",
            "Iteration: 5888; loss: 0.6939202547073364\n",
            "Iteration: 5889; loss: 0.690701425075531\n",
            "Iteration: 5890; loss: 0.6906412243843079\n",
            "Iteration: 5891; loss: 0.6917620897293091\n",
            "Iteration: 5892; loss: 0.6929517388343811\n",
            "Iteration: 5893; loss: 0.6902971863746643\n",
            "Iteration: 5894; loss: 0.6929589509963989\n",
            "Iteration: 5895; loss: 0.6916995644569397\n",
            "Iteration: 5896; loss: 0.6943594217300415\n",
            "Iteration: 5897; loss: 0.6964383721351624\n",
            "Iteration: 5898; loss: 0.6926596164703369\n",
            "Iteration: 5899; loss: 0.6912744045257568\n",
            "Iteration: 5900; loss: 0.6922668218612671\n",
            "Iteration: 5901; loss: 0.6930381655693054\n",
            "Iteration: 5902; loss: 0.6915637850761414\n",
            "Iteration: 5903; loss: 0.6945140957832336\n",
            "Iteration: 5904; loss: 0.6953215599060059\n",
            "Iteration: 5905; loss: 0.6922523975372314\n",
            "Iteration: 5906; loss: 0.6913255453109741\n",
            "Iteration: 5907; loss: 0.6880622506141663\n",
            "Iteration: 5908; loss: 0.6937137246131897\n",
            "Iteration: 5909; loss: 0.6933653950691223\n",
            "Iteration: 5910; loss: 0.6913743019104004\n",
            "Iteration: 5911; loss: 0.6926236748695374\n",
            "Iteration: 5912; loss: 0.6931712627410889\n",
            "Iteration: 5913; loss: 0.6943639516830444\n",
            "Iteration: 5914; loss: 0.696990430355072\n",
            "Iteration: 5915; loss: 0.6969538331031799\n",
            "Iteration: 5916; loss: 0.6918970942497253\n",
            "Iteration: 5917; loss: 0.6952881813049316\n",
            "Iteration: 5918; loss: 0.6924325227737427\n",
            "Iteration: 5919; loss: 0.6933667659759521\n",
            "Iteration: 5920; loss: 0.6928927898406982\n",
            "Iteration: 5921; loss: 0.6929786205291748\n",
            "Iteration: 5922; loss: 0.6935116052627563\n",
            "Iteration: 5923; loss: 0.696137011051178\n",
            "Iteration: 5924; loss: 0.690456748008728\n",
            "Iteration: 5925; loss: 0.6915909051895142\n",
            "Iteration: 5926; loss: 0.6921012997627258\n",
            "Iteration: 5927; loss: 0.6944745182991028\n",
            "Iteration: 5928; loss: 0.6935659646987915\n",
            "Iteration: 5929; loss: 0.6941485404968262\n",
            "Iteration: 5930; loss: 0.6957257986068726\n",
            "Iteration: 5931; loss: 0.6956585645675659\n",
            "Iteration: 5932; loss: 0.6917225122451782\n",
            "Iteration: 5933; loss: 0.6959628462791443\n",
            "Iteration: 5934; loss: 0.6929212808609009\n",
            "Iteration: 5935; loss: 0.6956545114517212\n",
            "Iteration: 5936; loss: 0.693784236907959\n",
            "Iteration: 5937; loss: 0.6915010809898376\n",
            "Iteration: 5938; loss: 0.692482590675354\n",
            "Iteration: 5939; loss: 0.6914030909538269\n",
            "Iteration: 5940; loss: 0.6925176978111267\n",
            "Iteration: 5941; loss: 0.6893408894538879\n",
            "Iteration: 5942; loss: 0.692345917224884\n",
            "Iteration: 5943; loss: 0.6925855875015259\n",
            "Iteration: 5944; loss: 0.6931871175765991\n",
            "Iteration: 5945; loss: 0.6947581768035889\n",
            "Iteration: 5946; loss: 0.6931380033493042\n",
            "Iteration: 5947; loss: 0.6909911036491394\n",
            "Iteration: 5948; loss: 0.6917333602905273\n",
            "Iteration: 5949; loss: 0.6939796209335327\n",
            "Iteration: 5950; loss: 0.6946514248847961\n",
            "Iteration: 5951; loss: 0.694916844367981\n",
            "Iteration: 5952; loss: 0.6944085955619812\n",
            "Iteration: 5953; loss: 0.693069577217102\n",
            "Iteration: 5954; loss: 0.6946747303009033\n",
            "Iteration: 5955; loss: 0.6923317313194275\n",
            "Iteration: 5956; loss: 0.6927063465118408\n",
            "Iteration: 5957; loss: 0.6925190687179565\n",
            "Iteration: 5958; loss: 0.6934215426445007\n",
            "Iteration: 5959; loss: 0.692899763584137\n",
            "Iteration: 5960; loss: 0.690260112285614\n",
            "Iteration: 5961; loss: 0.693170964717865\n",
            "Iteration: 5962; loss: 0.6911205649375916\n",
            "Iteration: 5963; loss: 0.6948264241218567\n",
            "Iteration: 5964; loss: 0.6924007534980774\n",
            "Iteration: 5965; loss: 0.6928849816322327\n",
            "Iteration: 5966; loss: 0.6924001574516296\n",
            "Iteration: 5967; loss: 0.6930062174797058\n",
            "Iteration: 5968; loss: 0.6918497085571289\n",
            "Iteration: 5969; loss: 0.6924934387207031\n",
            "Iteration: 5970; loss: 0.6938643455505371\n",
            "Iteration: 5971; loss: 0.6921627521514893\n",
            "Iteration: 5972; loss: 0.6919118165969849\n",
            "Iteration: 5973; loss: 0.6929762363433838\n",
            "Iteration: 5974; loss: 0.6911576986312866\n",
            "Iteration: 5975; loss: 0.6918090581893921\n",
            "Iteration: 5976; loss: 0.6946246027946472\n",
            "Iteration: 5977; loss: 0.6943865418434143\n",
            "Iteration: 5978; loss: 0.6951097249984741\n",
            "Iteration: 5979; loss: 0.6930978298187256\n",
            "Iteration: 5980; loss: 0.6925332546234131\n",
            "Iteration: 5981; loss: 0.6953576803207397\n",
            "Iteration: 5982; loss: 0.6923790574073792\n",
            "Iteration: 5983; loss: 0.6920442581176758\n",
            "Iteration: 5984; loss: 0.6958616971969604\n",
            "Iteration: 5985; loss: 0.692599892616272\n",
            "Iteration: 5986; loss: 0.6927515864372253\n",
            "Iteration: 5987; loss: 0.6896263360977173\n",
            "Iteration: 5988; loss: 0.6928438544273376\n",
            "Iteration: 5989; loss: 0.695765495300293\n",
            "Iteration: 5990; loss: 0.6929348111152649\n",
            "Iteration: 5991; loss: 0.6928109526634216\n",
            "Iteration: 5992; loss: 0.6956609487533569\n",
            "Iteration: 5993; loss: 0.6934272050857544\n",
            "Iteration: 5994; loss: 0.6919585466384888\n",
            "Iteration: 5995; loss: 0.6899421215057373\n",
            "Iteration: 5996; loss: 0.6962607502937317\n",
            "Iteration: 5997; loss: 0.6923981308937073\n",
            "Iteration: 5998; loss: 0.6945464015007019\n",
            "Iteration: 5999; loss: 0.6927752494812012\n",
            "Iteration: 6000; loss: 0.6923345327377319\n",
            "Iteration: 6001; loss: 0.6959617137908936\n",
            "Iteration: 6002; loss: 0.6914058327674866\n",
            "Iteration: 6003; loss: 0.6912243366241455\n",
            "Iteration: 6004; loss: 0.6965591311454773\n",
            "Iteration: 6005; loss: 0.6899571418762207\n",
            "Iteration: 6006; loss: 0.6947945952415466\n",
            "Iteration: 6007; loss: 0.6958056092262268\n",
            "Iteration: 6008; loss: 0.6935811042785645\n",
            "Iteration: 6009; loss: 0.6920769214630127\n",
            "Iteration: 6010; loss: 0.6942434906959534\n",
            "Iteration: 6011; loss: 0.6934095025062561\n",
            "Iteration: 6012; loss: 0.690352737903595\n",
            "Iteration: 6013; loss: 0.6951617002487183\n",
            "Iteration: 6014; loss: 0.691133439540863\n",
            "Iteration: 6015; loss: 0.6944706439971924\n",
            "Iteration: 6016; loss: 0.6920918226242065\n",
            "Iteration: 6017; loss: 0.6937344074249268\n",
            "Iteration: 6018; loss: 0.69093918800354\n",
            "Iteration: 6019; loss: 0.6917616724967957\n",
            "Iteration: 6020; loss: 0.6930252909660339\n",
            "Iteration: 6021; loss: 0.6940114498138428\n",
            "Iteration: 6022; loss: 0.6923940777778625\n",
            "Iteration: 6023; loss: 0.69280606508255\n",
            "Iteration: 6024; loss: 0.6927092671394348\n",
            "Iteration: 6025; loss: 0.6943745017051697\n",
            "Iteration: 6026; loss: 0.695311427116394\n",
            "Iteration: 6027; loss: 0.6954197883605957\n",
            "Iteration: 6028; loss: 0.6915634870529175\n",
            "Iteration: 6029; loss: 0.696301281452179\n",
            "Iteration: 6030; loss: 0.6920104622840881\n",
            "Iteration: 6031; loss: 0.6919034123420715\n",
            "Iteration: 6032; loss: 0.6938813328742981\n",
            "Iteration: 6033; loss: 0.6922040581703186\n",
            "Iteration: 6034; loss: 0.6949361562728882\n",
            "Iteration: 6035; loss: 0.6928887367248535\n",
            "Iteration: 6036; loss: 0.6927485466003418\n",
            "Iteration: 6037; loss: 0.6948426961898804\n",
            "Iteration: 6038; loss: 0.6910082697868347\n",
            "Iteration: 6039; loss: 0.6945301294326782\n",
            "Iteration: 6040; loss: 0.693449854850769\n",
            "Iteration: 6041; loss: 0.6924436092376709\n",
            "Iteration: 6042; loss: 0.6943614482879639\n",
            "Iteration: 6043; loss: 0.6915163397789001\n",
            "Iteration: 6044; loss: 0.6913233995437622\n",
            "Iteration: 6045; loss: 0.6962247490882874\n",
            "Iteration: 6046; loss: 0.6936042308807373\n",
            "Iteration: 6047; loss: 0.6918631792068481\n",
            "Iteration: 6048; loss: 0.6939727067947388\n",
            "Iteration: 6049; loss: 0.6909880638122559\n",
            "Iteration: 6050; loss: 0.6955874562263489\n",
            "Iteration: 6051; loss: 0.6930492520332336\n",
            "Iteration: 6052; loss: 0.6923547387123108\n",
            "Iteration: 6053; loss: 0.6958049535751343\n",
            "Iteration: 6054; loss: 0.6934168934822083\n",
            "Iteration: 6055; loss: 0.6928070783615112\n",
            "Iteration: 6056; loss: 0.6973355412483215\n",
            "Iteration: 6057; loss: 0.6946157813072205\n",
            "Iteration: 6058; loss: 0.6906996965408325\n",
            "Iteration: 6059; loss: 0.6950065493583679\n",
            "Iteration: 6060; loss: 0.6927642226219177\n",
            "Iteration: 6061; loss: 0.6963250041007996\n",
            "Iteration: 6062; loss: 0.6962487697601318\n",
            "Iteration: 6063; loss: 0.6915983557701111\n",
            "Iteration: 6064; loss: 0.693995475769043\n",
            "Iteration: 6065; loss: 0.6941382884979248\n",
            "Iteration: 6066; loss: 0.6932856440544128\n",
            "Iteration: 6067; loss: 0.6921305060386658\n",
            "Iteration: 6068; loss: 0.6937656998634338\n",
            "Iteration: 6069; loss: 0.6961694359779358\n",
            "Iteration: 6070; loss: 0.6935611963272095\n",
            "Iteration: 6071; loss: 0.6919153928756714\n",
            "Iteration: 6072; loss: 0.6934117674827576\n",
            "Iteration: 6073; loss: 0.6944454908370972\n",
            "Iteration: 6074; loss: 0.6961969137191772\n",
            "Iteration: 6075; loss: 0.6930000185966492\n",
            "Iteration: 6076; loss: 0.6925483345985413\n",
            "Iteration: 6077; loss: 0.6946377754211426\n",
            "Iteration: 6078; loss: 0.6935529112815857\n",
            "Iteration: 6079; loss: 0.693864107131958\n",
            "Iteration: 6080; loss: 0.6920187473297119\n",
            "Iteration: 6081; loss: 0.6928690075874329\n",
            "Iteration: 6082; loss: 0.6924983263015747\n",
            "Iteration: 6083; loss: 0.6914397478103638\n",
            "Iteration: 6084; loss: 0.6927573680877686\n",
            "Iteration: 6085; loss: 0.6946727633476257\n",
            "Iteration: 6086; loss: 0.6927475333213806\n",
            "Iteration: 6087; loss: 0.6932352781295776\n",
            "Iteration: 6088; loss: 0.6901785731315613\n",
            "Iteration: 6089; loss: 0.6909957528114319\n",
            "Iteration: 6090; loss: 0.6932138800621033\n",
            "Iteration: 6091; loss: 0.6943890452384949\n",
            "Iteration: 6092; loss: 0.6928927898406982\n",
            "Iteration: 6093; loss: 0.6939349174499512\n",
            "Iteration: 6094; loss: 0.6947541832923889\n",
            "Iteration: 6095; loss: 0.69120854139328\n",
            "Iteration: 6096; loss: 0.6921980977058411\n",
            "Iteration: 6097; loss: 0.6929827332496643\n",
            "Iteration: 6098; loss: 0.6932223439216614\n",
            "Iteration: 6099; loss: 0.6947783827781677\n",
            "Iteration: 6100; loss: 0.690888524055481\n",
            "Iteration: 6101; loss: 0.6947843432426453\n",
            "Iteration: 6102; loss: 0.6928654909133911\n",
            "Iteration: 6103; loss: 0.6942641735076904\n",
            "Iteration: 6104; loss: 0.6920276284217834\n",
            "Iteration: 6105; loss: 0.6930937170982361\n",
            "Iteration: 6106; loss: 0.6931185126304626\n",
            "Iteration: 6107; loss: 0.6938220262527466\n",
            "Iteration: 6108; loss: 0.6930609345436096\n",
            "Iteration: 6109; loss: 0.6918240785598755\n",
            "Iteration: 6110; loss: 0.6922425031661987\n",
            "Iteration: 6111; loss: 0.6926323175430298\n",
            "Iteration: 6112; loss: 0.6962619423866272\n",
            "Iteration: 6113; loss: 0.6915348172187805\n",
            "Iteration: 6114; loss: 0.6918839812278748\n",
            "Iteration: 6115; loss: 0.6929826736450195\n",
            "Iteration: 6116; loss: 0.6916337013244629\n",
            "Iteration: 6117; loss: 0.6925850510597229\n",
            "Iteration: 6118; loss: 0.6914053559303284\n",
            "Iteration: 6119; loss: 0.6968311667442322\n",
            "Iteration: 6120; loss: 0.6937242746353149\n",
            "Iteration: 6121; loss: 0.6958330869674683\n",
            "Iteration: 6122; loss: 0.6952756643295288\n",
            "Iteration: 6123; loss: 0.6910727024078369\n",
            "Iteration: 6124; loss: 0.69166100025177\n",
            "Iteration: 6125; loss: 0.6906527280807495\n",
            "Iteration: 6126; loss: 0.6906663179397583\n",
            "Iteration: 6127; loss: 0.6935986280441284\n",
            "Iteration: 6128; loss: 0.6931584477424622\n",
            "Iteration: 6129; loss: 0.6945804953575134\n",
            "Iteration: 6130; loss: 0.6938869953155518\n",
            "Iteration: 6131; loss: 0.6954995393753052\n",
            "Iteration: 6132; loss: 0.6951467394828796\n",
            "Iteration: 6133; loss: 0.6903041005134583\n",
            "Iteration: 6134; loss: 0.6936453580856323\n",
            "Iteration: 6135; loss: 0.6911733746528625\n",
            "Iteration: 6136; loss: 0.6933213472366333\n",
            "Iteration: 6137; loss: 0.6920521259307861\n",
            "Iteration: 6138; loss: 0.6916562914848328\n",
            "Iteration: 6139; loss: 0.693427562713623\n",
            "Iteration: 6140; loss: 0.6945052742958069\n",
            "Iteration: 6141; loss: 0.6945520043373108\n",
            "Iteration: 6142; loss: 0.6919536590576172\n",
            "Iteration: 6143; loss: 0.6923027634620667\n",
            "Iteration: 6144; loss: 0.6929423809051514\n",
            "Iteration: 6145; loss: 0.6907011270523071\n",
            "Iteration: 6146; loss: 0.6936243176460266\n",
            "Iteration: 6147; loss: 0.6930708885192871\n",
            "Iteration: 6148; loss: 0.6947108507156372\n",
            "Iteration: 6149; loss: 0.6945116519927979\n",
            "Iteration: 6150; loss: 0.6912398338317871\n",
            "Iteration: 6151; loss: 0.6921600103378296\n",
            "Iteration: 6152; loss: 0.6937323808670044\n",
            "Iteration: 6153; loss: 0.6928982138633728\n",
            "Iteration: 6154; loss: 0.6933999061584473\n",
            "Iteration: 6155; loss: 0.6926783323287964\n",
            "Iteration: 6156; loss: 0.6935902833938599\n",
            "Iteration: 6157; loss: 0.6935896277427673\n",
            "Iteration: 6158; loss: 0.6917634606361389\n",
            "Iteration: 6159; loss: 0.6901980638504028\n",
            "Iteration: 6160; loss: 0.6931030750274658\n",
            "Iteration: 6161; loss: 0.6943413019180298\n",
            "Iteration: 6162; loss: 0.6901031136512756\n",
            "Iteration: 6163; loss: 0.6914732456207275\n",
            "Iteration: 6164; loss: 0.6961817145347595\n",
            "Iteration: 6165; loss: 0.6969406008720398\n",
            "Iteration: 6166; loss: 0.6927223801612854\n",
            "Iteration: 6167; loss: 0.6939361691474915\n",
            "Iteration: 6168; loss: 0.6928579807281494\n",
            "Iteration: 6169; loss: 0.6912321448326111\n",
            "Iteration: 6170; loss: 0.6944527626037598\n",
            "Iteration: 6171; loss: 0.6939517855644226\n",
            "Iteration: 6172; loss: 0.6895599365234375\n",
            "Iteration: 6173; loss: 0.6909542679786682\n",
            "Iteration: 6174; loss: 0.6895184516906738\n",
            "Iteration: 6175; loss: 0.6925187110900879\n",
            "Iteration: 6176; loss: 0.6912183165550232\n",
            "Iteration: 6177; loss: 0.693389892578125\n",
            "Iteration: 6178; loss: 0.6949465870857239\n",
            "Iteration: 6179; loss: 0.6904025673866272\n",
            "Iteration: 6180; loss: 0.6943036317825317\n",
            "Iteration: 6181; loss: 0.6934563517570496\n",
            "Iteration: 6182; loss: 0.6939533948898315\n",
            "Iteration: 6183; loss: 0.6949342489242554\n",
            "Iteration: 6184; loss: 0.6932243704795837\n",
            "Iteration: 6185; loss: 0.690055787563324\n",
            "Iteration: 6186; loss: 0.6940721273422241\n",
            "Iteration: 6187; loss: 0.6959607601165771\n",
            "Iteration: 6188; loss: 0.6930628418922424\n",
            "Iteration: 6189; loss: 0.6931104063987732\n",
            "Iteration: 6190; loss: 0.693316638469696\n",
            "Iteration: 6191; loss: 0.6928345561027527\n",
            "Iteration: 6192; loss: 0.6944034099578857\n",
            "Iteration: 6193; loss: 0.6932242512702942\n",
            "Iteration: 6194; loss: 0.6961445808410645\n",
            "Iteration: 6195; loss: 0.6915642619132996\n",
            "Iteration: 6196; loss: 0.6931771636009216\n",
            "Iteration: 6197; loss: 0.6907023787498474\n",
            "Iteration: 6198; loss: 0.6925754547119141\n",
            "Iteration: 6199; loss: 0.693611741065979\n",
            "Iteration: 6200; loss: 0.6942408084869385\n",
            "Iteration: 6201; loss: 0.6931848526000977\n",
            "Iteration: 6202; loss: 0.692893922328949\n",
            "Iteration: 6203; loss: 0.69382643699646\n",
            "Iteration: 6204; loss: 0.6935787200927734\n",
            "Iteration: 6205; loss: 0.6912391781806946\n",
            "Iteration: 6206; loss: 0.6910960674285889\n",
            "Iteration: 6207; loss: 0.6937011480331421\n",
            "Iteration: 6208; loss: 0.6919784545898438\n",
            "Iteration: 6209; loss: 0.6939314007759094\n",
            "Iteration: 6210; loss: 0.6972336769104004\n",
            "Iteration: 6211; loss: 0.6915749311447144\n",
            "Iteration: 6212; loss: 0.6947932243347168\n",
            "Iteration: 6213; loss: 0.6961272954940796\n",
            "Iteration: 6214; loss: 0.690929651260376\n",
            "Iteration: 6215; loss: 0.6902711391448975\n",
            "Iteration: 6216; loss: 0.6939665675163269\n",
            "Iteration: 6217; loss: 0.6965784430503845\n",
            "Iteration: 6218; loss: 0.6928884387016296\n",
            "Iteration: 6219; loss: 0.6925709247589111\n",
            "Iteration: 6220; loss: 0.6910632848739624\n",
            "Iteration: 6221; loss: 0.694200336933136\n",
            "Iteration: 6222; loss: 0.6936571002006531\n",
            "Iteration: 6223; loss: 0.6922606229782104\n",
            "Iteration: 6224; loss: 0.6924899220466614\n",
            "Iteration: 6225; loss: 0.69404137134552\n",
            "Iteration: 6226; loss: 0.6938257217407227\n",
            "Iteration: 6227; loss: 0.6927186250686646\n",
            "Iteration: 6228; loss: 0.6959524154663086\n",
            "Iteration: 6229; loss: 0.6911746859550476\n",
            "Iteration: 6230; loss: 0.6948290467262268\n",
            "Iteration: 6231; loss: 0.6904102563858032\n",
            "Iteration: 6232; loss: 0.6916226148605347\n",
            "Iteration: 6233; loss: 0.6911101341247559\n",
            "Iteration: 6234; loss: 0.691633939743042\n",
            "Iteration: 6235; loss: 0.6929029226303101\n",
            "Iteration: 6236; loss: 0.6923320889472961\n",
            "Iteration: 6237; loss: 0.6903152465820312\n",
            "Iteration: 6238; loss: 0.6951174736022949\n",
            "Iteration: 6239; loss: 0.6937088370323181\n",
            "Iteration: 6240; loss: 0.6933045387268066\n",
            "Iteration: 6241; loss: 0.6955204010009766\n",
            "Iteration: 6242; loss: 0.6935805678367615\n",
            "Iteration: 6243; loss: 0.6925711631774902\n",
            "Iteration: 6244; loss: 0.6932806968688965\n",
            "Iteration: 6245; loss: 0.6909600496292114\n",
            "Iteration: 6246; loss: 0.6920075416564941\n",
            "Iteration: 6247; loss: 0.6943912506103516\n",
            "Iteration: 6248; loss: 0.6922615766525269\n",
            "Iteration: 6249; loss: 0.6917228102684021\n",
            "Iteration: 6250; loss: 0.696013331413269\n",
            "Iteration: 6251; loss: 0.6923835277557373\n",
            "Iteration: 6252; loss: 0.6961148381233215\n",
            "Iteration: 6253; loss: 0.6943641901016235\n",
            "Iteration: 6254; loss: 0.6936413049697876\n",
            "Iteration: 6255; loss: 0.6935046315193176\n",
            "Iteration: 6256; loss: 0.6935193538665771\n",
            "Iteration: 6257; loss: 0.692902684211731\n",
            "Iteration: 6258; loss: 0.6949874758720398\n",
            "Iteration: 6259; loss: 0.6948551535606384\n",
            "Iteration: 6260; loss: 0.6930251717567444\n",
            "Iteration: 6261; loss: 0.6943842172622681\n",
            "Iteration: 6262; loss: 0.693902313709259\n",
            "Iteration: 6263; loss: 0.691114068031311\n",
            "Iteration: 6264; loss: 0.6891739964485168\n",
            "Iteration: 6265; loss: 0.693077027797699\n",
            "Iteration: 6266; loss: 0.6933680772781372\n",
            "Iteration: 6267; loss: 0.6924699544906616\n",
            "Iteration: 6268; loss: 0.6943210363388062\n",
            "Iteration: 6269; loss: 0.6908976435661316\n",
            "Iteration: 6270; loss: 0.6959250569343567\n",
            "Iteration: 6271; loss: 0.6926765441894531\n",
            "Iteration: 6272; loss: 0.693458080291748\n",
            "Iteration: 6273; loss: 0.6935921311378479\n",
            "Iteration: 6274; loss: 0.6948390007019043\n",
            "Iteration: 6275; loss: 0.6941537261009216\n",
            "Iteration: 6276; loss: 0.692488431930542\n",
            "Iteration: 6277; loss: 0.694949209690094\n",
            "Iteration: 6278; loss: 0.691486120223999\n",
            "Iteration: 6279; loss: 0.6937670707702637\n",
            "Iteration: 6280; loss: 0.6930544972419739\n",
            "Iteration: 6281; loss: 0.6947965621948242\n",
            "Iteration: 6282; loss: 0.6943028569221497\n",
            "Iteration: 6283; loss: 0.6930029988288879\n",
            "Iteration: 6284; loss: 0.6914858222007751\n",
            "Iteration: 6285; loss: 0.6939429044723511\n",
            "Iteration: 6286; loss: 0.6933692693710327\n",
            "Iteration: 6287; loss: 0.6918951869010925\n",
            "Iteration: 6288; loss: 0.6918295621871948\n",
            "Iteration: 6289; loss: 0.6938133239746094\n",
            "Iteration: 6290; loss: 0.6923907399177551\n",
            "Iteration: 6291; loss: 0.6891688704490662\n",
            "Iteration: 6292; loss: 0.6918928623199463\n",
            "Iteration: 6293; loss: 0.6945781707763672\n",
            "Iteration: 6294; loss: 0.6915572881698608\n",
            "Iteration: 6295; loss: 0.6973940134048462\n",
            "Iteration: 6296; loss: 0.6938562393188477\n",
            "Iteration: 6297; loss: 0.6912104487419128\n",
            "Iteration: 6298; loss: 0.6952338218688965\n",
            "Iteration: 6299; loss: 0.6930655241012573\n",
            "Iteration: 6300; loss: 0.693835973739624\n",
            "Iteration: 6301; loss: 0.6924950480461121\n",
            "Iteration: 6302; loss: 0.6943135261535645\n",
            "Iteration: 6303; loss: 0.6914283037185669\n",
            "Iteration: 6304; loss: 0.6930065751075745\n",
            "Iteration: 6305; loss: 0.694222629070282\n",
            "Iteration: 6306; loss: 0.6948763132095337\n",
            "Iteration: 6307; loss: 0.6934963464736938\n",
            "Iteration: 6308; loss: 0.6924309134483337\n",
            "Iteration: 6309; loss: 0.693471372127533\n",
            "Iteration: 6310; loss: 0.6908119320869446\n",
            "Iteration: 6311; loss: 0.6923646926879883\n",
            "Iteration: 6312; loss: 0.6944862008094788\n",
            "Iteration: 6313; loss: 0.6932584643363953\n",
            "Iteration: 6314; loss: 0.6921685934066772\n",
            "Iteration: 6315; loss: 0.6929430365562439\n",
            "Iteration: 6316; loss: 0.6935904026031494\n",
            "Iteration: 6317; loss: 0.6928825378417969\n",
            "Iteration: 6318; loss: 0.6930316686630249\n",
            "Iteration: 6319; loss: 0.6908867955207825\n",
            "Iteration: 6320; loss: 0.6958435773849487\n",
            "Iteration: 6321; loss: 0.6928391456604004\n",
            "Iteration: 6322; loss: 0.6953483819961548\n",
            "Iteration: 6323; loss: 0.6938616633415222\n",
            "Iteration: 6324; loss: 0.692179799079895\n",
            "Iteration: 6325; loss: 0.6954765915870667\n",
            "Iteration: 6326; loss: 0.6938612461090088\n",
            "Iteration: 6327; loss: 0.6924233436584473\n",
            "Iteration: 6328; loss: 0.6942447423934937\n",
            "Iteration: 6329; loss: 0.6938533186912537\n",
            "Iteration: 6330; loss: 0.6934751868247986\n",
            "Iteration: 6331; loss: 0.6902272701263428\n",
            "Iteration: 6332; loss: 0.6941200494766235\n",
            "Iteration: 6333; loss: 0.6964148879051208\n",
            "Iteration: 6334; loss: 0.6925971508026123\n",
            "Iteration: 6335; loss: 0.6924189925193787\n",
            "Iteration: 6336; loss: 0.6942874789237976\n",
            "Iteration: 6337; loss: 0.6949549317359924\n",
            "Iteration: 6338; loss: 0.6926494240760803\n",
            "Iteration: 6339; loss: 0.6945105195045471\n",
            "Iteration: 6340; loss: 0.6939476132392883\n",
            "Iteration: 6341; loss: 0.693473756313324\n",
            "Iteration: 6342; loss: 0.6941506266593933\n",
            "Iteration: 6343; loss: 0.6958723068237305\n",
            "Iteration: 6344; loss: 0.6931728720664978\n",
            "Iteration: 6345; loss: 0.6932272911071777\n",
            "Iteration: 6346; loss: 0.6947313547134399\n",
            "Iteration: 6347; loss: 0.6936669945716858\n",
            "Iteration: 6348; loss: 0.6939325332641602\n",
            "Iteration: 6349; loss: 0.6947752237319946\n",
            "Iteration: 6350; loss: 0.6933010816574097\n",
            "Iteration: 6351; loss: 0.6886788010597229\n",
            "Iteration: 6352; loss: 0.6931504607200623\n",
            "Iteration: 6353; loss: 0.6942999362945557\n",
            "Iteration: 6354; loss: 0.6935044527053833\n",
            "Iteration: 6355; loss: 0.6939310431480408\n",
            "Iteration: 6356; loss: 0.6945326924324036\n",
            "Iteration: 6357; loss: 0.6943466067314148\n",
            "Iteration: 6358; loss: 0.6925055980682373\n",
            "Iteration: 6359; loss: 0.6947284936904907\n",
            "Iteration: 6360; loss: 0.6929782629013062\n",
            "Iteration: 6361; loss: 0.693567156791687\n",
            "Iteration: 6362; loss: 0.6944496631622314\n",
            "Iteration: 6363; loss: 0.6924149394035339\n",
            "Iteration: 6364; loss: 0.6942789554595947\n",
            "Iteration: 6365; loss: 0.6919379234313965\n",
            "Iteration: 6366; loss: 0.6929946541786194\n",
            "Iteration: 6367; loss: 0.6934448480606079\n",
            "Iteration: 6368; loss: 0.6930111646652222\n",
            "Iteration: 6369; loss: 0.6937055587768555\n",
            "Iteration: 6370; loss: 0.693792462348938\n",
            "Iteration: 6371; loss: 0.6943437457084656\n",
            "Iteration: 6372; loss: 0.6941136121749878\n",
            "Iteration: 6373; loss: 0.6941738724708557\n",
            "Iteration: 6374; loss: 0.6912217140197754\n",
            "Iteration: 6375; loss: 0.6927285194396973\n",
            "Iteration: 6376; loss: 0.693874180316925\n",
            "Iteration: 6377; loss: 0.6929042339324951\n",
            "Iteration: 6378; loss: 0.6924746036529541\n",
            "Iteration: 6379; loss: 0.6928150653839111\n",
            "Iteration: 6380; loss: 0.6929785013198853\n",
            "Iteration: 6381; loss: 0.6937394142150879\n",
            "Iteration: 6382; loss: 0.6927357912063599\n",
            "Iteration: 6383; loss: 0.694438099861145\n",
            "Iteration: 6384; loss: 0.6943246126174927\n",
            "Iteration: 6385; loss: 0.6936686635017395\n",
            "Iteration: 6386; loss: 0.6938565969467163\n",
            "Iteration: 6387; loss: 0.6935790181159973\n",
            "Iteration: 6388; loss: 0.692511796951294\n",
            "Iteration: 6389; loss: 0.6918388605117798\n",
            "Iteration: 6390; loss: 0.6948704719543457\n",
            "Iteration: 6391; loss: 0.6918691992759705\n",
            "Iteration: 6392; loss: 0.6930921673774719\n",
            "Iteration: 6393; loss: 0.6939694881439209\n",
            "Iteration: 6394; loss: 0.6950398683547974\n",
            "Iteration: 6395; loss: 0.6928334832191467\n",
            "Iteration: 6396; loss: 0.6936284899711609\n",
            "Iteration: 6397; loss: 0.6937162280082703\n",
            "Iteration: 6398; loss: 0.6925816535949707\n",
            "Iteration: 6399; loss: 0.6922794580459595\n",
            "Iteration: 6400; loss: 0.6943697929382324\n",
            "Iteration: 6401; loss: 0.6935076713562012\n",
            "Iteration: 6402; loss: 0.6934342384338379\n",
            "Iteration: 6403; loss: 0.6947112679481506\n",
            "Iteration: 6404; loss: 0.6944102644920349\n",
            "Iteration: 6405; loss: 0.6936370730400085\n",
            "Iteration: 6406; loss: 0.6935116052627563\n",
            "Iteration: 6407; loss: 0.6909278035163879\n",
            "Iteration: 6408; loss: 0.6930912733078003\n",
            "Iteration: 6409; loss: 0.6938885450363159\n",
            "Iteration: 6410; loss: 0.6937103271484375\n",
            "Iteration: 6411; loss: 0.6920289397239685\n",
            "Iteration: 6412; loss: 0.6926208138465881\n",
            "Iteration: 6413; loss: 0.6944065690040588\n",
            "Iteration: 6414; loss: 0.6935228705406189\n",
            "Iteration: 6415; loss: 0.6937496066093445\n",
            "Iteration: 6416; loss: 0.6918113827705383\n",
            "Iteration: 6417; loss: 0.693455696105957\n",
            "Iteration: 6418; loss: 0.6928603649139404\n",
            "Iteration: 6419; loss: 0.6922304630279541\n",
            "Iteration: 6420; loss: 0.69171541929245\n",
            "Iteration: 6421; loss: 0.6927449703216553\n",
            "Iteration: 6422; loss: 0.691213071346283\n",
            "Iteration: 6423; loss: 0.6933305859565735\n",
            "Iteration: 6424; loss: 0.6947404146194458\n",
            "Iteration: 6425; loss: 0.6917670965194702\n",
            "Iteration: 6426; loss: 0.695552408695221\n",
            "Iteration: 6427; loss: 0.6934647560119629\n",
            "Iteration: 6428; loss: 0.6936506032943726\n",
            "Iteration: 6429; loss: 0.6935843825340271\n",
            "Iteration: 6430; loss: 0.6904727220535278\n",
            "Iteration: 6431; loss: 0.6924721598625183\n",
            "Iteration: 6432; loss: 0.6937086582183838\n",
            "Iteration: 6433; loss: 0.6940103769302368\n",
            "Iteration: 6434; loss: 0.6926556825637817\n",
            "Iteration: 6435; loss: 0.6947898268699646\n",
            "Iteration: 6436; loss: 0.6935328245162964\n",
            "Iteration: 6437; loss: 0.6917526721954346\n",
            "Iteration: 6438; loss: 0.6929861307144165\n",
            "Iteration: 6439; loss: 0.6928720474243164\n",
            "Iteration: 6440; loss: 0.6936869025230408\n",
            "Iteration: 6441; loss: 0.693814754486084\n",
            "Iteration: 6442; loss: 0.6923215985298157\n",
            "Iteration: 6443; loss: 0.6940004825592041\n",
            "Iteration: 6444; loss: 0.6938231587409973\n",
            "Iteration: 6445; loss: 0.6922199726104736\n",
            "Iteration: 6446; loss: 0.6949148774147034\n",
            "Iteration: 6447; loss: 0.6915973424911499\n",
            "Iteration: 6448; loss: 0.6928364038467407\n",
            "Iteration: 6449; loss: 0.6928630471229553\n",
            "Iteration: 6450; loss: 0.6929326057434082\n",
            "Iteration: 6451; loss: 0.6944828033447266\n",
            "Iteration: 6452; loss: 0.6907218098640442\n",
            "Iteration: 6453; loss: 0.6926214694976807\n",
            "Iteration: 6454; loss: 0.6924486756324768\n",
            "Iteration: 6455; loss: 0.6928377747535706\n",
            "Iteration: 6456; loss: 0.6936752200126648\n",
            "Iteration: 6457; loss: 0.6901947259902954\n",
            "Iteration: 6458; loss: 0.6919503808021545\n",
            "Iteration: 6459; loss: 0.6933545470237732\n",
            "Iteration: 6460; loss: 0.6942005753517151\n",
            "Iteration: 6461; loss: 0.6942116618156433\n",
            "Iteration: 6462; loss: 0.6930263638496399\n",
            "Iteration: 6463; loss: 0.6927187442779541\n",
            "Iteration: 6464; loss: 0.6939645409584045\n",
            "Iteration: 6465; loss: 0.6952328681945801\n",
            "Iteration: 6466; loss: 0.692226767539978\n",
            "Iteration: 6467; loss: 0.6926428079605103\n",
            "Iteration: 6468; loss: 0.692915678024292\n",
            "Iteration: 6469; loss: 0.6929665803909302\n",
            "Iteration: 6470; loss: 0.6926761269569397\n",
            "Iteration: 6471; loss: 0.6946283578872681\n",
            "Iteration: 6472; loss: 0.6929290890693665\n",
            "Iteration: 6473; loss: 0.6917570233345032\n",
            "Iteration: 6474; loss: 0.6918342113494873\n",
            "Iteration: 6475; loss: 0.6924012899398804\n",
            "Iteration: 6476; loss: 0.6922090649604797\n",
            "Iteration: 6477; loss: 0.6901050209999084\n",
            "Iteration: 6478; loss: 0.6936092972755432\n",
            "Iteration: 6479; loss: 0.6950287222862244\n",
            "Iteration: 6480; loss: 0.6928544044494629\n",
            "Iteration: 6481; loss: 0.6925663948059082\n",
            "Iteration: 6482; loss: 0.6936072707176208\n",
            "Iteration: 6483; loss: 0.6921172142028809\n",
            "Iteration: 6484; loss: 0.695408821105957\n",
            "Iteration: 6485; loss: 0.6938276290893555\n",
            "Iteration: 6486; loss: 0.6929734349250793\n",
            "Iteration: 6487; loss: 0.6920655965805054\n",
            "Iteration: 6488; loss: 0.6926515698432922\n",
            "Iteration: 6489; loss: 0.6906006336212158\n",
            "Iteration: 6490; loss: 0.6936972141265869\n",
            "Iteration: 6491; loss: 0.6959488987922668\n",
            "Iteration: 6492; loss: 0.6940155625343323\n",
            "Iteration: 6493; loss: 0.6916622519493103\n",
            "Iteration: 6494; loss: 0.6929067373275757\n",
            "Iteration: 6495; loss: 0.6930386424064636\n",
            "Iteration: 6496; loss: 0.6898845434188843\n",
            "Iteration: 6497; loss: 0.6930400729179382\n",
            "Iteration: 6498; loss: 0.6909360289573669\n",
            "Iteration: 6499; loss: 0.6931756138801575\n",
            "Iteration: 6500; loss: 0.6929944157600403\n",
            "Iteration: 6501; loss: 0.6941192150115967\n",
            "Iteration: 6502; loss: 0.6946589946746826\n",
            "Iteration: 6503; loss: 0.6908959746360779\n",
            "Iteration: 6504; loss: 0.6928835511207581\n",
            "Iteration: 6505; loss: 0.6920923590660095\n",
            "Iteration: 6506; loss: 0.6914093494415283\n",
            "Iteration: 6507; loss: 0.6933486461639404\n",
            "Iteration: 6508; loss: 0.6916264891624451\n",
            "Iteration: 6509; loss: 0.6930335760116577\n",
            "Iteration: 6510; loss: 0.6913023591041565\n",
            "Iteration: 6511; loss: 0.6941065788269043\n",
            "Iteration: 6512; loss: 0.6913578510284424\n",
            "Iteration: 6513; loss: 0.6942200064659119\n",
            "Iteration: 6514; loss: 0.6922820806503296\n",
            "Iteration: 6515; loss: 0.6949554085731506\n",
            "Iteration: 6516; loss: 0.6917935609817505\n",
            "Iteration: 6517; loss: 0.6925316452980042\n",
            "Iteration: 6518; loss: 0.6917639970779419\n",
            "Iteration: 6519; loss: 0.6950985193252563\n",
            "Iteration: 6520; loss: 0.6946202516555786\n",
            "Iteration: 6521; loss: 0.6935179233551025\n",
            "Iteration: 6522; loss: 0.6904546618461609\n",
            "Iteration: 6523; loss: 0.693993330001831\n",
            "Iteration: 6524; loss: 0.6909952759742737\n",
            "Iteration: 6525; loss: 0.6921837329864502\n",
            "Iteration: 6526; loss: 0.6941235661506653\n",
            "Iteration: 6527; loss: 0.6949071884155273\n",
            "Iteration: 6528; loss: 0.6910085082054138\n",
            "Iteration: 6529; loss: 0.6950397491455078\n",
            "Iteration: 6530; loss: 0.6959992051124573\n",
            "Iteration: 6531; loss: 0.690087080001831\n",
            "Iteration: 6532; loss: 0.6932665705680847\n",
            "Iteration: 6533; loss: 0.6914429664611816\n",
            "Iteration: 6534; loss: 0.6946898698806763\n",
            "Iteration: 6535; loss: 0.6925704479217529\n",
            "Iteration: 6536; loss: 0.6954622268676758\n",
            "Iteration: 6537; loss: 0.6911925077438354\n",
            "Iteration: 6538; loss: 0.6943651437759399\n",
            "Iteration: 6539; loss: 0.6940434575080872\n",
            "Iteration: 6540; loss: 0.6944732069969177\n",
            "Iteration: 6541; loss: 0.6923671364784241\n",
            "Iteration: 6542; loss: 0.6898033022880554\n",
            "Iteration: 6543; loss: 0.6939482688903809\n",
            "Iteration: 6544; loss: 0.6940571069717407\n",
            "Iteration: 6545; loss: 0.6940677165985107\n",
            "Iteration: 6546; loss: 0.6920409202575684\n",
            "Iteration: 6547; loss: 0.6945682764053345\n",
            "Iteration: 6548; loss: 0.6893093585968018\n",
            "Iteration: 6549; loss: 0.6917072534561157\n",
            "Iteration: 6550; loss: 0.696584939956665\n",
            "Iteration: 6551; loss: 0.6899318695068359\n",
            "Iteration: 6552; loss: 0.6918975114822388\n",
            "Iteration: 6553; loss: 0.6915282011032104\n",
            "Iteration: 6554; loss: 0.6928166747093201\n",
            "Iteration: 6555; loss: 0.6928856372833252\n",
            "Iteration: 6556; loss: 0.6912055611610413\n",
            "Iteration: 6557; loss: 0.6920104622840881\n",
            "Iteration: 6558; loss: 0.6942154169082642\n",
            "Iteration: 6559; loss: 0.6918171644210815\n",
            "Iteration: 6560; loss: 0.6926872730255127\n",
            "Iteration: 6561; loss: 0.6922622919082642\n",
            "Iteration: 6562; loss: 0.6910686492919922\n",
            "Iteration: 6563; loss: 0.6879522800445557\n",
            "Iteration: 6564; loss: 0.6908223628997803\n",
            "Iteration: 6565; loss: 0.6952031254768372\n",
            "Iteration: 6566; loss: 0.6910960078239441\n",
            "Iteration: 6567; loss: 0.6925039887428284\n",
            "Iteration: 6568; loss: 0.6924972534179688\n",
            "Iteration: 6569; loss: 0.693708062171936\n",
            "Iteration: 6570; loss: 0.6914267539978027\n",
            "Iteration: 6571; loss: 0.6932000517845154\n",
            "Iteration: 6572; loss: 0.6925910115242004\n",
            "Iteration: 6573; loss: 0.6922088265419006\n",
            "Iteration: 6574; loss: 0.6928121447563171\n",
            "Iteration: 6575; loss: 0.6935416460037231\n",
            "Iteration: 6576; loss: 0.6929872632026672\n",
            "Iteration: 6577; loss: 0.6927463412284851\n",
            "Iteration: 6578; loss: 0.6917134523391724\n",
            "Iteration: 6579; loss: 0.6966387629508972\n",
            "Iteration: 6580; loss: 0.6943141222000122\n",
            "Iteration: 6581; loss: 0.6906822323799133\n",
            "Iteration: 6582; loss: 0.6902168393135071\n",
            "Iteration: 6583; loss: 0.6925808787345886\n",
            "Iteration: 6584; loss: 0.6936277747154236\n",
            "Iteration: 6585; loss: 0.6917842030525208\n",
            "Iteration: 6586; loss: 0.6941983699798584\n",
            "Iteration: 6587; loss: 0.6952371597290039\n",
            "Iteration: 6588; loss: 0.6953205466270447\n",
            "Iteration: 6589; loss: 0.6922877430915833\n",
            "Iteration: 6590; loss: 0.6927489638328552\n",
            "Iteration: 6591; loss: 0.6935155391693115\n",
            "Iteration: 6592; loss: 0.6918256878852844\n",
            "Iteration: 6593; loss: 0.6931276321411133\n",
            "Iteration: 6594; loss: 0.6896550059318542\n",
            "Iteration: 6595; loss: 0.6919636130332947\n",
            "Iteration: 6596; loss: 0.6935952305793762\n",
            "Iteration: 6597; loss: 0.6908026337623596\n",
            "Iteration: 6598; loss: 0.6942832469940186\n",
            "Iteration: 6599; loss: 0.690914511680603\n",
            "Iteration: 6600; loss: 0.6924970149993896\n",
            "Iteration: 6601; loss: 0.6945315599441528\n",
            "Iteration: 6602; loss: 0.6910029649734497\n",
            "Iteration: 6603; loss: 0.6940677762031555\n",
            "Iteration: 6604; loss: 0.6931896209716797\n",
            "Iteration: 6605; loss: 0.6919181942939758\n",
            "Iteration: 6606; loss: 0.6956431269645691\n",
            "Iteration: 6607; loss: 0.688758134841919\n",
            "Iteration: 6608; loss: 0.6956499218940735\n",
            "Iteration: 6609; loss: 0.6946776509284973\n",
            "Iteration: 6610; loss: 0.6966603994369507\n",
            "Iteration: 6611; loss: 0.6950891017913818\n",
            "Iteration: 6612; loss: 0.6960276961326599\n",
            "Iteration: 6613; loss: 0.6928292512893677\n",
            "Iteration: 6614; loss: 0.69200199842453\n",
            "Iteration: 6615; loss: 0.694724977016449\n",
            "Iteration: 6616; loss: 0.6887678503990173\n",
            "Iteration: 6617; loss: 0.6940853595733643\n",
            "Iteration: 6618; loss: 0.6920727491378784\n",
            "Iteration: 6619; loss: 0.6959477066993713\n",
            "Iteration: 6620; loss: 0.6928325295448303\n",
            "Iteration: 6621; loss: 0.6928538680076599\n",
            "Iteration: 6622; loss: 0.6947079300880432\n",
            "Iteration: 6623; loss: 0.6943372488021851\n",
            "Iteration: 6624; loss: 0.6924999952316284\n",
            "Iteration: 6625; loss: 0.6952791213989258\n",
            "Iteration: 6626; loss: 0.6926753520965576\n",
            "Iteration: 6627; loss: 0.6932136416435242\n",
            "Iteration: 6628; loss: 0.6927535533905029\n",
            "Iteration: 6629; loss: 0.693383514881134\n",
            "Iteration: 6630; loss: 0.6910921931266785\n",
            "Iteration: 6631; loss: 0.6942645907402039\n",
            "Iteration: 6632; loss: 0.69440758228302\n",
            "Iteration: 6633; loss: 0.6944690346717834\n",
            "Iteration: 6634; loss: 0.6924750208854675\n",
            "Iteration: 6635; loss: 0.6907221078872681\n",
            "Iteration: 6636; loss: 0.6978002786636353\n",
            "Iteration: 6637; loss: 0.6959074139595032\n",
            "Iteration: 6638; loss: 0.6914549469947815\n",
            "Iteration: 6639; loss: 0.69242262840271\n",
            "Iteration: 6640; loss: 0.6905823349952698\n",
            "Iteration: 6641; loss: 0.6930191516876221\n",
            "Iteration: 6642; loss: 0.6955286860466003\n",
            "Iteration: 6643; loss: 0.693740963935852\n",
            "Iteration: 6644; loss: 0.6974107027053833\n",
            "Iteration: 6645; loss: 0.6904709339141846\n",
            "Iteration: 6646; loss: 0.693257749080658\n",
            "Iteration: 6647; loss: 0.6929473280906677\n",
            "Iteration: 6648; loss: 0.6894502639770508\n",
            "Iteration: 6649; loss: 0.6914157271385193\n",
            "Iteration: 6650; loss: 0.6930695176124573\n",
            "Iteration: 6651; loss: 0.6900122165679932\n",
            "Iteration: 6652; loss: 0.692490816116333\n",
            "Iteration: 6653; loss: 0.6921334266662598\n",
            "Iteration: 6654; loss: 0.6919029355049133\n",
            "Iteration: 6655; loss: 0.6978537440299988\n",
            "Iteration: 6656; loss: 0.6936432719230652\n",
            "Iteration: 6657; loss: 0.6943036913871765\n",
            "Iteration: 6658; loss: 0.6949300169944763\n",
            "Iteration: 6659; loss: 0.6914999485015869\n",
            "Iteration: 6660; loss: 0.6925982236862183\n",
            "Iteration: 6661; loss: 0.6932281255722046\n",
            "Iteration: 6662; loss: 0.6942532658576965\n",
            "Iteration: 6663; loss: 0.6922321319580078\n",
            "Iteration: 6664; loss: 0.693099319934845\n",
            "Iteration: 6665; loss: 0.6962052583694458\n",
            "Iteration: 6666; loss: 0.6922978162765503\n",
            "Iteration: 6667; loss: 0.6940160393714905\n",
            "Iteration: 6668; loss: 0.6929446458816528\n",
            "Iteration: 6669; loss: 0.6975372433662415\n",
            "Iteration: 6670; loss: 0.692132294178009\n",
            "Iteration: 6671; loss: 0.6951193809509277\n",
            "Iteration: 6672; loss: 0.6943253874778748\n",
            "Iteration: 6673; loss: 0.6932432055473328\n",
            "Iteration: 6674; loss: 0.6941587924957275\n",
            "Iteration: 6675; loss: 0.6939607858657837\n",
            "Iteration: 6676; loss: 0.6983879804611206\n",
            "Iteration: 6677; loss: 0.690287172794342\n",
            "Iteration: 6678; loss: 0.6967634558677673\n",
            "Iteration: 6679; loss: 0.6945921182632446\n",
            "Iteration: 6680; loss: 0.6919675469398499\n",
            "Iteration: 6681; loss: 0.6960257887840271\n",
            "Iteration: 6682; loss: 0.689969539642334\n",
            "Iteration: 6683; loss: 0.6913630962371826\n",
            "Iteration: 6684; loss: 0.6964582204818726\n",
            "Iteration: 6685; loss: 0.6933332681655884\n",
            "Iteration: 6686; loss: 0.6930191516876221\n",
            "Iteration: 6687; loss: 0.6930413842201233\n",
            "Iteration: 6688; loss: 0.6950145363807678\n",
            "Iteration: 6689; loss: 0.6943949460983276\n",
            "Iteration: 6690; loss: 0.6921460628509521\n",
            "Iteration: 6691; loss: 0.693863034248352\n",
            "Iteration: 6692; loss: 0.6928074359893799\n",
            "Iteration: 6693; loss: 0.6942919492721558\n",
            "Iteration: 6694; loss: 0.6947997808456421\n",
            "Iteration: 6695; loss: 0.6923372745513916\n",
            "Iteration: 6696; loss: 0.6938735842704773\n",
            "Iteration: 6697; loss: 0.6915571093559265\n",
            "Iteration: 6698; loss: 0.691677451133728\n",
            "Iteration: 6699; loss: 0.6942846179008484\n",
            "Iteration: 6700; loss: 0.6947940587997437\n",
            "Iteration: 6701; loss: 0.6916131377220154\n",
            "Iteration: 6702; loss: 0.6906460523605347\n",
            "Iteration: 6703; loss: 0.6911336183547974\n",
            "Iteration: 6704; loss: 0.6914926767349243\n",
            "Iteration: 6705; loss: 0.6929962038993835\n",
            "Iteration: 6706; loss: 0.6939369440078735\n",
            "Iteration: 6707; loss: 0.6944695711135864\n",
            "Iteration: 6708; loss: 0.6931027173995972\n",
            "Iteration: 6709; loss: 0.6924640536308289\n",
            "Iteration: 6710; loss: 0.6914557814598083\n",
            "Iteration: 6711; loss: 0.6940642595291138\n",
            "Iteration: 6712; loss: 0.6926612257957458\n",
            "Iteration: 6713; loss: 0.6905145645141602\n",
            "Iteration: 6714; loss: 0.6927629709243774\n",
            "Iteration: 6715; loss: 0.6927330493927002\n",
            "Iteration: 6716; loss: 0.6929621696472168\n",
            "Iteration: 6717; loss: 0.6940898895263672\n",
            "Iteration: 6718; loss: 0.6934714913368225\n",
            "Iteration: 6719; loss: 0.6897857189178467\n",
            "Iteration: 6720; loss: 0.6925974488258362\n",
            "Iteration: 6721; loss: 0.6944899559020996\n",
            "Iteration: 6722; loss: 0.6947811245918274\n",
            "Iteration: 6723; loss: 0.6944445967674255\n",
            "Iteration: 6724; loss: 0.6948882341384888\n",
            "Iteration: 6725; loss: 0.691110372543335\n",
            "Iteration: 6726; loss: 0.6882065534591675\n",
            "Iteration: 6727; loss: 0.6919007897377014\n",
            "Iteration: 6728; loss: 0.6921467185020447\n",
            "Iteration: 6729; loss: 0.6928818821907043\n",
            "Iteration: 6730; loss: 0.6935971975326538\n",
            "Iteration: 6731; loss: 0.6948456764221191\n",
            "Iteration: 6732; loss: 0.6931427717208862\n",
            "Iteration: 6733; loss: 0.6915885806083679\n",
            "Iteration: 6734; loss: 0.6923461556434631\n",
            "Iteration: 6735; loss: 0.6940762400627136\n",
            "Iteration: 6736; loss: 0.6924119591712952\n",
            "Iteration: 6737; loss: 0.6929128170013428\n",
            "Iteration: 6738; loss: 0.6928489208221436\n",
            "Iteration: 6739; loss: 0.6931427121162415\n",
            "Iteration: 6740; loss: 0.6937026977539062\n",
            "Iteration: 6741; loss: 0.6923812627792358\n",
            "Iteration: 6742; loss: 0.6951334476470947\n",
            "Iteration: 6743; loss: 0.6962987184524536\n",
            "Iteration: 6744; loss: 0.6948740482330322\n",
            "Iteration: 6745; loss: 0.69588702917099\n",
            "Iteration: 6746; loss: 0.6914774775505066\n",
            "Iteration: 6747; loss: 0.69293212890625\n",
            "Iteration: 6748; loss: 0.6936073303222656\n",
            "Iteration: 6749; loss: 0.6964826583862305\n",
            "Iteration: 6750; loss: 0.6931715607643127\n",
            "Iteration: 6751; loss: 0.6926581263542175\n",
            "Iteration: 6752; loss: 0.6911309361457825\n",
            "Iteration: 6753; loss: 0.6932013630867004\n",
            "Iteration: 6754; loss: 0.690460741519928\n",
            "Iteration: 6755; loss: 0.6925106048583984\n",
            "Iteration: 6756; loss: 0.6933209896087646\n",
            "Iteration: 6757; loss: 0.6914706826210022\n",
            "Iteration: 6758; loss: 0.6937865614891052\n",
            "Iteration: 6759; loss: 0.6940604448318481\n",
            "Iteration: 6760; loss: 0.6945609450340271\n",
            "Iteration: 6761; loss: 0.6923267841339111\n",
            "Iteration: 6762; loss: 0.6928582787513733\n",
            "Iteration: 6763; loss: 0.6928174495697021\n",
            "Iteration: 6764; loss: 0.6928895711898804\n",
            "Iteration: 6765; loss: 0.6941326856613159\n",
            "Iteration: 6766; loss: 0.692573070526123\n",
            "Iteration: 6767; loss: 0.6902410387992859\n",
            "Iteration: 6768; loss: 0.6928998827934265\n",
            "Iteration: 6769; loss: 0.693215012550354\n",
            "Iteration: 6770; loss: 0.6932258605957031\n",
            "Iteration: 6771; loss: 0.6926309466362\n",
            "Iteration: 6772; loss: 0.6933651566505432\n",
            "Iteration: 6773; loss: 0.6950487494468689\n",
            "Iteration: 6774; loss: 0.6925220489501953\n",
            "Iteration: 6775; loss: 0.6914042830467224\n",
            "Iteration: 6776; loss: 0.6922377347946167\n",
            "Iteration: 6777; loss: 0.6935150623321533\n",
            "Iteration: 6778; loss: 0.6947792768478394\n",
            "Iteration: 6779; loss: 0.6953307390213013\n",
            "Iteration: 6780; loss: 0.6914545893669128\n",
            "Iteration: 6781; loss: 0.6955994367599487\n",
            "Iteration: 6782; loss: 0.6948909759521484\n",
            "Iteration: 6783; loss: 0.6907939314842224\n",
            "Iteration: 6784; loss: 0.6923967003822327\n",
            "Iteration: 6785; loss: 0.6939724087715149\n",
            "Iteration: 6786; loss: 0.6941850781440735\n",
            "Iteration: 6787; loss: 0.6932029724121094\n",
            "Iteration: 6788; loss: 0.6934079527854919\n",
            "Iteration: 6789; loss: 0.6920596957206726\n",
            "Iteration: 6790; loss: 0.6939690709114075\n",
            "Iteration: 6791; loss: 0.6920232176780701\n",
            "Iteration: 6792; loss: 0.6902027130126953\n",
            "Iteration: 6793; loss: 0.6941412687301636\n",
            "Iteration: 6794; loss: 0.6935234665870667\n",
            "Iteration: 6795; loss: 0.6929413080215454\n",
            "Iteration: 6796; loss: 0.690688967704773\n",
            "Iteration: 6797; loss: 0.6950485110282898\n",
            "Iteration: 6798; loss: 0.6941249370574951\n",
            "Iteration: 6799; loss: 0.6920541524887085\n",
            "Iteration: 6800; loss: 0.6922874450683594\n",
            "Iteration: 6801; loss: 0.691723108291626\n",
            "Iteration: 6802; loss: 0.6915774345397949\n",
            "Iteration: 6803; loss: 0.6937618851661682\n",
            "Iteration: 6804; loss: 0.6932505965232849\n",
            "Iteration: 6805; loss: 0.6913120746612549\n",
            "Iteration: 6806; loss: 0.6925266981124878\n",
            "Iteration: 6807; loss: 0.6975435614585876\n",
            "Iteration: 6808; loss: 0.6903688311576843\n",
            "Iteration: 6809; loss: 0.6906318664550781\n",
            "Iteration: 6810; loss: 0.6927966475486755\n",
            "Iteration: 6811; loss: 0.6918662190437317\n",
            "Iteration: 6812; loss: 0.6942085027694702\n",
            "Iteration: 6813; loss: 0.689385175704956\n",
            "Iteration: 6814; loss: 0.690047025680542\n",
            "Iteration: 6815; loss: 0.6940992474555969\n",
            "Iteration: 6816; loss: 0.6990357637405396\n",
            "Iteration: 6817; loss: 0.6931620240211487\n",
            "Iteration: 6818; loss: 0.6907316446304321\n",
            "Iteration: 6819; loss: 0.6923601031303406\n",
            "Iteration: 6820; loss: 0.6897870898246765\n",
            "Iteration: 6821; loss: 0.6934963464736938\n",
            "Iteration: 6822; loss: 0.6962119936943054\n",
            "Iteration: 6823; loss: 0.6960837244987488\n",
            "Iteration: 6824; loss: 0.6910017132759094\n",
            "Iteration: 6825; loss: 0.6928208470344543\n",
            "Iteration: 6826; loss: 0.6943209171295166\n",
            "Iteration: 6827; loss: 0.6925938129425049\n",
            "Iteration: 6828; loss: 0.6914517879486084\n",
            "Iteration: 6829; loss: 0.6923717260360718\n",
            "Iteration: 6830; loss: 0.6922399997711182\n",
            "Iteration: 6831; loss: 0.6941484212875366\n",
            "Iteration: 6832; loss: 0.6963425874710083\n",
            "Iteration: 6833; loss: 0.6978529691696167\n",
            "Iteration: 6834; loss: 0.6938489079475403\n",
            "Iteration: 6835; loss: 0.6925463080406189\n",
            "Iteration: 6836; loss: 0.6943168640136719\n",
            "Iteration: 6837; loss: 0.6927714347839355\n",
            "Iteration: 6838; loss: 0.6920903921127319\n",
            "Iteration: 6839; loss: 0.6951465010643005\n",
            "Iteration: 6840; loss: 0.693009614944458\n",
            "Iteration: 6841; loss: 0.6923550963401794\n",
            "Iteration: 6842; loss: 0.6937665939331055\n",
            "Iteration: 6843; loss: 0.6935496926307678\n",
            "Iteration: 6844; loss: 0.6940432190895081\n",
            "Iteration: 6845; loss: 0.6928210258483887\n",
            "Iteration: 6846; loss: 0.6922058463096619\n",
            "Iteration: 6847; loss: 0.6930445432662964\n",
            "Iteration: 6848; loss: 0.6924636363983154\n",
            "Iteration: 6849; loss: 0.6947723627090454\n",
            "Iteration: 6850; loss: 0.6924304962158203\n",
            "Iteration: 6851; loss: 0.6919885277748108\n",
            "Iteration: 6852; loss: 0.69089674949646\n",
            "Iteration: 6853; loss: 0.6912424564361572\n",
            "Iteration: 6854; loss: 0.6912283301353455\n",
            "Iteration: 6855; loss: 0.6946689486503601\n",
            "Iteration: 6856; loss: 0.6933537721633911\n",
            "Iteration: 6857; loss: 0.6946523785591125\n",
            "Iteration: 6858; loss: 0.6948851943016052\n",
            "Iteration: 6859; loss: 0.6911409497261047\n",
            "Iteration: 6860; loss: 0.692639172077179\n",
            "Iteration: 6861; loss: 0.6914767026901245\n",
            "Iteration: 6862; loss: 0.6953354477882385\n",
            "Iteration: 6863; loss: 0.693498969078064\n",
            "Iteration: 6864; loss: 0.6912859082221985\n",
            "Iteration: 6865; loss: 0.6946694850921631\n",
            "Iteration: 6866; loss: 0.6902261972427368\n",
            "Iteration: 6867; loss: 0.6923720836639404\n",
            "Iteration: 6868; loss: 0.6882575750350952\n",
            "Iteration: 6869; loss: 0.6970903277397156\n",
            "Iteration: 6870; loss: 0.6941471099853516\n",
            "Iteration: 6871; loss: 0.6947371959686279\n",
            "Iteration: 6872; loss: 0.6979861855506897\n",
            "Iteration: 6873; loss: 0.6911666989326477\n",
            "Iteration: 6874; loss: 0.6914475560188293\n",
            "Iteration: 6875; loss: 0.6954062581062317\n",
            "Iteration: 6876; loss: 0.6906678676605225\n",
            "Iteration: 6877; loss: 0.6970958709716797\n",
            "Iteration: 6878; loss: 0.6916552782058716\n",
            "Iteration: 6879; loss: 0.6954646110534668\n",
            "Iteration: 6880; loss: 0.6918671131134033\n",
            "Iteration: 6881; loss: 0.6949353814125061\n",
            "Iteration: 6882; loss: 0.6919880509376526\n",
            "Iteration: 6883; loss: 0.6927829384803772\n",
            "Iteration: 6884; loss: 0.693182110786438\n",
            "Iteration: 6885; loss: 0.6928518414497375\n",
            "Iteration: 6886; loss: 0.6922804117202759\n",
            "Iteration: 6887; loss: 0.6965113282203674\n",
            "Iteration: 6888; loss: 0.6936154365539551\n",
            "Iteration: 6889; loss: 0.6947630047798157\n",
            "Iteration: 6890; loss: 0.6948307752609253\n",
            "Iteration: 6891; loss: 0.6923285126686096\n",
            "Iteration: 6892; loss: 0.6926130056381226\n",
            "Iteration: 6893; loss: 0.6935812830924988\n",
            "Iteration: 6894; loss: 0.6926348209381104\n",
            "Iteration: 6895; loss: 0.6921835541725159\n",
            "Iteration: 6896; loss: 0.6922063231468201\n",
            "Iteration: 6897; loss: 0.6923909187316895\n",
            "Iteration: 6898; loss: 0.695500373840332\n",
            "Iteration: 6899; loss: 0.6942628026008606\n",
            "Iteration: 6900; loss: 0.6936906576156616\n",
            "Iteration: 6901; loss: 0.6949365735054016\n",
            "Iteration: 6902; loss: 0.6940218806266785\n",
            "Iteration: 6903; loss: 0.6929900050163269\n",
            "Iteration: 6904; loss: 0.6938375234603882\n",
            "Iteration: 6905; loss: 0.6950662136077881\n",
            "Iteration: 6906; loss: 0.6936790943145752\n",
            "Iteration: 6907; loss: 0.6934933662414551\n",
            "Iteration: 6908; loss: 0.6944214701652527\n",
            "Iteration: 6909; loss: 0.6925742030143738\n",
            "Iteration: 6910; loss: 0.6946436166763306\n",
            "Iteration: 6911; loss: 0.6955024600028992\n",
            "Iteration: 6912; loss: 0.6943632364273071\n",
            "Iteration: 6913; loss: 0.694331169128418\n",
            "Iteration: 6914; loss: 0.6927401423454285\n",
            "Iteration: 6915; loss: 0.6898123621940613\n",
            "Iteration: 6916; loss: 0.6902338862419128\n",
            "Iteration: 6917; loss: 0.693011462688446\n",
            "Iteration: 6918; loss: 0.6920896172523499\n",
            "Iteration: 6919; loss: 0.6932112574577332\n",
            "Iteration: 6920; loss: 0.6951296329498291\n",
            "Iteration: 6921; loss: 0.6946596503257751\n",
            "Iteration: 6922; loss: 0.6917212605476379\n",
            "Iteration: 6923; loss: 0.697045087814331\n",
            "Iteration: 6924; loss: 0.6930912137031555\n",
            "Iteration: 6925; loss: 0.6903456449508667\n",
            "Iteration: 6926; loss: 0.6926385164260864\n",
            "Iteration: 6927; loss: 0.6912573575973511\n",
            "Iteration: 6928; loss: 0.6934559941291809\n",
            "Iteration: 6929; loss: 0.6957517862319946\n",
            "Iteration: 6930; loss: 0.6941763758659363\n",
            "Iteration: 6931; loss: 0.6919312477111816\n",
            "Iteration: 6932; loss: 0.6911604404449463\n",
            "Iteration: 6933; loss: 0.6897481679916382\n",
            "Iteration: 6934; loss: 0.6933586597442627\n",
            "Iteration: 6935; loss: 0.6947745680809021\n",
            "Iteration: 6936; loss: 0.6946751475334167\n",
            "Iteration: 6937; loss: 0.6913061738014221\n",
            "Iteration: 6938; loss: 0.6943026781082153\n",
            "Iteration: 6939; loss: 0.692514181137085\n",
            "Iteration: 6940; loss: 0.6952688694000244\n",
            "Iteration: 6941; loss: 0.6898489594459534\n",
            "Iteration: 6942; loss: 0.6940310597419739\n",
            "Iteration: 6943; loss: 0.692266047000885\n",
            "Iteration: 6944; loss: 0.6886417865753174\n",
            "Iteration: 6945; loss: 0.692590057849884\n",
            "Iteration: 6946; loss: 0.6927503943443298\n",
            "Iteration: 6947; loss: 0.694227397441864\n",
            "Iteration: 6948; loss: 0.6932036876678467\n",
            "Iteration: 6949; loss: 0.6923717856407166\n",
            "Iteration: 6950; loss: 0.6938990950584412\n",
            "Iteration: 6951; loss: 0.6923668384552002\n",
            "Iteration: 6952; loss: 0.6930662393569946\n",
            "Iteration: 6953; loss: 0.6920822858810425\n",
            "Iteration: 6954; loss: 0.6930056810379028\n",
            "Iteration: 6955; loss: 0.6919593811035156\n",
            "Iteration: 6956; loss: 0.6896317601203918\n",
            "Iteration: 6957; loss: 0.6915898323059082\n",
            "Iteration: 6958; loss: 0.6949231624603271\n",
            "Iteration: 6959; loss: 0.6935410499572754\n",
            "Iteration: 6960; loss: 0.6933881044387817\n",
            "Iteration: 6961; loss: 0.6916656494140625\n",
            "Iteration: 6962; loss: 0.6927478909492493\n",
            "Iteration: 6963; loss: 0.6921486258506775\n",
            "Iteration: 6964; loss: 0.6959103941917419\n",
            "Iteration: 6965; loss: 0.6916705965995789\n",
            "Iteration: 6966; loss: 0.6936384439468384\n",
            "Iteration: 6967; loss: 0.6935370564460754\n",
            "Iteration: 6968; loss: 0.6914754509925842\n",
            "Iteration: 6969; loss: 0.692302942276001\n",
            "Iteration: 6970; loss: 0.6928542852401733\n",
            "Iteration: 6971; loss: 0.69424968957901\n",
            "Iteration: 6972; loss: 0.6957381963729858\n",
            "Iteration: 6973; loss: 0.6953245997428894\n",
            "Iteration: 6974; loss: 0.6984721422195435\n",
            "Iteration: 6975; loss: 0.6936042308807373\n",
            "Iteration: 6976; loss: 0.6938018798828125\n",
            "Iteration: 6977; loss: 0.6933894157409668\n",
            "Iteration: 6978; loss: 0.692939043045044\n",
            "Iteration: 6979; loss: 0.6890318989753723\n",
            "Iteration: 6980; loss: 0.6938981413841248\n",
            "Iteration: 6981; loss: 0.6922210454940796\n",
            "Iteration: 6982; loss: 0.6930002570152283\n",
            "Iteration: 6983; loss: 0.6930543184280396\n",
            "Iteration: 6984; loss: 0.6924022436141968\n",
            "Iteration: 6985; loss: 0.6947270631790161\n",
            "Iteration: 6986; loss: 0.6944584846496582\n",
            "Iteration: 6987; loss: 0.6912621259689331\n",
            "Iteration: 6988; loss: 0.6953171491622925\n",
            "Iteration: 6989; loss: 0.6923959255218506\n",
            "Iteration: 6990; loss: 0.6886257529258728\n",
            "Iteration: 6991; loss: 0.6945276260375977\n",
            "Iteration: 6992; loss: 0.6945290565490723\n",
            "Iteration: 6993; loss: 0.6941761374473572\n",
            "Iteration: 6994; loss: 0.6975053548812866\n",
            "Iteration: 6995; loss: 0.6917685866355896\n",
            "Iteration: 6996; loss: 0.6936906576156616\n",
            "Iteration: 6997; loss: 0.6931914687156677\n",
            "Iteration: 6998; loss: 0.6899843811988831\n",
            "Iteration: 6999; loss: 0.6913917064666748\n",
            "Iteration: 7000; loss: 0.6954487562179565\n",
            "Iteration: 7001; loss: 0.688843309879303\n",
            "Iteration: 7002; loss: 0.695111095905304\n",
            "Iteration: 7003; loss: 0.6923592686653137\n",
            "Iteration: 7004; loss: 0.6925544142723083\n",
            "Iteration: 7005; loss: 0.6936670541763306\n",
            "Iteration: 7006; loss: 0.6944401264190674\n",
            "Iteration: 7007; loss: 0.6928488612174988\n",
            "Iteration: 7008; loss: 0.6954659819602966\n",
            "Iteration: 7009; loss: 0.6938148140907288\n",
            "Iteration: 7010; loss: 0.693681538105011\n",
            "Iteration: 7011; loss: 0.6908959746360779\n",
            "Iteration: 7012; loss: 0.6915260553359985\n",
            "Iteration: 7013; loss: 0.6931301355361938\n",
            "Iteration: 7014; loss: 0.6915102601051331\n",
            "Iteration: 7015; loss: 0.6920817494392395\n",
            "Iteration: 7016; loss: 0.6935056447982788\n",
            "Iteration: 7017; loss: 0.6899252533912659\n",
            "Iteration: 7018; loss: 0.6938905715942383\n",
            "Iteration: 7019; loss: 0.690224289894104\n",
            "Iteration: 7020; loss: 0.6942015290260315\n",
            "Iteration: 7021; loss: 0.6912354826927185\n",
            "Iteration: 7022; loss: 0.6952133774757385\n",
            "Iteration: 7023; loss: 0.6920596957206726\n",
            "Iteration: 7024; loss: 0.6944896578788757\n",
            "Iteration: 7025; loss: 0.6928697824478149\n",
            "Iteration: 7026; loss: 0.6913433074951172\n",
            "Iteration: 7027; loss: 0.6951454281806946\n",
            "Iteration: 7028; loss: 0.6919613480567932\n",
            "Iteration: 7029; loss: 0.6950390338897705\n",
            "Iteration: 7030; loss: 0.6928977966308594\n",
            "Iteration: 7031; loss: 0.6944520473480225\n",
            "Iteration: 7032; loss: 0.6931280493736267\n",
            "Iteration: 7033; loss: 0.6936931014060974\n",
            "Iteration: 7034; loss: 0.6937029361724854\n",
            "Iteration: 7035; loss: 0.6919493079185486\n",
            "Iteration: 7036; loss: 0.6942251920700073\n",
            "Iteration: 7037; loss: 0.6926872134208679\n",
            "Iteration: 7038; loss: 0.696225643157959\n",
            "Iteration: 7039; loss: 0.6913216710090637\n",
            "Iteration: 7040; loss: 0.6912948489189148\n",
            "Iteration: 7041; loss: 0.6928929090499878\n",
            "Iteration: 7042; loss: 0.6882994174957275\n",
            "Iteration: 7043; loss: 0.6940047740936279\n",
            "Iteration: 7044; loss: 0.6917332410812378\n",
            "Iteration: 7045; loss: 0.690869927406311\n",
            "Iteration: 7046; loss: 0.6941861510276794\n",
            "Iteration: 7047; loss: 0.6958503723144531\n",
            "Iteration: 7048; loss: 0.6922329664230347\n",
            "Iteration: 7049; loss: 0.6950086355209351\n",
            "Iteration: 7050; loss: 0.6926233172416687\n",
            "Iteration: 7051; loss: 0.6930349469184875\n",
            "Iteration: 7052; loss: 0.6947017312049866\n",
            "Iteration: 7053; loss: 0.6915215253829956\n",
            "Iteration: 7054; loss: 0.6942231059074402\n",
            "Iteration: 7055; loss: 0.6961504220962524\n",
            "Iteration: 7056; loss: 0.6927408576011658\n",
            "Iteration: 7057; loss: 0.6919382810592651\n",
            "Iteration: 7058; loss: 0.692805290222168\n",
            "Iteration: 7059; loss: 0.6952033042907715\n",
            "Iteration: 7060; loss: 0.6935238242149353\n",
            "Iteration: 7061; loss: 0.6948689222335815\n",
            "Iteration: 7062; loss: 0.6908017992973328\n",
            "Iteration: 7063; loss: 0.6944313645362854\n",
            "Iteration: 7064; loss: 0.6934939026832581\n",
            "Iteration: 7065; loss: 0.6964892148971558\n",
            "Iteration: 7066; loss: 0.6944310069084167\n",
            "Iteration: 7067; loss: 0.6933246850967407\n",
            "Iteration: 7068; loss: 0.6937745809555054\n",
            "Iteration: 7069; loss: 0.6928812265396118\n",
            "Iteration: 7070; loss: 0.6939572095870972\n",
            "Iteration: 7071; loss: 0.6941518187522888\n",
            "Iteration: 7072; loss: 0.6945425271987915\n",
            "Iteration: 7073; loss: 0.6909306049346924\n",
            "Iteration: 7074; loss: 0.6950029134750366\n",
            "Iteration: 7075; loss: 0.6911332607269287\n",
            "Iteration: 7076; loss: 0.6908056139945984\n",
            "Iteration: 7077; loss: 0.6960291862487793\n",
            "Iteration: 7078; loss: 0.6919150948524475\n",
            "Iteration: 7079; loss: 0.6933329105377197\n",
            "Iteration: 7080; loss: 0.6928590536117554\n",
            "Iteration: 7081; loss: 0.688974916934967\n",
            "Iteration: 7082; loss: 0.693839430809021\n",
            "Iteration: 7083; loss: 0.6966955065727234\n",
            "Iteration: 7084; loss: 0.6919227838516235\n",
            "Iteration: 7085; loss: 0.6912868022918701\n",
            "Iteration: 7086; loss: 0.692977786064148\n",
            "Iteration: 7087; loss: 0.6930489540100098\n",
            "Iteration: 7088; loss: 0.6942694783210754\n",
            "Iteration: 7089; loss: 0.6921575665473938\n",
            "Iteration: 7090; loss: 0.6957670450210571\n",
            "Iteration: 7091; loss: 0.6908632516860962\n",
            "Iteration: 7092; loss: 0.6917638182640076\n",
            "Iteration: 7093; loss: 0.6928240060806274\n",
            "Iteration: 7094; loss: 0.6926378011703491\n",
            "Iteration: 7095; loss: 0.6941112279891968\n",
            "Iteration: 7096; loss: 0.6953386664390564\n",
            "Iteration: 7097; loss: 0.6919158697128296\n",
            "Iteration: 7098; loss: 0.6926525831222534\n",
            "Iteration: 7099; loss: 0.6897870302200317\n",
            "Iteration: 7100; loss: 0.6920353174209595\n",
            "Iteration: 7101; loss: 0.6930936574935913\n",
            "Iteration: 7102; loss: 0.6968779563903809\n",
            "Iteration: 7103; loss: 0.6920907497406006\n",
            "Iteration: 7104; loss: 0.693710446357727\n",
            "Iteration: 7105; loss: 0.6921532154083252\n",
            "Iteration: 7106; loss: 0.69377201795578\n",
            "Iteration: 7107; loss: 0.6932514309883118\n",
            "Iteration: 7108; loss: 0.6938380002975464\n",
            "Iteration: 7109; loss: 0.6891608834266663\n",
            "Iteration: 7110; loss: 0.693293035030365\n",
            "Iteration: 7111; loss: 0.6907640099525452\n",
            "Iteration: 7112; loss: 0.6935489773750305\n",
            "Iteration: 7113; loss: 0.6915967464447021\n",
            "Iteration: 7114; loss: 0.692352294921875\n",
            "Iteration: 7115; loss: 0.6938559412956238\n",
            "Iteration: 7116; loss: 0.6955432295799255\n",
            "Iteration: 7117; loss: 0.6929053068161011\n",
            "Iteration: 7118; loss: 0.6912740468978882\n",
            "Iteration: 7119; loss: 0.6959244608879089\n",
            "Iteration: 7120; loss: 0.6944607496261597\n",
            "Iteration: 7121; loss: 0.6951915621757507\n",
            "Iteration: 7122; loss: 0.6930294036865234\n",
            "Iteration: 7123; loss: 0.6918976306915283\n",
            "Iteration: 7124; loss: 0.6939918994903564\n",
            "Iteration: 7125; loss: 0.6975147724151611\n",
            "Iteration: 7126; loss: 0.6920482516288757\n",
            "Iteration: 7127; loss: 0.6916554570198059\n",
            "Iteration: 7128; loss: 0.6915531158447266\n",
            "Iteration: 7129; loss: 0.6887425184249878\n",
            "Iteration: 7130; loss: 0.6916936039924622\n",
            "Iteration: 7131; loss: 0.694739043712616\n",
            "Iteration: 7132; loss: 0.6923570036888123\n",
            "Iteration: 7133; loss: 0.6952676177024841\n",
            "Iteration: 7134; loss: 0.6927677989006042\n",
            "Iteration: 7135; loss: 0.6936302781105042\n",
            "Iteration: 7136; loss: 0.6889556646347046\n",
            "Iteration: 7137; loss: 0.6914507746696472\n",
            "Iteration: 7138; loss: 0.6883277297019958\n",
            "Iteration: 7139; loss: 0.6925258040428162\n",
            "Iteration: 7140; loss: 0.6911959052085876\n",
            "Iteration: 7141; loss: 0.6925228834152222\n",
            "Iteration: 7142; loss: 0.6867760419845581\n",
            "Iteration: 7143; loss: 0.6945623755455017\n",
            "Iteration: 7144; loss: 0.6925719380378723\n",
            "Iteration: 7145; loss: 0.6948069334030151\n",
            "Iteration: 7146; loss: 0.6932938098907471\n",
            "Iteration: 7147; loss: 0.6910949349403381\n",
            "Iteration: 7148; loss: 0.6937140226364136\n",
            "Iteration: 7149; loss: 0.6932215094566345\n",
            "Iteration: 7150; loss: 0.6954938173294067\n",
            "Iteration: 7151; loss: 0.6936509013175964\n",
            "Iteration: 7152; loss: 0.6905457973480225\n",
            "Iteration: 7153; loss: 0.6913508176803589\n",
            "Iteration: 7154; loss: 0.6921154260635376\n",
            "Iteration: 7155; loss: 0.6908498406410217\n",
            "Iteration: 7156; loss: 0.6946849226951599\n",
            "Iteration: 7157; loss: 0.6890031695365906\n",
            "Iteration: 7158; loss: 0.6964802742004395\n",
            "Iteration: 7159; loss: 0.6938713192939758\n",
            "Iteration: 7160; loss: 0.691351592540741\n",
            "Iteration: 7161; loss: 0.6928871870040894\n",
            "Iteration: 7162; loss: 0.6939147114753723\n",
            "Iteration: 7163; loss: 0.6942294836044312\n",
            "Iteration: 7164; loss: 0.6953291893005371\n",
            "Iteration: 7165; loss: 0.6951422095298767\n",
            "Iteration: 7166; loss: 0.6933223009109497\n",
            "Iteration: 7167; loss: 0.6943163871765137\n",
            "Iteration: 7168; loss: 0.6954122185707092\n",
            "Iteration: 7169; loss: 0.6892779469490051\n",
            "Iteration: 7170; loss: 0.6943566799163818\n",
            "Iteration: 7171; loss: 0.6940836310386658\n",
            "Iteration: 7172; loss: 0.6918742656707764\n",
            "Iteration: 7173; loss: 0.6900765895843506\n",
            "Iteration: 7174; loss: 0.6967164278030396\n",
            "Iteration: 7175; loss: 0.6926444172859192\n",
            "Iteration: 7176; loss: 0.6933467984199524\n",
            "Iteration: 7177; loss: 0.6924692392349243\n",
            "Iteration: 7178; loss: 0.6935509443283081\n",
            "Iteration: 7179; loss: 0.6914231181144714\n",
            "Iteration: 7180; loss: 0.692066490650177\n",
            "Iteration: 7181; loss: 0.6893414855003357\n",
            "Iteration: 7182; loss: 0.695488452911377\n",
            "Iteration: 7183; loss: 0.691896378993988\n",
            "Iteration: 7184; loss: 0.696674108505249\n",
            "Iteration: 7185; loss: 0.6908202767372131\n",
            "Iteration: 7186; loss: 0.6939156651496887\n",
            "Iteration: 7187; loss: 0.695470929145813\n",
            "Iteration: 7188; loss: 0.6953113675117493\n",
            "Iteration: 7189; loss: 0.6921041011810303\n",
            "Iteration: 7190; loss: 0.6937026381492615\n",
            "Iteration: 7191; loss: 0.6897009015083313\n",
            "Iteration: 7192; loss: 0.6979745030403137\n",
            "Iteration: 7193; loss: 0.690637469291687\n",
            "Iteration: 7194; loss: 0.6956753134727478\n",
            "Iteration: 7195; loss: 0.6939024925231934\n",
            "Iteration: 7196; loss: 0.693506121635437\n",
            "Iteration: 7197; loss: 0.6956321001052856\n",
            "Iteration: 7198; loss: 0.6902428865432739\n",
            "Iteration: 7199; loss: 0.6934335827827454\n",
            "Iteration: 7200; loss: 0.6915210485458374\n",
            "Iteration: 7201; loss: 0.6931001543998718\n",
            "Iteration: 7202; loss: 0.692676305770874\n",
            "Iteration: 7203; loss: 0.6948647499084473\n",
            "Iteration: 7204; loss: 0.6907322406768799\n",
            "Iteration: 7205; loss: 0.6912335753440857\n",
            "Iteration: 7206; loss: 0.6900052428245544\n",
            "Iteration: 7207; loss: 0.6881993412971497\n",
            "Iteration: 7208; loss: 0.6926494240760803\n",
            "Iteration: 7209; loss: 0.69270920753479\n",
            "Iteration: 7210; loss: 0.6915255784988403\n",
            "Iteration: 7211; loss: 0.6947294473648071\n",
            "Iteration: 7212; loss: 0.6941092610359192\n",
            "Iteration: 7213; loss: 0.6923708915710449\n",
            "Iteration: 7214; loss: 0.6919127106666565\n",
            "Iteration: 7215; loss: 0.6928738951683044\n",
            "Iteration: 7216; loss: 0.6932116746902466\n",
            "Iteration: 7217; loss: 0.6916415691375732\n",
            "Iteration: 7218; loss: 0.6939423084259033\n",
            "Iteration: 7219; loss: 0.6951205134391785\n",
            "Iteration: 7220; loss: 0.6941855549812317\n",
            "Iteration: 7221; loss: 0.6881227493286133\n",
            "Iteration: 7222; loss: 0.6891063451766968\n",
            "Iteration: 7223; loss: 0.6912294030189514\n",
            "Iteration: 7224; loss: 0.6931897401809692\n",
            "Iteration: 7225; loss: 0.6912130117416382\n",
            "Iteration: 7226; loss: 0.6934588551521301\n",
            "Iteration: 7227; loss: 0.6902410984039307\n",
            "Iteration: 7228; loss: 0.6950578093528748\n",
            "Iteration: 7229; loss: 0.6942883133888245\n",
            "Iteration: 7230; loss: 0.6936181783676147\n",
            "Iteration: 7231; loss: 0.6914759874343872\n",
            "Iteration: 7232; loss: 0.6920915842056274\n",
            "Iteration: 7233; loss: 0.6923172473907471\n",
            "Iteration: 7234; loss: 0.6950178146362305\n",
            "Iteration: 7235; loss: 0.694057822227478\n",
            "Iteration: 7236; loss: 0.6947251558303833\n",
            "Iteration: 7237; loss: 0.6944825649261475\n",
            "Iteration: 7238; loss: 0.6894574165344238\n",
            "Iteration: 7239; loss: 0.6968043446540833\n",
            "Iteration: 7240; loss: 0.6950478553771973\n",
            "Iteration: 7241; loss: 0.691231906414032\n",
            "Iteration: 7242; loss: 0.6931679248809814\n",
            "Iteration: 7243; loss: 0.6946828365325928\n",
            "Iteration: 7244; loss: 0.6885762214660645\n",
            "Iteration: 7245; loss: 0.6942655444145203\n",
            "Iteration: 7246; loss: 0.6912745237350464\n",
            "Iteration: 7247; loss: 0.6919329166412354\n",
            "Iteration: 7248; loss: 0.6899486780166626\n",
            "Iteration: 7249; loss: 0.698358952999115\n",
            "Iteration: 7250; loss: 0.69246906042099\n",
            "Iteration: 7251; loss: 0.6925076246261597\n",
            "Iteration: 7252; loss: 0.6939486265182495\n",
            "Iteration: 7253; loss: 0.6920011639595032\n",
            "Iteration: 7254; loss: 0.6883471012115479\n",
            "Iteration: 7255; loss: 0.6921800374984741\n",
            "Iteration: 7256; loss: 0.6914541721343994\n",
            "Iteration: 7257; loss: 0.6932085752487183\n",
            "Iteration: 7258; loss: 0.6979993581771851\n",
            "Iteration: 7259; loss: 0.6919870972633362\n",
            "Iteration: 7260; loss: 0.6965441107749939\n",
            "Iteration: 7261; loss: 0.6975348591804504\n",
            "Iteration: 7262; loss: 0.6908196210861206\n",
            "Iteration: 7263; loss: 0.6954565048217773\n",
            "Iteration: 7264; loss: 0.692168653011322\n",
            "Iteration: 7265; loss: 0.6955485939979553\n",
            "Iteration: 7266; loss: 0.6925249695777893\n",
            "Iteration: 7267; loss: 0.6941425204277039\n",
            "Iteration: 7268; loss: 0.6932202577590942\n",
            "Iteration: 7269; loss: 0.6938466429710388\n",
            "Iteration: 7270; loss: 0.6952324509620667\n",
            "Iteration: 7271; loss: 0.6926523447036743\n",
            "Iteration: 7272; loss: 0.6955124139785767\n",
            "Iteration: 7273; loss: 0.6978049278259277\n",
            "Iteration: 7274; loss: 0.6922708749771118\n",
            "Iteration: 7275; loss: 0.6942859888076782\n",
            "Iteration: 7276; loss: 0.6949084997177124\n",
            "Iteration: 7277; loss: 0.6921055912971497\n",
            "Iteration: 7278; loss: 0.6931717395782471\n",
            "Iteration: 7279; loss: 0.6925779581069946\n",
            "Iteration: 7280; loss: 0.6942702531814575\n",
            "Iteration: 7281; loss: 0.6899763345718384\n",
            "Iteration: 7282; loss: 0.693107545375824\n",
            "Iteration: 7283; loss: 0.6897976994514465\n",
            "Iteration: 7284; loss: 0.6959567070007324\n",
            "Iteration: 7285; loss: 0.6931225657463074\n",
            "Iteration: 7286; loss: 0.6949945092201233\n",
            "Iteration: 7287; loss: 0.6933053135871887\n",
            "Iteration: 7288; loss: 0.6976661682128906\n",
            "Iteration: 7289; loss: 0.6929962038993835\n",
            "Iteration: 7290; loss: 0.6929261684417725\n",
            "Iteration: 7291; loss: 0.6928548812866211\n",
            "Iteration: 7292; loss: 0.6934019327163696\n",
            "Iteration: 7293; loss: 0.6908367872238159\n",
            "Iteration: 7294; loss: 0.6919896006584167\n",
            "Iteration: 7295; loss: 0.6948570609092712\n",
            "Iteration: 7296; loss: 0.6913360953330994\n",
            "Iteration: 7297; loss: 0.6888197660446167\n",
            "Iteration: 7298; loss: 0.6919698715209961\n",
            "Iteration: 7299; loss: 0.692887008190155\n",
            "Iteration: 7300; loss: 0.6939093470573425\n",
            "Iteration: 7301; loss: 0.6932375431060791\n",
            "Iteration: 7302; loss: 0.6924636960029602\n",
            "Iteration: 7303; loss: 0.6924235224723816\n",
            "Iteration: 7304; loss: 0.6925154328346252\n",
            "Iteration: 7305; loss: 0.6926534175872803\n",
            "Iteration: 7306; loss: 0.6934900879859924\n",
            "Iteration: 7307; loss: 0.6911879181861877\n",
            "Iteration: 7308; loss: 0.6933834552764893\n",
            "Iteration: 7309; loss: 0.6929181218147278\n",
            "Iteration: 7310; loss: 0.6922453045845032\n",
            "Iteration: 7311; loss: 0.6910998225212097\n",
            "Iteration: 7312; loss: 0.692572832107544\n",
            "Iteration: 7313; loss: 0.6926683783531189\n",
            "Iteration: 7314; loss: 0.6898145079612732\n",
            "Iteration: 7315; loss: 0.6916364431381226\n",
            "Iteration: 7316; loss: 0.6967876553535461\n",
            "Iteration: 7317; loss: 0.691359281539917\n",
            "Iteration: 7318; loss: 0.6945023536682129\n",
            "Iteration: 7319; loss: 0.690234899520874\n",
            "Iteration: 7320; loss: 0.6976180076599121\n",
            "Iteration: 7321; loss: 0.692072331905365\n",
            "Iteration: 7322; loss: 0.6929897665977478\n",
            "Iteration: 7323; loss: 0.6944549083709717\n",
            "Iteration: 7324; loss: 0.6938571333885193\n",
            "Iteration: 7325; loss: 0.6953636407852173\n",
            "Iteration: 7326; loss: 0.6905215382575989\n",
            "Iteration: 7327; loss: 0.692959189414978\n",
            "Iteration: 7328; loss: 0.6931200623512268\n",
            "Iteration: 7329; loss: 0.6929845809936523\n",
            "Iteration: 7330; loss: 0.69388347864151\n",
            "Iteration: 7331; loss: 0.6955716609954834\n",
            "Iteration: 7332; loss: 0.6904932856559753\n",
            "Iteration: 7333; loss: 0.6913301944732666\n",
            "Iteration: 7334; loss: 0.6943035125732422\n",
            "Iteration: 7335; loss: 0.695086658000946\n",
            "Iteration: 7336; loss: 0.6950141191482544\n",
            "Iteration: 7337; loss: 0.6910431385040283\n",
            "Iteration: 7338; loss: 0.6954708695411682\n",
            "Iteration: 7339; loss: 0.6922287344932556\n",
            "Iteration: 7340; loss: 0.6908984184265137\n",
            "Iteration: 7341; loss: 0.6912353038787842\n",
            "Iteration: 7342; loss: 0.6929375529289246\n",
            "Iteration: 7343; loss: 0.6911292672157288\n",
            "Iteration: 7344; loss: 0.691929817199707\n",
            "Iteration: 7345; loss: 0.6942200660705566\n",
            "Iteration: 7346; loss: 0.6927095055580139\n",
            "Iteration: 7347; loss: 0.6942930221557617\n",
            "Iteration: 7348; loss: 0.6942423582077026\n",
            "Iteration: 7349; loss: 0.6909719705581665\n",
            "Iteration: 7350; loss: 0.6967748403549194\n",
            "Iteration: 7351; loss: 0.693347156047821\n",
            "Iteration: 7352; loss: 0.6918285489082336\n",
            "Iteration: 7353; loss: 0.6943325400352478\n",
            "Iteration: 7354; loss: 0.6940927505493164\n",
            "Iteration: 7355; loss: 0.6916568875312805\n",
            "Iteration: 7356; loss: 0.6937600374221802\n",
            "Iteration: 7357; loss: 0.6928454041481018\n",
            "Iteration: 7358; loss: 0.6940929889678955\n",
            "Iteration: 7359; loss: 0.6914299726486206\n",
            "Iteration: 7360; loss: 0.6907052993774414\n",
            "Iteration: 7361; loss: 0.6975287795066833\n",
            "Iteration: 7362; loss: 0.6924268007278442\n",
            "Iteration: 7363; loss: 0.6937721967697144\n",
            "Iteration: 7364; loss: 0.6925128698348999\n",
            "Iteration: 7365; loss: 0.69223952293396\n",
            "Iteration: 7366; loss: 0.6929895877838135\n",
            "Iteration: 7367; loss: 0.6934617161750793\n",
            "Iteration: 7368; loss: 0.6950187087059021\n",
            "Iteration: 7369; loss: 0.692428469657898\n",
            "Iteration: 7370; loss: 0.6911114454269409\n",
            "Iteration: 7371; loss: 0.6960605978965759\n",
            "Iteration: 7372; loss: 0.6946632266044617\n",
            "Iteration: 7373; loss: 0.6925327181816101\n",
            "Iteration: 7374; loss: 0.693360447883606\n",
            "Iteration: 7375; loss: 0.6948230266571045\n",
            "Iteration: 7376; loss: 0.6966436505317688\n",
            "Iteration: 7377; loss: 0.6914050579071045\n",
            "Iteration: 7378; loss: 0.6956203579902649\n",
            "Iteration: 7379; loss: 0.6929807662963867\n",
            "Iteration: 7380; loss: 0.6943170428276062\n",
            "Iteration: 7381; loss: 0.6933572292327881\n",
            "Iteration: 7382; loss: 0.6920261383056641\n",
            "Iteration: 7383; loss: 0.6922147870063782\n",
            "Iteration: 7384; loss: 0.6966572403907776\n",
            "Iteration: 7385; loss: 0.6923952102661133\n",
            "Iteration: 7386; loss: 0.692333996295929\n",
            "Iteration: 7387; loss: 0.6952695846557617\n",
            "Iteration: 7388; loss: 0.6941965818405151\n",
            "Iteration: 7389; loss: 0.6866894960403442\n",
            "Iteration: 7390; loss: 0.6942346692085266\n",
            "Iteration: 7391; loss: 0.6917046308517456\n",
            "Iteration: 7392; loss: 0.6924630403518677\n",
            "Iteration: 7393; loss: 0.6936621069908142\n",
            "Iteration: 7394; loss: 0.6917533278465271\n",
            "Iteration: 7395; loss: 0.6921308040618896\n",
            "Iteration: 7396; loss: 0.6927740573883057\n",
            "Iteration: 7397; loss: 0.6968375444412231\n",
            "Iteration: 7398; loss: 0.6964105367660522\n",
            "Iteration: 7399; loss: 0.6930195093154907\n",
            "Iteration: 7400; loss: 0.6913246512413025\n",
            "Iteration: 7401; loss: 0.6916415691375732\n",
            "Iteration: 7402; loss: 0.691561758518219\n",
            "Iteration: 7403; loss: 0.6937147378921509\n",
            "Iteration: 7404; loss: 0.6929590702056885\n",
            "Iteration: 7405; loss: 0.6894907355308533\n",
            "Iteration: 7406; loss: 0.6934198141098022\n",
            "Iteration: 7407; loss: 0.6921666264533997\n",
            "Iteration: 7408; loss: 0.6918323636054993\n",
            "Iteration: 7409; loss: 0.6904836893081665\n",
            "Iteration: 7410; loss: 0.6933284997940063\n",
            "Iteration: 7411; loss: 0.6958249807357788\n",
            "Iteration: 7412; loss: 0.6901323795318604\n",
            "Iteration: 7413; loss: 0.6889927983283997\n",
            "Iteration: 7414; loss: 0.6954714059829712\n",
            "Iteration: 7415; loss: 0.6961230039596558\n",
            "Iteration: 7416; loss: 0.6921937465667725\n",
            "Iteration: 7417; loss: 0.6903141140937805\n",
            "Iteration: 7418; loss: 0.6936243176460266\n",
            "Iteration: 7419; loss: 0.69450443983078\n",
            "Iteration: 7420; loss: 0.6942133903503418\n",
            "Iteration: 7421; loss: 0.6955165266990662\n",
            "Iteration: 7422; loss: 0.6925051808357239\n",
            "Iteration: 7423; loss: 0.6931002140045166\n",
            "Iteration: 7424; loss: 0.6912608742713928\n",
            "Iteration: 7425; loss: 0.6937566995620728\n",
            "Iteration: 7426; loss: 0.6960442066192627\n",
            "Iteration: 7427; loss: 0.6934312582015991\n",
            "Iteration: 7428; loss: 0.6965130567550659\n",
            "Iteration: 7429; loss: 0.6924752593040466\n",
            "Iteration: 7430; loss: 0.6915066838264465\n",
            "Iteration: 7431; loss: 0.6928331851959229\n",
            "Iteration: 7432; loss: 0.6923967599868774\n",
            "Iteration: 7433; loss: 0.6938534379005432\n",
            "Iteration: 7434; loss: 0.6934089064598083\n",
            "Iteration: 7435; loss: 0.6923953294754028\n",
            "Iteration: 7436; loss: 0.6924295425415039\n",
            "Iteration: 7437; loss: 0.6937790513038635\n",
            "Iteration: 7438; loss: 0.6919171810150146\n",
            "Iteration: 7439; loss: 0.6932361721992493\n",
            "Iteration: 7440; loss: 0.6928750276565552\n",
            "Iteration: 7441; loss: 0.6943421363830566\n",
            "Iteration: 7442; loss: 0.6943926215171814\n",
            "Iteration: 7443; loss: 0.6916707754135132\n",
            "Iteration: 7444; loss: 0.692625880241394\n",
            "Iteration: 7445; loss: 0.6911075711250305\n",
            "Iteration: 7446; loss: 0.6931228041648865\n",
            "Iteration: 7447; loss: 0.6920714378356934\n",
            "Iteration: 7448; loss: 0.6900835037231445\n",
            "Iteration: 7449; loss: 0.6937294602394104\n",
            "Iteration: 7450; loss: 0.6918401122093201\n",
            "Iteration: 7451; loss: 0.6930466890335083\n",
            "Iteration: 7452; loss: 0.6916218400001526\n",
            "Iteration: 7453; loss: 0.6932087540626526\n",
            "Iteration: 7454; loss: 0.6920841932296753\n",
            "Iteration: 7455; loss: 0.6937038898468018\n",
            "Iteration: 7456; loss: 0.6914989352226257\n",
            "Iteration: 7457; loss: 0.6927969455718994\n",
            "Iteration: 7458; loss: 0.6942000389099121\n",
            "Iteration: 7459; loss: 0.6978117823600769\n",
            "Iteration: 7460; loss: 0.6893619298934937\n",
            "Iteration: 7461; loss: 0.6930518746376038\n",
            "Iteration: 7462; loss: 0.691353976726532\n",
            "Iteration: 7463; loss: 0.6935805678367615\n",
            "Iteration: 7464; loss: 0.6952176094055176\n",
            "Iteration: 7465; loss: 0.69364994764328\n",
            "Iteration: 7466; loss: 0.6925732493400574\n",
            "Iteration: 7467; loss: 0.6944212317466736\n",
            "Iteration: 7468; loss: 0.6933357119560242\n",
            "Iteration: 7469; loss: 0.6921759843826294\n",
            "Iteration: 7470; loss: 0.6954789757728577\n",
            "Iteration: 7471; loss: 0.690624475479126\n",
            "Iteration: 7472; loss: 0.6902115345001221\n",
            "Iteration: 7473; loss: 0.6918275952339172\n",
            "Iteration: 7474; loss: 0.6913145184516907\n",
            "Iteration: 7475; loss: 0.6966553330421448\n",
            "Iteration: 7476; loss: 0.6918787360191345\n",
            "Iteration: 7477; loss: 0.6943551301956177\n",
            "Iteration: 7478; loss: 0.6940452456474304\n",
            "Iteration: 7479; loss: 0.6934095025062561\n",
            "Iteration: 7480; loss: 0.6949670910835266\n",
            "Iteration: 7481; loss: 0.692702054977417\n",
            "Iteration: 7482; loss: 0.6940634846687317\n",
            "Iteration: 7483; loss: 0.6905309557914734\n",
            "Iteration: 7484; loss: 0.692821741104126\n",
            "Iteration: 7485; loss: 0.6910420656204224\n",
            "Iteration: 7486; loss: 0.6930007934570312\n",
            "Iteration: 7487; loss: 0.6932805180549622\n",
            "Iteration: 7488; loss: 0.691862165927887\n",
            "Iteration: 7489; loss: 0.6924297213554382\n",
            "Iteration: 7490; loss: 0.6913414001464844\n",
            "Iteration: 7491; loss: 0.6909622550010681\n",
            "Iteration: 7492; loss: 0.6925321817398071\n",
            "Iteration: 7493; loss: 0.6961678266525269\n",
            "Iteration: 7494; loss: 0.6923825740814209\n",
            "Iteration: 7495; loss: 0.6963263750076294\n",
            "Iteration: 7496; loss: 0.6943327188491821\n",
            "Iteration: 7497; loss: 0.6927385330200195\n",
            "Iteration: 7498; loss: 0.6920362710952759\n",
            "Iteration: 7499; loss: 0.6929822564125061\n",
            "Iteration: 7500; loss: 0.693618893623352\n",
            "Iteration: 7501; loss: 0.6964674592018127\n",
            "Iteration: 7502; loss: 0.6941857933998108\n",
            "Iteration: 7503; loss: 0.6933646202087402\n",
            "Iteration: 7504; loss: 0.691594123840332\n",
            "Iteration: 7505; loss: 0.6951774954795837\n",
            "Iteration: 7506; loss: 0.6904131174087524\n",
            "Iteration: 7507; loss: 0.6926870346069336\n",
            "Iteration: 7508; loss: 0.6902450919151306\n",
            "Iteration: 7509; loss: 0.6917840242385864\n",
            "Iteration: 7510; loss: 0.6922597885131836\n",
            "Iteration: 7511; loss: 0.6939401626586914\n",
            "Iteration: 7512; loss: 0.6917836666107178\n",
            "Iteration: 7513; loss: 0.6942241787910461\n",
            "Iteration: 7514; loss: 0.6964139938354492\n",
            "Iteration: 7515; loss: 0.6943689584732056\n",
            "Iteration: 7516; loss: 0.69295334815979\n",
            "Iteration: 7517; loss: 0.6913719177246094\n",
            "Iteration: 7518; loss: 0.6905357837677002\n",
            "Iteration: 7519; loss: 0.6878843307495117\n",
            "Iteration: 7520; loss: 0.6938663721084595\n",
            "Iteration: 7521; loss: 0.6914368867874146\n",
            "Iteration: 7522; loss: 0.6909146904945374\n",
            "Iteration: 7523; loss: 0.6939746141433716\n",
            "Iteration: 7524; loss: 0.6918279528617859\n",
            "Iteration: 7525; loss: 0.6910502910614014\n",
            "Iteration: 7526; loss: 0.6881574392318726\n",
            "Iteration: 7527; loss: 0.6928480863571167\n",
            "Iteration: 7528; loss: 0.6930981278419495\n",
            "Iteration: 7529; loss: 0.6904805898666382\n",
            "Iteration: 7530; loss: 0.6972576975822449\n",
            "Iteration: 7531; loss: 0.6928737163543701\n",
            "Iteration: 7532; loss: 0.6921983957290649\n",
            "Iteration: 7533; loss: 0.6950639486312866\n",
            "Iteration: 7534; loss: 0.6943856477737427\n",
            "Iteration: 7535; loss: 0.6923487782478333\n",
            "Iteration: 7536; loss: 0.691490113735199\n",
            "Iteration: 7537; loss: 0.6929056644439697\n",
            "Iteration: 7538; loss: 0.6903186440467834\n",
            "Iteration: 7539; loss: 0.6946375370025635\n",
            "Iteration: 7540; loss: 0.6900126338005066\n",
            "Iteration: 7541; loss: 0.6915871500968933\n",
            "Iteration: 7542; loss: 0.6953657269477844\n",
            "Iteration: 7543; loss: 0.6929935812950134\n",
            "Iteration: 7544; loss: 0.6928041577339172\n",
            "Iteration: 7545; loss: 0.6949408054351807\n",
            "Iteration: 7546; loss: 0.6893923878669739\n",
            "Iteration: 7547; loss: 0.6915531158447266\n",
            "Iteration: 7548; loss: 0.6965534687042236\n",
            "Iteration: 7549; loss: 0.6909221410751343\n",
            "Iteration: 7550; loss: 0.6919546723365784\n",
            "Iteration: 7551; loss: 0.6890659332275391\n",
            "Iteration: 7552; loss: 0.6943911910057068\n",
            "Iteration: 7553; loss: 0.6927039623260498\n",
            "Iteration: 7554; loss: 0.6978497505187988\n",
            "Iteration: 7555; loss: 0.6916888356208801\n",
            "Iteration: 7556; loss: 0.6920230388641357\n",
            "Iteration: 7557; loss: 0.6941840648651123\n",
            "Iteration: 7558; loss: 0.6948452591896057\n",
            "Iteration: 7559; loss: 0.6954907774925232\n",
            "Iteration: 7560; loss: 0.6908668279647827\n",
            "Iteration: 7561; loss: 0.6956226229667664\n",
            "Iteration: 7562; loss: 0.6926273107528687\n",
            "Iteration: 7563; loss: 0.6916247010231018\n",
            "Iteration: 7564; loss: 0.6909391283988953\n",
            "Iteration: 7565; loss: 0.693364143371582\n",
            "Iteration: 7566; loss: 0.6943572759628296\n",
            "Iteration: 7567; loss: 0.6904686093330383\n",
            "Iteration: 7568; loss: 0.6921108961105347\n",
            "Iteration: 7569; loss: 0.6928731203079224\n",
            "Iteration: 7570; loss: 0.6934996843338013\n",
            "Iteration: 7571; loss: 0.6925147175788879\n",
            "Iteration: 7572; loss: 0.6931588649749756\n",
            "Iteration: 7573; loss: 0.6930358409881592\n",
            "Iteration: 7574; loss: 0.6935453414916992\n",
            "Iteration: 7575; loss: 0.689690887928009\n",
            "Iteration: 7576; loss: 0.6946711540222168\n",
            "Iteration: 7577; loss: 0.6933739185333252\n",
            "Iteration: 7578; loss: 0.6935973763465881\n",
            "Iteration: 7579; loss: 0.6919282674789429\n",
            "Iteration: 7580; loss: 0.6918925642967224\n",
            "Iteration: 7581; loss: 0.6922607421875\n",
            "Iteration: 7582; loss: 0.6936166882514954\n",
            "Iteration: 7583; loss: 0.6872857213020325\n",
            "Iteration: 7584; loss: 0.6932086944580078\n",
            "Iteration: 7585; loss: 0.6925264000892639\n",
            "Iteration: 7586; loss: 0.6877262592315674\n",
            "Iteration: 7587; loss: 0.6930945515632629\n",
            "Iteration: 7588; loss: 0.692606508731842\n",
            "Iteration: 7589; loss: 0.695469856262207\n",
            "Iteration: 7590; loss: 0.6953285932540894\n",
            "Iteration: 7591; loss: 0.6907156705856323\n",
            "Iteration: 7592; loss: 0.6952337622642517\n",
            "Iteration: 7593; loss: 0.6940193772315979\n",
            "Iteration: 7594; loss: 0.6969249248504639\n",
            "Iteration: 7595; loss: 0.6915010213851929\n",
            "Iteration: 7596; loss: 0.6968671083450317\n",
            "Iteration: 7597; loss: 0.6908583641052246\n",
            "Iteration: 7598; loss: 0.692129909992218\n",
            "Iteration: 7599; loss: 0.6913831830024719\n",
            "Iteration: 7600; loss: 0.69548100233078\n",
            "Iteration: 7601; loss: 0.6892616748809814\n",
            "Iteration: 7602; loss: 0.6966981887817383\n",
            "Iteration: 7603; loss: 0.6968722343444824\n",
            "Iteration: 7604; loss: 0.6897937655448914\n",
            "Iteration: 7605; loss: 0.6912255883216858\n",
            "Iteration: 7606; loss: 0.6945343017578125\n",
            "Iteration: 7607; loss: 0.6927062273025513\n",
            "Iteration: 7608; loss: 0.6924840211868286\n",
            "Iteration: 7609; loss: 0.6946395635604858\n",
            "Iteration: 7610; loss: 0.6943531632423401\n",
            "Iteration: 7611; loss: 0.6956844329833984\n",
            "Iteration: 7612; loss: 0.6891524791717529\n",
            "Iteration: 7613; loss: 0.69328773021698\n",
            "Iteration: 7614; loss: 0.6913619041442871\n",
            "Iteration: 7615; loss: 0.6915766596794128\n",
            "Iteration: 7616; loss: 0.6895164251327515\n",
            "Iteration: 7617; loss: 0.6916607618331909\n",
            "Iteration: 7618; loss: 0.6951791048049927\n",
            "Iteration: 7619; loss: 0.6925829648971558\n",
            "Iteration: 7620; loss: 0.6932255625724792\n",
            "Iteration: 7621; loss: 0.6910898685455322\n",
            "Iteration: 7622; loss: 0.6909202337265015\n",
            "Iteration: 7623; loss: 0.6920444965362549\n",
            "Iteration: 7624; loss: 0.6912305951118469\n",
            "Iteration: 7625; loss: 0.6935210227966309\n",
            "Iteration: 7626; loss: 0.6926190257072449\n",
            "Iteration: 7627; loss: 0.6927623748779297\n",
            "Iteration: 7628; loss: 0.6923093199729919\n",
            "Iteration: 7629; loss: 0.6921721696853638\n",
            "Iteration: 7630; loss: 0.6942704916000366\n",
            "Iteration: 7631; loss: 0.6890493631362915\n",
            "Iteration: 7632; loss: 0.6911669969558716\n",
            "Iteration: 7633; loss: 0.6952229738235474\n",
            "Iteration: 7634; loss: 0.6911394596099854\n",
            "Iteration: 7635; loss: 0.6907777190208435\n",
            "Iteration: 7636; loss: 0.6952148079872131\n",
            "Iteration: 7637; loss: 0.6876663565635681\n",
            "Iteration: 7638; loss: 0.6927379369735718\n",
            "Iteration: 7639; loss: 0.6937913298606873\n",
            "Iteration: 7640; loss: 0.6956663131713867\n",
            "Iteration: 7641; loss: 0.6928876638412476\n",
            "Iteration: 7642; loss: 0.6924906373023987\n",
            "Iteration: 7643; loss: 0.696221113204956\n",
            "Iteration: 7644; loss: 0.6932099461555481\n",
            "Iteration: 7645; loss: 0.6920572519302368\n",
            "Iteration: 7646; loss: 0.6878708004951477\n",
            "Iteration: 7647; loss: 0.6923750042915344\n",
            "Iteration: 7648; loss: 0.6937987208366394\n",
            "Iteration: 7649; loss: 0.6921501159667969\n",
            "Iteration: 7650; loss: 0.6974202394485474\n",
            "Iteration: 7651; loss: 0.6929150223731995\n",
            "Iteration: 7652; loss: 0.6928896903991699\n",
            "Iteration: 7653; loss: 0.6943467259407043\n",
            "Iteration: 7654; loss: 0.6936313509941101\n",
            "Iteration: 7655; loss: 0.6914876103401184\n",
            "Iteration: 7656; loss: 0.6961058378219604\n",
            "Iteration: 7657; loss: 0.6925283670425415\n",
            "Iteration: 7658; loss: 0.6951091885566711\n",
            "Iteration: 7659; loss: 0.6887352466583252\n",
            "Iteration: 7660; loss: 0.6928460597991943\n",
            "Iteration: 7661; loss: 0.6947046518325806\n",
            "Iteration: 7662; loss: 0.6913819909095764\n",
            "Iteration: 7663; loss: 0.690004289150238\n",
            "Iteration: 7664; loss: 0.6941831111907959\n",
            "Iteration: 7665; loss: 0.6896046996116638\n",
            "Iteration: 7666; loss: 0.6907471418380737\n",
            "Iteration: 7667; loss: 0.6911423206329346\n",
            "Iteration: 7668; loss: 0.6972259879112244\n",
            "Iteration: 7669; loss: 0.6911364793777466\n",
            "Iteration: 7670; loss: 0.6929069757461548\n",
            "Iteration: 7671; loss: 0.6893898844718933\n",
            "Iteration: 7672; loss: 0.6938047409057617\n",
            "Iteration: 7673; loss: 0.6910793781280518\n",
            "Iteration: 7674; loss: 0.69775390625\n",
            "Iteration: 7675; loss: 0.6888151168823242\n",
            "Iteration: 7676; loss: 0.6916787624359131\n",
            "Iteration: 7677; loss: 0.6920961141586304\n",
            "Iteration: 7678; loss: 0.7009909749031067\n",
            "Iteration: 7679; loss: 0.6876746416091919\n",
            "Iteration: 7680; loss: 0.6899728178977966\n",
            "Iteration: 7681; loss: 0.6931413412094116\n",
            "Iteration: 7682; loss: 0.6909478306770325\n",
            "Iteration: 7683; loss: 0.6917895674705505\n",
            "Iteration: 7684; loss: 0.6939215064048767\n",
            "Iteration: 7685; loss: 0.694230318069458\n",
            "Iteration: 7686; loss: 0.6909691095352173\n",
            "Iteration: 7687; loss: 0.6962900161743164\n",
            "Iteration: 7688; loss: 0.6935789585113525\n",
            "Iteration: 7689; loss: 0.6904241442680359\n",
            "Iteration: 7690; loss: 0.6978307366371155\n",
            "Iteration: 7691; loss: 0.6964999437332153\n",
            "Iteration: 7692; loss: 0.6917083263397217\n",
            "Iteration: 7693; loss: 0.6941543817520142\n",
            "Iteration: 7694; loss: 0.6945114731788635\n",
            "Iteration: 7695; loss: 0.6900220513343811\n",
            "Iteration: 7696; loss: 0.695985734462738\n",
            "Iteration: 7697; loss: 0.6940853595733643\n",
            "Iteration: 7698; loss: 0.6963499784469604\n",
            "Iteration: 7699; loss: 0.6911109685897827\n",
            "Iteration: 7700; loss: 0.6918656229972839\n",
            "Iteration: 7701; loss: 0.6922067999839783\n",
            "Iteration: 7702; loss: 0.6946948766708374\n",
            "Iteration: 7703; loss: 0.6919499635696411\n",
            "Iteration: 7704; loss: 0.6925954222679138\n",
            "Iteration: 7705; loss: 0.6977750062942505\n",
            "Iteration: 7706; loss: 0.6924188137054443\n",
            "Iteration: 7707; loss: 0.6920261383056641\n",
            "Iteration: 7708; loss: 0.6974484324455261\n",
            "Iteration: 7709; loss: 0.6944320797920227\n",
            "Iteration: 7710; loss: 0.6930171847343445\n",
            "Iteration: 7711; loss: 0.6934030055999756\n",
            "Iteration: 7712; loss: 0.6909165978431702\n",
            "Iteration: 7713; loss: 0.6937379837036133\n",
            "Iteration: 7714; loss: 0.6937505006790161\n",
            "Iteration: 7715; loss: 0.6914599537849426\n",
            "Iteration: 7716; loss: 0.6923869252204895\n",
            "Iteration: 7717; loss: 0.6909596920013428\n",
            "Iteration: 7718; loss: 0.6939152479171753\n",
            "Iteration: 7719; loss: 0.6940537691116333\n",
            "Iteration: 7720; loss: 0.6920119524002075\n",
            "Iteration: 7721; loss: 0.6954396367073059\n",
            "Iteration: 7722; loss: 0.6903955936431885\n",
            "Iteration: 7723; loss: 0.6982367038726807\n",
            "Iteration: 7724; loss: 0.6937509179115295\n",
            "Iteration: 7725; loss: 0.6942850351333618\n",
            "Iteration: 7726; loss: 0.6919460296630859\n",
            "Iteration: 7727; loss: 0.6929370760917664\n",
            "Iteration: 7728; loss: 0.6934863328933716\n",
            "Iteration: 7729; loss: 0.6959444284439087\n",
            "Iteration: 7730; loss: 0.6943849325180054\n",
            "Iteration: 7731; loss: 0.6934829950332642\n",
            "Iteration: 7732; loss: 0.6899600028991699\n",
            "Iteration: 7733; loss: 0.6916015148162842\n",
            "Iteration: 7734; loss: 0.6950101256370544\n",
            "Iteration: 7735; loss: 0.6928759813308716\n",
            "Iteration: 7736; loss: 0.6941096186637878\n",
            "Iteration: 7737; loss: 0.6927666664123535\n",
            "Iteration: 7738; loss: 0.6900637149810791\n",
            "Iteration: 7739; loss: 0.6934812664985657\n",
            "Iteration: 7740; loss: 0.6940149068832397\n",
            "Iteration: 7741; loss: 0.6916607022285461\n",
            "Iteration: 7742; loss: 0.6912782788276672\n",
            "Iteration: 7743; loss: 0.6944474577903748\n",
            "Iteration: 7744; loss: 0.6920566558837891\n",
            "Iteration: 7745; loss: 0.6945044994354248\n",
            "Iteration: 7746; loss: 0.692659854888916\n",
            "Iteration: 7747; loss: 0.6943928003311157\n",
            "Iteration: 7748; loss: 0.6875178813934326\n",
            "Iteration: 7749; loss: 0.6936158537864685\n",
            "Iteration: 7750; loss: 0.6933398842811584\n",
            "Iteration: 7751; loss: 0.6924452781677246\n",
            "Iteration: 7752; loss: 0.6947420239448547\n",
            "Iteration: 7753; loss: 0.6951404213905334\n",
            "Iteration: 7754; loss: 0.6986823081970215\n",
            "Iteration: 7755; loss: 0.6947268843650818\n",
            "Iteration: 7756; loss: 0.6921470165252686\n",
            "Iteration: 7757; loss: 0.6951493620872498\n",
            "Iteration: 7758; loss: 0.6929976940155029\n",
            "Iteration: 7759; loss: 0.6929649710655212\n",
            "Iteration: 7760; loss: 0.6925411224365234\n",
            "Iteration: 7761; loss: 0.6961606740951538\n",
            "Iteration: 7762; loss: 0.6927110552787781\n",
            "Iteration: 7763; loss: 0.6908589005470276\n",
            "Iteration: 7764; loss: 0.691042959690094\n",
            "Iteration: 7765; loss: 0.6908403038978577\n",
            "Iteration: 7766; loss: 0.6931050419807434\n",
            "Iteration: 7767; loss: 0.6947536468505859\n",
            "Iteration: 7768; loss: 0.6941171884536743\n",
            "Iteration: 7769; loss: 0.6914898753166199\n",
            "Iteration: 7770; loss: 0.6916103363037109\n",
            "Iteration: 7771; loss: 0.6943288445472717\n",
            "Iteration: 7772; loss: 0.6917741298675537\n",
            "Iteration: 7773; loss: 0.6934905648231506\n",
            "Iteration: 7774; loss: 0.690248966217041\n",
            "Iteration: 7775; loss: 0.6938959360122681\n",
            "Iteration: 7776; loss: 0.6948081254959106\n",
            "Iteration: 7777; loss: 0.6957980394363403\n",
            "Iteration: 7778; loss: 0.6911334991455078\n",
            "Iteration: 7779; loss: 0.6968640089035034\n",
            "Iteration: 7780; loss: 0.6914979219436646\n",
            "Iteration: 7781; loss: 0.6892344355583191\n",
            "Iteration: 7782; loss: 0.6936694979667664\n",
            "Iteration: 7783; loss: 0.6966831684112549\n",
            "Iteration: 7784; loss: 0.691771388053894\n",
            "Iteration: 7785; loss: 0.6943001747131348\n",
            "Iteration: 7786; loss: 0.6924235820770264\n",
            "Iteration: 7787; loss: 0.6931952834129333\n",
            "Iteration: 7788; loss: 0.6922718286514282\n",
            "Iteration: 7789; loss: 0.6905829906463623\n",
            "Iteration: 7790; loss: 0.6894724369049072\n",
            "Iteration: 7791; loss: 0.6913548111915588\n",
            "Iteration: 7792; loss: 0.6914499998092651\n",
            "Iteration: 7793; loss: 0.69108647108078\n",
            "Iteration: 7794; loss: 0.6892585754394531\n",
            "Iteration: 7795; loss: 0.6948831081390381\n",
            "Iteration: 7796; loss: 0.691358208656311\n",
            "Iteration: 7797; loss: 0.6921007037162781\n",
            "Iteration: 7798; loss: 0.6921272873878479\n",
            "Iteration: 7799; loss: 0.6921131014823914\n",
            "Iteration: 7800; loss: 0.6955059170722961\n",
            "Iteration: 7801; loss: 0.6900487542152405\n",
            "Iteration: 7802; loss: 0.6941134333610535\n",
            "Iteration: 7803; loss: 0.6916416883468628\n",
            "Iteration: 7804; loss: 0.6946090459823608\n",
            "Iteration: 7805; loss: 0.6956916451454163\n",
            "Iteration: 7806; loss: 0.6927481889724731\n",
            "Iteration: 7807; loss: 0.694166898727417\n",
            "Iteration: 7808; loss: 0.6923710107803345\n",
            "Iteration: 7809; loss: 0.6949186325073242\n",
            "Iteration: 7810; loss: 0.6949527859687805\n",
            "Iteration: 7811; loss: 0.6936891078948975\n",
            "Iteration: 7812; loss: 0.6954402327537537\n",
            "Iteration: 7813; loss: 0.6932055354118347\n",
            "Iteration: 7814; loss: 0.6922187805175781\n",
            "Iteration: 7815; loss: 0.6907871961593628\n",
            "Iteration: 7816; loss: 0.6900813579559326\n",
            "Iteration: 7817; loss: 0.6933753490447998\n",
            "Iteration: 7818; loss: 0.6909874677658081\n",
            "Iteration: 7819; loss: 0.6940685510635376\n",
            "Iteration: 7820; loss: 0.6949625611305237\n",
            "Iteration: 7821; loss: 0.6967806220054626\n",
            "Iteration: 7822; loss: 0.694979727268219\n",
            "Iteration: 7823; loss: 0.6924961805343628\n",
            "Iteration: 7824; loss: 0.6929641366004944\n",
            "Iteration: 7825; loss: 0.6883929967880249\n",
            "Iteration: 7826; loss: 0.690289318561554\n",
            "Iteration: 7827; loss: 0.69062739610672\n",
            "Iteration: 7828; loss: 0.6965373754501343\n",
            "Iteration: 7829; loss: 0.6936061382293701\n",
            "Iteration: 7830; loss: 0.6928039193153381\n",
            "Iteration: 7831; loss: 0.6935837864875793\n",
            "Iteration: 7832; loss: 0.6933908462524414\n",
            "Iteration: 7833; loss: 0.6912810802459717\n",
            "Iteration: 7834; loss: 0.6933013796806335\n",
            "Iteration: 7835; loss: 0.6899962425231934\n",
            "Iteration: 7836; loss: 0.6948956251144409\n",
            "Iteration: 7837; loss: 0.6912367939949036\n",
            "Iteration: 7838; loss: 0.6907899379730225\n",
            "Iteration: 7839; loss: 0.6926565170288086\n",
            "Iteration: 7840; loss: 0.6964667439460754\n",
            "Iteration: 7841; loss: 0.6963176727294922\n",
            "Iteration: 7842; loss: 0.6923694610595703\n",
            "Iteration: 7843; loss: 0.6937283277511597\n",
            "Iteration: 7844; loss: 0.6934787034988403\n",
            "Iteration: 7845; loss: 0.6963159441947937\n",
            "Iteration: 7846; loss: 0.6950089931488037\n",
            "Iteration: 7847; loss: 0.6943215727806091\n",
            "Iteration: 7848; loss: 0.6931162476539612\n",
            "Iteration: 7849; loss: 0.6918748021125793\n",
            "Iteration: 7850; loss: 0.6918613910675049\n",
            "Iteration: 7851; loss: 0.6948363780975342\n",
            "Iteration: 7852; loss: 0.6951600313186646\n",
            "Iteration: 7853; loss: 0.693321168422699\n",
            "Iteration: 7854; loss: 0.6909435987472534\n",
            "Iteration: 7855; loss: 0.6915268301963806\n",
            "Iteration: 7856; loss: 0.6966230273246765\n",
            "Iteration: 7857; loss: 0.6902928352355957\n",
            "Iteration: 7858; loss: 0.6917291879653931\n",
            "Iteration: 7859; loss: 0.6955074071884155\n",
            "Iteration: 7860; loss: 0.6958833932876587\n",
            "Iteration: 7861; loss: 0.6924471855163574\n",
            "Iteration: 7862; loss: 0.6927051544189453\n",
            "Iteration: 7863; loss: 0.6915434002876282\n",
            "Iteration: 7864; loss: 0.691521942615509\n",
            "Iteration: 7865; loss: 0.6903067827224731\n",
            "Iteration: 7866; loss: 0.6960277557373047\n",
            "Iteration: 7867; loss: 0.6935409307479858\n",
            "Iteration: 7868; loss: 0.69444340467453\n",
            "Iteration: 7869; loss: 0.6935682892799377\n",
            "Iteration: 7870; loss: 0.6948244571685791\n",
            "Iteration: 7871; loss: 0.6938754916191101\n",
            "Iteration: 7872; loss: 0.694605827331543\n",
            "Iteration: 7873; loss: 0.693377673625946\n",
            "Iteration: 7874; loss: 0.6929870843887329\n",
            "Iteration: 7875; loss: 0.697137176990509\n",
            "Iteration: 7876; loss: 0.688413679599762\n",
            "Iteration: 7877; loss: 0.6948307752609253\n",
            "Iteration: 7878; loss: 0.6933608055114746\n",
            "Iteration: 7879; loss: 0.6905915141105652\n",
            "Iteration: 7880; loss: 0.6917126774787903\n",
            "Iteration: 7881; loss: 0.6899154186248779\n",
            "Iteration: 7882; loss: 0.6909363865852356\n",
            "Iteration: 7883; loss: 0.6952584981918335\n",
            "Iteration: 7884; loss: 0.6933004856109619\n",
            "Iteration: 7885; loss: 0.691357433795929\n",
            "Iteration: 7886; loss: 0.692689836025238\n",
            "Iteration: 7887; loss: 0.6945904493331909\n",
            "Iteration: 7888; loss: 0.6950746178627014\n",
            "Iteration: 7889; loss: 0.6890662908554077\n",
            "Iteration: 7890; loss: 0.6914563775062561\n",
            "Iteration: 7891; loss: 0.6960514783859253\n",
            "Iteration: 7892; loss: 0.6945364475250244\n",
            "Iteration: 7893; loss: 0.6911219358444214\n",
            "Iteration: 7894; loss: 0.6961921453475952\n",
            "Iteration: 7895; loss: 0.6911892890930176\n",
            "Iteration: 7896; loss: 0.6952211856842041\n",
            "Iteration: 7897; loss: 0.6902821063995361\n",
            "Iteration: 7898; loss: 0.690119743347168\n",
            "Iteration: 7899; loss: 0.6944971084594727\n",
            "Iteration: 7900; loss: 0.6903781890869141\n",
            "Iteration: 7901; loss: 0.6909158825874329\n",
            "Iteration: 7902; loss: 0.6907070279121399\n",
            "Iteration: 7903; loss: 0.6917726397514343\n",
            "Iteration: 7904; loss: 0.6922039985656738\n",
            "Iteration: 7905; loss: 0.6925138235092163\n",
            "Iteration: 7906; loss: 0.6920969486236572\n",
            "Iteration: 7907; loss: 0.6906487941741943\n",
            "Iteration: 7908; loss: 0.6967134475708008\n",
            "Iteration: 7909; loss: 0.6941720843315125\n",
            "Iteration: 7910; loss: 0.6921083927154541\n",
            "Iteration: 7911; loss: 0.6935902833938599\n",
            "Iteration: 7912; loss: 0.6930110454559326\n",
            "Iteration: 7913; loss: 0.6932714581489563\n",
            "Iteration: 7914; loss: 0.6893975138664246\n",
            "Iteration: 7915; loss: 0.6929309368133545\n",
            "Iteration: 7916; loss: 0.6926354765892029\n",
            "Iteration: 7917; loss: 0.6949593424797058\n",
            "Iteration: 7918; loss: 0.6917495131492615\n",
            "Iteration: 7919; loss: 0.6929324269294739\n",
            "Iteration: 7920; loss: 0.6892051100730896\n",
            "Iteration: 7921; loss: 0.6916702389717102\n",
            "Iteration: 7922; loss: 0.6928771734237671\n",
            "Iteration: 7923; loss: 0.6966766119003296\n",
            "Iteration: 7924; loss: 0.6941457986831665\n",
            "Iteration: 7925; loss: 0.694068193435669\n",
            "Iteration: 7926; loss: 0.6916882395744324\n",
            "Iteration: 7927; loss: 0.6928528547286987\n",
            "Iteration: 7928; loss: 0.69517982006073\n",
            "Iteration: 7929; loss: 0.6911659240722656\n",
            "Iteration: 7930; loss: 0.6915570497512817\n",
            "Iteration: 7931; loss: 0.6940075159072876\n",
            "Iteration: 7932; loss: 0.6949038505554199\n",
            "Iteration: 7933; loss: 0.6955444812774658\n",
            "Iteration: 7934; loss: 0.6909104585647583\n",
            "Iteration: 7935; loss: 0.6902190446853638\n",
            "Iteration: 7936; loss: 0.6953981518745422\n",
            "Iteration: 7937; loss: 0.6946839690208435\n",
            "Iteration: 7938; loss: 0.697057843208313\n",
            "Iteration: 7939; loss: 0.6898316144943237\n",
            "Iteration: 7940; loss: 0.6957555413246155\n",
            "Iteration: 7941; loss: 0.6946353316307068\n",
            "Iteration: 7942; loss: 0.6923577785491943\n",
            "Iteration: 7943; loss: 0.6960347890853882\n",
            "Iteration: 7944; loss: 0.6950126886367798\n",
            "Iteration: 7945; loss: 0.6897826194763184\n",
            "Iteration: 7946; loss: 0.695300281047821\n",
            "Iteration: 7947; loss: 0.6916807293891907\n",
            "Iteration: 7948; loss: 0.6911552548408508\n",
            "Iteration: 7949; loss: 0.6934105157852173\n",
            "Iteration: 7950; loss: 0.692087709903717\n",
            "Iteration: 7951; loss: 0.6894941329956055\n",
            "Iteration: 7952; loss: 0.6887184977531433\n",
            "Iteration: 7953; loss: 0.6958695650100708\n",
            "Iteration: 7954; loss: 0.6945564150810242\n",
            "Iteration: 7955; loss: 0.6891365647315979\n",
            "Iteration: 7956; loss: 0.6931927800178528\n",
            "Iteration: 7957; loss: 0.691814661026001\n",
            "Iteration: 7958; loss: 0.6924794912338257\n",
            "Iteration: 7959; loss: 0.6879693269729614\n",
            "Iteration: 7960; loss: 0.6932151317596436\n",
            "Iteration: 7961; loss: 0.696022629737854\n",
            "Iteration: 7962; loss: 0.6979791522026062\n",
            "Iteration: 7963; loss: 0.6920867562294006\n",
            "Iteration: 7964; loss: 0.695142924785614\n",
            "Iteration: 7965; loss: 0.694175660610199\n",
            "Iteration: 7966; loss: 0.6881434917449951\n",
            "Iteration: 7967; loss: 0.6917584538459778\n",
            "Iteration: 7968; loss: 0.6900112628936768\n",
            "Iteration: 7969; loss: 0.6956268548965454\n",
            "Iteration: 7970; loss: 0.6927893161773682\n",
            "Iteration: 7971; loss: 0.6887959241867065\n",
            "Iteration: 7972; loss: 0.6902754902839661\n",
            "Iteration: 7973; loss: 0.6915660500526428\n",
            "Iteration: 7974; loss: 0.6930031776428223\n",
            "Iteration: 7975; loss: 0.6890712976455688\n",
            "Iteration: 7976; loss: 0.6915966272354126\n",
            "Iteration: 7977; loss: 0.6935593485832214\n",
            "Iteration: 7978; loss: 0.6902621984481812\n",
            "Iteration: 7979; loss: 0.6975897550582886\n",
            "Iteration: 7980; loss: 0.6901676654815674\n",
            "Iteration: 7981; loss: 0.70130455493927\n",
            "Iteration: 7982; loss: 0.6884490251541138\n",
            "Iteration: 7983; loss: 0.6951326727867126\n",
            "Iteration: 7984; loss: 0.6919052004814148\n",
            "Iteration: 7985; loss: 0.6920052170753479\n",
            "Iteration: 7986; loss: 0.6929532885551453\n",
            "Iteration: 7987; loss: 0.6909894347190857\n",
            "Iteration: 7988; loss: 0.6945751905441284\n",
            "Iteration: 7989; loss: 0.6973722577095032\n",
            "Iteration: 7990; loss: 0.6885607838630676\n",
            "Iteration: 7991; loss: 0.6903400421142578\n",
            "Iteration: 7992; loss: 0.6906552314758301\n",
            "Iteration: 7993; loss: 0.6927938461303711\n",
            "Iteration: 7994; loss: 0.6941046118736267\n",
            "Iteration: 7995; loss: 0.6927165985107422\n",
            "Iteration: 7996; loss: 0.6908495426177979\n",
            "Iteration: 7997; loss: 0.694128692150116\n",
            "Iteration: 7998; loss: 0.6932294368743896\n",
            "Iteration: 7999; loss: 0.6920716762542725\n",
            "Iteration: 8000; loss: 0.6934189200401306\n",
            "Iteration: 8001; loss: 0.6921958327293396\n",
            "Iteration: 8002; loss: 0.6936658024787903\n",
            "Iteration: 8003; loss: 0.6911776065826416\n",
            "Iteration: 8004; loss: 0.6883905529975891\n",
            "Iteration: 8005; loss: 0.698895275592804\n",
            "Iteration: 8006; loss: 0.6943780779838562\n",
            "Iteration: 8007; loss: 0.6900867223739624\n",
            "Iteration: 8008; loss: 0.6993464827537537\n",
            "Iteration: 8009; loss: 0.6931834816932678\n",
            "Iteration: 8010; loss: 0.6983948945999146\n",
            "Iteration: 8011; loss: 0.693153977394104\n",
            "Iteration: 8012; loss: 0.6914114356040955\n",
            "Iteration: 8013; loss: 0.6914938688278198\n",
            "Iteration: 8014; loss: 0.6956854462623596\n",
            "Iteration: 8015; loss: 0.6946716904640198\n",
            "Iteration: 8016; loss: 0.6936355829238892\n",
            "Iteration: 8017; loss: 0.6947175860404968\n",
            "Iteration: 8018; loss: 0.6916529536247253\n",
            "Iteration: 8019; loss: 0.6965042352676392\n",
            "Iteration: 8020; loss: 0.691337525844574\n",
            "Iteration: 8021; loss: 0.6955229043960571\n",
            "Iteration: 8022; loss: 0.692566454410553\n",
            "Iteration: 8023; loss: 0.6921469569206238\n",
            "Iteration: 8024; loss: 0.6906088590621948\n",
            "Iteration: 8025; loss: 0.6929608583450317\n",
            "Iteration: 8026; loss: 0.6927858591079712\n",
            "Iteration: 8027; loss: 0.6934492588043213\n",
            "Iteration: 8028; loss: 0.6936098337173462\n",
            "Iteration: 8029; loss: 0.6911507248878479\n",
            "Iteration: 8030; loss: 0.6954087615013123\n",
            "Iteration: 8031; loss: 0.69069904088974\n",
            "Iteration: 8032; loss: 0.6906211376190186\n",
            "Iteration: 8033; loss: 0.6927869915962219\n",
            "Iteration: 8034; loss: 0.6926965713500977\n",
            "Iteration: 8035; loss: 0.6923936605453491\n",
            "Iteration: 8036; loss: 0.6920052766799927\n",
            "Iteration: 8037; loss: 0.6904972791671753\n",
            "Iteration: 8038; loss: 0.6937034726142883\n",
            "Iteration: 8039; loss: 0.6945089101791382\n",
            "Iteration: 8040; loss: 0.69608473777771\n",
            "Iteration: 8041; loss: 0.6966449618339539\n",
            "Iteration: 8042; loss: 0.6912930011749268\n",
            "Iteration: 8043; loss: 0.6934718489646912\n",
            "Iteration: 8044; loss: 0.6920294165611267\n",
            "Iteration: 8045; loss: 0.6974655985832214\n",
            "Iteration: 8046; loss: 0.6954282522201538\n",
            "Iteration: 8047; loss: 0.6900283098220825\n",
            "Iteration: 8048; loss: 0.6903618574142456\n",
            "Iteration: 8049; loss: 0.6916380524635315\n",
            "Iteration: 8050; loss: 0.6942735314369202\n",
            "Iteration: 8051; loss: 0.6916194558143616\n",
            "Iteration: 8052; loss: 0.6936470866203308\n",
            "Iteration: 8053; loss: 0.6947808861732483\n",
            "Iteration: 8054; loss: 0.6917564868927002\n",
            "Iteration: 8055; loss: 0.6963202357292175\n",
            "Iteration: 8056; loss: 0.688457190990448\n",
            "Iteration: 8057; loss: 0.6928555965423584\n",
            "Iteration: 8058; loss: 0.6957167387008667\n",
            "Iteration: 8059; loss: 0.6983268857002258\n",
            "Iteration: 8060; loss: 0.6898842453956604\n",
            "Iteration: 8061; loss: 0.6927077174186707\n",
            "Iteration: 8062; loss: 0.6926005482673645\n",
            "Iteration: 8063; loss: 0.6936669945716858\n",
            "Iteration: 8064; loss: 0.6882328391075134\n",
            "Iteration: 8065; loss: 0.6971627473831177\n",
            "Iteration: 8066; loss: 0.6954472064971924\n",
            "Iteration: 8067; loss: 0.6914094090461731\n",
            "Iteration: 8068; loss: 0.6897623538970947\n",
            "Iteration: 8069; loss: 0.6910429000854492\n",
            "Iteration: 8070; loss: 0.6900315284729004\n",
            "Iteration: 8071; loss: 0.6948864459991455\n",
            "Iteration: 8072; loss: 0.6864610910415649\n",
            "Iteration: 8073; loss: 0.6921089291572571\n",
            "Iteration: 8074; loss: 0.6913647651672363\n",
            "Iteration: 8075; loss: 0.6919969320297241\n",
            "Iteration: 8076; loss: 0.6950476765632629\n",
            "Iteration: 8077; loss: 0.6950467228889465\n",
            "Iteration: 8078; loss: 0.6899296045303345\n",
            "Iteration: 8079; loss: 0.696104109287262\n",
            "Iteration: 8080; loss: 0.691177248954773\n",
            "Iteration: 8081; loss: 0.6919974088668823\n",
            "Iteration: 8082; loss: 0.6909493207931519\n",
            "Iteration: 8083; loss: 0.6966508626937866\n",
            "Iteration: 8084; loss: 0.6920884251594543\n",
            "Iteration: 8085; loss: 0.6972357630729675\n",
            "Iteration: 8086; loss: 0.6913496851921082\n",
            "Iteration: 8087; loss: 0.6918562650680542\n",
            "Iteration: 8088; loss: 0.6883822083473206\n",
            "Iteration: 8089; loss: 0.6941196918487549\n",
            "Iteration: 8090; loss: 0.6922891736030579\n",
            "Iteration: 8091; loss: 0.6935214996337891\n",
            "Iteration: 8092; loss: 0.6901368498802185\n",
            "Iteration: 8093; loss: 0.6931768655776978\n",
            "Iteration: 8094; loss: 0.6911246180534363\n",
            "Iteration: 8095; loss: 0.6925921440124512\n",
            "Iteration: 8096; loss: 0.6911543011665344\n",
            "Iteration: 8097; loss: 0.6932578682899475\n",
            "Iteration: 8098; loss: 0.6923810243606567\n",
            "Iteration: 8099; loss: 0.6937971711158752\n",
            "Iteration: 8100; loss: 0.7011953592300415\n",
            "Iteration: 8101; loss: 0.6930584907531738\n",
            "Iteration: 8102; loss: 0.6929404139518738\n",
            "Iteration: 8103; loss: 0.6958630084991455\n",
            "Iteration: 8104; loss: 0.6949470639228821\n",
            "Iteration: 8105; loss: 0.70081627368927\n",
            "Iteration: 8106; loss: 0.6919447779655457\n",
            "Iteration: 8107; loss: 0.6925135254859924\n",
            "Iteration: 8108; loss: 0.6894319653511047\n",
            "Iteration: 8109; loss: 0.693893551826477\n",
            "Iteration: 8110; loss: 0.6954293847084045\n",
            "Iteration: 8111; loss: 0.6945127844810486\n",
            "Iteration: 8112; loss: 0.6946240663528442\n",
            "Iteration: 8113; loss: 0.6939263939857483\n",
            "Iteration: 8114; loss: 0.6912661194801331\n",
            "Iteration: 8115; loss: 0.6919844150543213\n",
            "Iteration: 8116; loss: 0.6977688074111938\n",
            "Iteration: 8117; loss: 0.6915863752365112\n",
            "Iteration: 8118; loss: 0.6945416331291199\n",
            "Iteration: 8119; loss: 0.6946307420730591\n",
            "Iteration: 8120; loss: 0.6927779912948608\n",
            "Iteration: 8121; loss: 0.6906278133392334\n",
            "Iteration: 8122; loss: 0.6874319911003113\n",
            "Iteration: 8123; loss: 0.693703830242157\n",
            "Iteration: 8124; loss: 0.6949121952056885\n",
            "Iteration: 8125; loss: 0.6938601136207581\n",
            "Iteration: 8126; loss: 0.6951262950897217\n",
            "Iteration: 8127; loss: 0.693442702293396\n",
            "Iteration: 8128; loss: 0.6913233995437622\n",
            "Iteration: 8129; loss: 0.692119836807251\n",
            "Iteration: 8130; loss: 0.6885520219802856\n",
            "Iteration: 8131; loss: 0.6931443810462952\n",
            "Iteration: 8132; loss: 0.6926352977752686\n",
            "Iteration: 8133; loss: 0.6933040022850037\n",
            "Iteration: 8134; loss: 0.6887613534927368\n",
            "Iteration: 8135; loss: 0.6936823129653931\n",
            "Iteration: 8136; loss: 0.692061185836792\n",
            "Iteration: 8137; loss: 0.6932066679000854\n",
            "Iteration: 8138; loss: 0.6926891803741455\n",
            "Iteration: 8139; loss: 0.689124584197998\n",
            "Iteration: 8140; loss: 0.6930879950523376\n",
            "Iteration: 8141; loss: 0.6938690543174744\n",
            "Iteration: 8142; loss: 0.6919497847557068\n",
            "Iteration: 8143; loss: 0.6945146322250366\n",
            "Iteration: 8144; loss: 0.695499062538147\n",
            "Iteration: 8145; loss: 0.6899031400680542\n",
            "Iteration: 8146; loss: 0.6963047981262207\n",
            "Iteration: 8147; loss: 0.6900328397750854\n",
            "Iteration: 8148; loss: 0.6958301067352295\n",
            "Iteration: 8149; loss: 0.6909866333007812\n",
            "Iteration: 8150; loss: 0.6922426223754883\n",
            "Iteration: 8151; loss: 0.6895059943199158\n",
            "Iteration: 8152; loss: 0.6907428503036499\n",
            "Iteration: 8153; loss: 0.6923163533210754\n",
            "Iteration: 8154; loss: 0.6949126124382019\n",
            "Iteration: 8155; loss: 0.6991040706634521\n",
            "Iteration: 8156; loss: 0.6901426911354065\n",
            "Iteration: 8157; loss: 0.691400945186615\n",
            "Iteration: 8158; loss: 0.6908809542655945\n",
            "Iteration: 8159; loss: 0.693064272403717\n",
            "Iteration: 8160; loss: 0.6934422254562378\n",
            "Iteration: 8161; loss: 0.6944646835327148\n",
            "Iteration: 8162; loss: 0.6932568550109863\n",
            "Iteration: 8163; loss: 0.6959408521652222\n",
            "Iteration: 8164; loss: 0.6888772249221802\n",
            "Iteration: 8165; loss: 0.6927476525306702\n",
            "Iteration: 8166; loss: 0.6938605904579163\n",
            "Iteration: 8167; loss: 0.6926890015602112\n",
            "Iteration: 8168; loss: 0.689860999584198\n",
            "Iteration: 8169; loss: 0.693984866142273\n",
            "Iteration: 8170; loss: 0.694694459438324\n",
            "Iteration: 8171; loss: 0.6923849582672119\n",
            "Iteration: 8172; loss: 0.688915491104126\n",
            "Iteration: 8173; loss: 0.6934084296226501\n",
            "Iteration: 8174; loss: 0.6925457715988159\n",
            "Iteration: 8175; loss: 0.692293107509613\n",
            "Iteration: 8176; loss: 0.6928699612617493\n",
            "Iteration: 8177; loss: 0.6898626089096069\n",
            "Iteration: 8178; loss: 0.6916201114654541\n",
            "Iteration: 8179; loss: 0.6913237571716309\n",
            "Iteration: 8180; loss: 0.689738929271698\n",
            "Iteration: 8181; loss: 0.6906503438949585\n",
            "Iteration: 8182; loss: 0.6889631748199463\n",
            "Iteration: 8183; loss: 0.6924707889556885\n",
            "Iteration: 8184; loss: 0.6937592029571533\n",
            "Iteration: 8185; loss: 0.6942314505577087\n",
            "Iteration: 8186; loss: 0.6929327845573425\n",
            "Iteration: 8187; loss: 0.6943603157997131\n",
            "Iteration: 8188; loss: 0.6919010877609253\n",
            "Iteration: 8189; loss: 0.6933774948120117\n",
            "Iteration: 8190; loss: 0.6891985535621643\n",
            "Iteration: 8191; loss: 0.69334477186203\n",
            "Iteration: 8192; loss: 0.6951022744178772\n",
            "Iteration: 8193; loss: 0.6927646994590759\n",
            "Iteration: 8194; loss: 0.6960245370864868\n",
            "Iteration: 8195; loss: 0.7006872892379761\n",
            "Iteration: 8196; loss: 0.6921207904815674\n",
            "Iteration: 8197; loss: 0.6914823055267334\n",
            "Iteration: 8198; loss: 0.6943250894546509\n",
            "Iteration: 8199; loss: 0.6941956877708435\n",
            "Iteration: 8200; loss: 0.690479040145874\n",
            "Iteration: 8201; loss: 0.6911314725875854\n",
            "Iteration: 8202; loss: 0.6896427273750305\n",
            "Iteration: 8203; loss: 0.6917840838432312\n",
            "Iteration: 8204; loss: 0.6943979263305664\n",
            "Iteration: 8205; loss: 0.695960283279419\n",
            "Iteration: 8206; loss: 0.6934643983840942\n",
            "Iteration: 8207; loss: 0.6957783699035645\n",
            "Iteration: 8208; loss: 0.6918858289718628\n",
            "Iteration: 8209; loss: 0.6903834342956543\n",
            "Iteration: 8210; loss: 0.69359290599823\n",
            "Iteration: 8211; loss: 0.6981902122497559\n",
            "Iteration: 8212; loss: 0.6938363313674927\n",
            "Iteration: 8213; loss: 0.6923301815986633\n",
            "Iteration: 8214; loss: 0.6918096542358398\n",
            "Iteration: 8215; loss: 0.6950609683990479\n",
            "Iteration: 8216; loss: 0.69050133228302\n",
            "Iteration: 8217; loss: 0.6926791071891785\n",
            "Iteration: 8218; loss: 0.690650999546051\n",
            "Iteration: 8219; loss: 0.6931651830673218\n",
            "Iteration: 8220; loss: 0.6944186091423035\n",
            "Iteration: 8221; loss: 0.6911672353744507\n",
            "Iteration: 8222; loss: 0.6970717310905457\n",
            "Iteration: 8223; loss: 0.6901144981384277\n",
            "Iteration: 8224; loss: 0.6923908591270447\n",
            "Iteration: 8225; loss: 0.694510817527771\n",
            "Iteration: 8226; loss: 0.6936468482017517\n",
            "Iteration: 8227; loss: 0.6960921287536621\n",
            "Iteration: 8228; loss: 0.689781904220581\n",
            "Iteration: 8229; loss: 0.6904065012931824\n",
            "Iteration: 8230; loss: 0.6917207837104797\n",
            "Iteration: 8231; loss: 0.6926616430282593\n",
            "Iteration: 8232; loss: 0.6883625388145447\n",
            "Iteration: 8233; loss: 0.6920918822288513\n",
            "Iteration: 8234; loss: 0.6903309226036072\n",
            "Iteration: 8235; loss: 0.6952866911888123\n",
            "Iteration: 8236; loss: 0.6954689025878906\n",
            "Iteration: 8237; loss: 0.693770170211792\n",
            "Iteration: 8238; loss: 0.6899880170822144\n",
            "Iteration: 8239; loss: 0.6929585933685303\n",
            "Iteration: 8240; loss: 0.6935469508171082\n",
            "Iteration: 8241; loss: 0.6907941102981567\n",
            "Iteration: 8242; loss: 0.6986610293388367\n",
            "Iteration: 8243; loss: 0.6922171711921692\n",
            "Iteration: 8244; loss: 0.69402015209198\n",
            "Iteration: 8245; loss: 0.6931372284889221\n",
            "Iteration: 8246; loss: 0.6893583536148071\n",
            "Iteration: 8247; loss: 0.6931509375572205\n",
            "Iteration: 8248; loss: 0.6899803876876831\n",
            "Iteration: 8249; loss: 0.6907674074172974\n",
            "Iteration: 8250; loss: 0.692767322063446\n",
            "Iteration: 8251; loss: 0.6934372782707214\n",
            "Iteration: 8252; loss: 0.6957380771636963\n",
            "Iteration: 8253; loss: 0.6966298818588257\n",
            "Iteration: 8254; loss: 0.6925620436668396\n",
            "Iteration: 8255; loss: 0.6936559081077576\n",
            "Iteration: 8256; loss: 0.6943738460540771\n",
            "Iteration: 8257; loss: 0.6927027106285095\n",
            "Iteration: 8258; loss: 0.6939647793769836\n",
            "Iteration: 8259; loss: 0.6931013464927673\n",
            "Iteration: 8260; loss: 0.6972194314002991\n",
            "Iteration: 8261; loss: 0.6896075010299683\n",
            "Iteration: 8262; loss: 0.6929439306259155\n",
            "Iteration: 8263; loss: 0.6955685615539551\n",
            "Iteration: 8264; loss: 0.6933025121688843\n",
            "Iteration: 8265; loss: 0.696746289730072\n",
            "Iteration: 8266; loss: 0.6913231611251831\n",
            "Iteration: 8267; loss: 0.6935570240020752\n",
            "Iteration: 8268; loss: 0.6936621069908142\n",
            "Iteration: 8269; loss: 0.6926462054252625\n",
            "Iteration: 8270; loss: 0.6956344246864319\n",
            "Iteration: 8271; loss: 0.6928026676177979\n",
            "Iteration: 8272; loss: 0.694995105266571\n",
            "Iteration: 8273; loss: 0.6938059329986572\n",
            "Iteration: 8274; loss: 0.6951621770858765\n",
            "Iteration: 8275; loss: 0.6905419826507568\n",
            "Iteration: 8276; loss: 0.6894088387489319\n",
            "Iteration: 8277; loss: 0.6890333890914917\n",
            "Iteration: 8278; loss: 0.6935213804244995\n",
            "Iteration: 8279; loss: 0.6936067342758179\n",
            "Iteration: 8280; loss: 0.6928059458732605\n",
            "Iteration: 8281; loss: 0.6917492151260376\n",
            "Iteration: 8282; loss: 0.6920299530029297\n",
            "Iteration: 8283; loss: 0.6960604190826416\n",
            "Iteration: 8284; loss: 0.6910507082939148\n",
            "Iteration: 8285; loss: 0.6927497982978821\n",
            "Iteration: 8286; loss: 0.6941771507263184\n",
            "Iteration: 8287; loss: 0.6927231550216675\n",
            "Iteration: 8288; loss: 0.6910433173179626\n",
            "Iteration: 8289; loss: 0.6950765252113342\n",
            "Iteration: 8290; loss: 0.6962356567382812\n",
            "Iteration: 8291; loss: 0.6918964385986328\n",
            "Iteration: 8292; loss: 0.6896200776100159\n",
            "Iteration: 8293; loss: 0.6910991668701172\n",
            "Iteration: 8294; loss: 0.6941903829574585\n",
            "Iteration: 8295; loss: 0.691670835018158\n",
            "Iteration: 8296; loss: 0.6898109912872314\n",
            "Iteration: 8297; loss: 0.6929311752319336\n",
            "Iteration: 8298; loss: 0.6912904977798462\n",
            "Iteration: 8299; loss: 0.6898482441902161\n",
            "Iteration: 8300; loss: 0.6926212906837463\n",
            "Iteration: 8301; loss: 0.6904324293136597\n",
            "Iteration: 8302; loss: 0.6916525959968567\n",
            "Iteration: 8303; loss: 0.6950087547302246\n",
            "Iteration: 8304; loss: 0.6962776184082031\n",
            "Iteration: 8305; loss: 0.6910871267318726\n",
            "Iteration: 8306; loss: 0.6924818754196167\n",
            "Iteration: 8307; loss: 0.6933979988098145\n",
            "Iteration: 8308; loss: 0.6926464438438416\n",
            "Iteration: 8309; loss: 0.6904601454734802\n",
            "Iteration: 8310; loss: 0.6934765577316284\n",
            "Iteration: 8311; loss: 0.6907844543457031\n",
            "Iteration: 8312; loss: 0.6959322094917297\n",
            "Iteration: 8313; loss: 0.6891315579414368\n",
            "Iteration: 8314; loss: 0.6895643472671509\n",
            "Iteration: 8315; loss: 0.6926112174987793\n",
            "Iteration: 8316; loss: 0.6988378763198853\n",
            "Iteration: 8317; loss: 0.695763111114502\n",
            "Iteration: 8318; loss: 0.6917619705200195\n",
            "Iteration: 8319; loss: 0.6904420852661133\n",
            "Iteration: 8320; loss: 0.6904557943344116\n",
            "Iteration: 8321; loss: 0.6891337037086487\n",
            "Iteration: 8322; loss: 0.6899964809417725\n",
            "Iteration: 8323; loss: 0.6931821703910828\n",
            "Iteration: 8324; loss: 0.6947250962257385\n",
            "Iteration: 8325; loss: 0.6948760151863098\n",
            "Iteration: 8326; loss: 0.6953545212745667\n",
            "Iteration: 8327; loss: 0.6953052282333374\n",
            "Iteration: 8328; loss: 0.6922674775123596\n",
            "Iteration: 8329; loss: 0.692598819732666\n",
            "Iteration: 8330; loss: 0.6922262907028198\n",
            "Iteration: 8331; loss: 0.6884931325912476\n",
            "Iteration: 8332; loss: 0.6904993653297424\n",
            "Iteration: 8333; loss: 0.6931251883506775\n",
            "Iteration: 8334; loss: 0.6914803385734558\n",
            "Iteration: 8335; loss: 0.6945263147354126\n",
            "Iteration: 8336; loss: 0.6964750289916992\n",
            "Iteration: 8337; loss: 0.687671959400177\n",
            "Iteration: 8338; loss: 0.6928530931472778\n",
            "Iteration: 8339; loss: 0.6946966052055359\n",
            "Iteration: 8340; loss: 0.6944428086280823\n",
            "Iteration: 8341; loss: 0.6887586712837219\n",
            "Iteration: 8342; loss: 0.6938325762748718\n",
            "Iteration: 8343; loss: 0.6883597373962402\n",
            "Iteration: 8344; loss: 0.6893408894538879\n",
            "Iteration: 8345; loss: 0.6936116814613342\n",
            "Iteration: 8346; loss: 0.6960084438323975\n",
            "Iteration: 8347; loss: 0.6905840635299683\n",
            "Iteration: 8348; loss: 0.6930797100067139\n",
            "Iteration: 8349; loss: 0.6914433240890503\n",
            "Iteration: 8350; loss: 0.6899182796478271\n",
            "Iteration: 8351; loss: 0.6926931142807007\n",
            "Iteration: 8352; loss: 0.6944137811660767\n",
            "Iteration: 8353; loss: 0.6978519558906555\n",
            "Iteration: 8354; loss: 0.6938084959983826\n",
            "Iteration: 8355; loss: 0.6950676441192627\n",
            "Iteration: 8356; loss: 0.6956204771995544\n",
            "Iteration: 8357; loss: 0.6928258538246155\n",
            "Iteration: 8358; loss: 0.6965956687927246\n",
            "Iteration: 8359; loss: 0.6910068988800049\n",
            "Iteration: 8360; loss: 0.6932597160339355\n",
            "Iteration: 8361; loss: 0.6897451877593994\n",
            "Iteration: 8362; loss: 0.6924937963485718\n",
            "Iteration: 8363; loss: 0.6915456652641296\n",
            "Iteration: 8364; loss: 0.6907007694244385\n",
            "Iteration: 8365; loss: 0.6879965662956238\n",
            "Iteration: 8366; loss: 0.6951515674591064\n",
            "Iteration: 8367; loss: 0.6946829557418823\n",
            "Iteration: 8368; loss: 0.6922359466552734\n",
            "Iteration: 8369; loss: 0.6943492889404297\n",
            "Iteration: 8370; loss: 0.6912318468093872\n",
            "Iteration: 8371; loss: 0.6944238543510437\n",
            "Iteration: 8372; loss: 0.6900523900985718\n",
            "Iteration: 8373; loss: 0.688384473323822\n",
            "Iteration: 8374; loss: 0.6962809562683105\n",
            "Iteration: 8375; loss: 0.6923025846481323\n",
            "Iteration: 8376; loss: 0.6956181526184082\n",
            "Iteration: 8377; loss: 0.6935814619064331\n",
            "Iteration: 8378; loss: 0.691757321357727\n",
            "Iteration: 8379; loss: 0.6920888423919678\n",
            "Iteration: 8380; loss: 0.6918016672134399\n",
            "Iteration: 8381; loss: 0.6885562539100647\n",
            "Iteration: 8382; loss: 0.6962813138961792\n",
            "Iteration: 8383; loss: 0.6940558552742004\n",
            "Iteration: 8384; loss: 0.6881778240203857\n",
            "Iteration: 8385; loss: 0.6921254992485046\n",
            "Iteration: 8386; loss: 0.6903080344200134\n",
            "Iteration: 8387; loss: 0.6888183951377869\n",
            "Iteration: 8388; loss: 0.6974104046821594\n",
            "Iteration: 8389; loss: 0.6932201385498047\n",
            "Iteration: 8390; loss: 0.6987727880477905\n",
            "Iteration: 8391; loss: 0.6926924586296082\n",
            "Iteration: 8392; loss: 0.6908945441246033\n",
            "Iteration: 8393; loss: 0.6966105103492737\n",
            "Iteration: 8394; loss: 0.6916329264640808\n",
            "Iteration: 8395; loss: 0.6894100904464722\n",
            "Iteration: 8396; loss: 0.6950230002403259\n",
            "Iteration: 8397; loss: 0.6934368014335632\n",
            "Iteration: 8398; loss: 0.693329930305481\n",
            "Iteration: 8399; loss: 0.6909441947937012\n",
            "Iteration: 8400; loss: 0.6956411004066467\n",
            "Iteration: 8401; loss: 0.6932366490364075\n",
            "Iteration: 8402; loss: 0.6911097764968872\n",
            "Iteration: 8403; loss: 0.6925589442253113\n",
            "Iteration: 8404; loss: 0.6949536800384521\n",
            "Iteration: 8405; loss: 0.6943846344947815\n",
            "Iteration: 8406; loss: 0.69084233045578\n",
            "Iteration: 8407; loss: 0.6898062229156494\n",
            "Iteration: 8408; loss: 0.6892436742782593\n",
            "Iteration: 8409; loss: 0.6881454586982727\n",
            "Iteration: 8410; loss: 0.6913188099861145\n",
            "Iteration: 8411; loss: 0.6917820572853088\n",
            "Iteration: 8412; loss: 0.6925249695777893\n",
            "Iteration: 8413; loss: 0.6961221694946289\n",
            "Iteration: 8414; loss: 0.6983751058578491\n",
            "Iteration: 8415; loss: 0.6881506443023682\n",
            "Iteration: 8416; loss: 0.6950019598007202\n",
            "Iteration: 8417; loss: 0.6940258741378784\n",
            "Iteration: 8418; loss: 0.6930796504020691\n",
            "Iteration: 8419; loss: 0.6920509934425354\n",
            "Iteration: 8420; loss: 0.6985083818435669\n",
            "Iteration: 8421; loss: 0.6953967809677124\n",
            "Iteration: 8422; loss: 0.6899183988571167\n",
            "Iteration: 8423; loss: 0.6904999017715454\n",
            "Iteration: 8424; loss: 0.6960190534591675\n",
            "Iteration: 8425; loss: 0.6913003921508789\n",
            "Iteration: 8426; loss: 0.6891061067581177\n",
            "Iteration: 8427; loss: 0.6903863549232483\n",
            "Iteration: 8428; loss: 0.6906453371047974\n",
            "Iteration: 8429; loss: 0.6896727085113525\n",
            "Iteration: 8430; loss: 0.6946755647659302\n",
            "Iteration: 8431; loss: 0.6936392784118652\n",
            "Iteration: 8432; loss: 0.6915523409843445\n",
            "Iteration: 8433; loss: 0.6940497159957886\n",
            "Iteration: 8434; loss: 0.6892601847648621\n",
            "Iteration: 8435; loss: 0.6952852606773376\n",
            "Iteration: 8436; loss: 0.6899005174636841\n",
            "Iteration: 8437; loss: 0.6875669360160828\n",
            "Iteration: 8438; loss: 0.6937404870986938\n",
            "Iteration: 8439; loss: 0.6867440938949585\n",
            "Iteration: 8440; loss: 0.6906032562255859\n",
            "Iteration: 8441; loss: 0.6909230947494507\n",
            "Iteration: 8442; loss: 0.6886081695556641\n",
            "Iteration: 8443; loss: 0.6925909519195557\n",
            "Iteration: 8444; loss: 0.6953040957450867\n",
            "Iteration: 8445; loss: 0.6931394338607788\n",
            "Iteration: 8446; loss: 0.6976354122161865\n",
            "Iteration: 8447; loss: 0.6876095533370972\n",
            "Iteration: 8448; loss: 0.6974458694458008\n",
            "Iteration: 8449; loss: 0.6936408877372742\n",
            "Iteration: 8450; loss: 0.6950398683547974\n",
            "Iteration: 8451; loss: 0.693618893623352\n",
            "Iteration: 8452; loss: 0.695530891418457\n",
            "Iteration: 8453; loss: 0.6918659806251526\n",
            "Iteration: 8454; loss: 0.694940984249115\n",
            "Iteration: 8455; loss: 0.6912738680839539\n",
            "Iteration: 8456; loss: 0.6949030756950378\n",
            "Iteration: 8457; loss: 0.6887761950492859\n",
            "Iteration: 8458; loss: 0.6937936544418335\n",
            "Iteration: 8459; loss: 0.6950898170471191\n",
            "Iteration: 8460; loss: 0.6909687519073486\n",
            "Iteration: 8461; loss: 0.6924284100532532\n",
            "Iteration: 8462; loss: 0.6894623041152954\n",
            "Iteration: 8463; loss: 0.6902098655700684\n",
            "Iteration: 8464; loss: 0.6909855604171753\n",
            "Iteration: 8465; loss: 0.6919757127761841\n",
            "Iteration: 8466; loss: 0.6911104917526245\n",
            "Iteration: 8467; loss: 0.6918903589248657\n",
            "Iteration: 8468; loss: 0.694303572177887\n",
            "Iteration: 8469; loss: 0.6907174587249756\n",
            "Iteration: 8470; loss: 0.6878643035888672\n",
            "Iteration: 8471; loss: 0.6878305673599243\n",
            "Iteration: 8472; loss: 0.6901984214782715\n",
            "Iteration: 8473; loss: 0.693785548210144\n",
            "Iteration: 8474; loss: 0.6927973031997681\n",
            "Iteration: 8475; loss: 0.6908308267593384\n",
            "Iteration: 8476; loss: 0.6937254667282104\n",
            "Iteration: 8477; loss: 0.690567672252655\n",
            "Iteration: 8478; loss: 0.6935248374938965\n",
            "Iteration: 8479; loss: 0.69460129737854\n",
            "Iteration: 8480; loss: 0.6984267234802246\n",
            "Iteration: 8481; loss: 0.6876296997070312\n",
            "Iteration: 8482; loss: 0.6935747265815735\n",
            "Iteration: 8483; loss: 0.6951484084129333\n",
            "Iteration: 8484; loss: 0.6903658509254456\n",
            "Iteration: 8485; loss: 0.6881917119026184\n",
            "Iteration: 8486; loss: 0.6922227144241333\n",
            "Iteration: 8487; loss: 0.6901368498802185\n",
            "Iteration: 8488; loss: 0.6925398111343384\n",
            "Iteration: 8489; loss: 0.6926566362380981\n",
            "Iteration: 8490; loss: 0.7010179162025452\n",
            "Iteration: 8491; loss: 0.6919758915901184\n",
            "Iteration: 8492; loss: 0.6952831149101257\n",
            "Iteration: 8493; loss: 0.6932432651519775\n",
            "Iteration: 8494; loss: 0.692536473274231\n",
            "Iteration: 8495; loss: 0.6939660310745239\n",
            "Iteration: 8496; loss: 0.6969813108444214\n",
            "Iteration: 8497; loss: 0.6923072338104248\n",
            "Iteration: 8498; loss: 0.6932699084281921\n",
            "Iteration: 8499; loss: 0.690701425075531\n",
            "Iteration: 8500; loss: 0.6957597732543945\n",
            "Iteration: 8501; loss: 0.6907728910446167\n",
            "Iteration: 8502; loss: 0.6941636800765991\n",
            "Iteration: 8503; loss: 0.692223310470581\n",
            "Iteration: 8504; loss: 0.686407208442688\n",
            "Iteration: 8505; loss: 0.6927261352539062\n",
            "Iteration: 8506; loss: 0.6912767291069031\n",
            "Iteration: 8507; loss: 0.6936440467834473\n",
            "Iteration: 8508; loss: 0.6889487504959106\n",
            "Iteration: 8509; loss: 0.6910296678543091\n",
            "Iteration: 8510; loss: 0.6922670602798462\n",
            "Iteration: 8511; loss: 0.6923794746398926\n",
            "Iteration: 8512; loss: 0.6901007294654846\n",
            "Iteration: 8513; loss: 0.6951087117195129\n",
            "Iteration: 8514; loss: 0.6856565475463867\n",
            "Iteration: 8515; loss: 0.689409613609314\n",
            "Iteration: 8516; loss: 0.693418562412262\n",
            "Iteration: 8517; loss: 0.6887447237968445\n",
            "Iteration: 8518; loss: 0.6932420134544373\n",
            "Iteration: 8519; loss: 0.695436418056488\n",
            "Iteration: 8520; loss: 0.6923055052757263\n",
            "Iteration: 8521; loss: 0.6918953061103821\n",
            "Iteration: 8522; loss: 0.694915771484375\n",
            "Iteration: 8523; loss: 0.6923496723175049\n",
            "Iteration: 8524; loss: 0.6956136226654053\n",
            "Iteration: 8525; loss: 0.6919119954109192\n",
            "Iteration: 8526; loss: 0.6938595771789551\n",
            "Iteration: 8527; loss: 0.6913115382194519\n",
            "Iteration: 8528; loss: 0.6956171989440918\n",
            "Iteration: 8529; loss: 0.6942539215087891\n",
            "Iteration: 8530; loss: 0.6943519115447998\n",
            "Iteration: 8531; loss: 0.6965277194976807\n",
            "Iteration: 8532; loss: 0.6986724138259888\n",
            "Iteration: 8533; loss: 0.6880859136581421\n",
            "Iteration: 8534; loss: 0.6926501393318176\n",
            "Iteration: 8535; loss: 0.6870236992835999\n",
            "Iteration: 8536; loss: 0.6923335790634155\n",
            "Iteration: 8537; loss: 0.6901165246963501\n",
            "Iteration: 8538; loss: 0.6884631514549255\n",
            "Iteration: 8539; loss: 0.6985231637954712\n",
            "Iteration: 8540; loss: 0.6883971691131592\n",
            "Iteration: 8541; loss: 0.693939208984375\n",
            "Iteration: 8542; loss: 0.6868520975112915\n",
            "Iteration: 8543; loss: 0.6937031745910645\n",
            "Iteration: 8544; loss: 0.6959040760993958\n",
            "Iteration: 8545; loss: 0.695075273513794\n",
            "Iteration: 8546; loss: 0.6927618980407715\n",
            "Iteration: 8547; loss: 0.6929231286048889\n",
            "Iteration: 8548; loss: 0.6912086009979248\n",
            "Iteration: 8549; loss: 0.693103551864624\n",
            "Iteration: 8550; loss: 0.6916593313217163\n",
            "Iteration: 8551; loss: 0.6958181858062744\n",
            "Iteration: 8552; loss: 0.694075345993042\n",
            "Iteration: 8553; loss: 0.6935850381851196\n",
            "Iteration: 8554; loss: 0.6939724087715149\n",
            "Iteration: 8555; loss: 0.692915141582489\n",
            "Iteration: 8556; loss: 0.6931158304214478\n",
            "Iteration: 8557; loss: 0.6872518062591553\n",
            "Iteration: 8558; loss: 0.6938717365264893\n",
            "Iteration: 8559; loss: 0.691251277923584\n",
            "Iteration: 8560; loss: 0.6931149959564209\n",
            "Iteration: 8561; loss: 0.695991039276123\n",
            "Iteration: 8562; loss: 0.6912181377410889\n",
            "Iteration: 8563; loss: 0.6899780035018921\n",
            "Iteration: 8564; loss: 0.6894481182098389\n",
            "Iteration: 8565; loss: 0.6887618899345398\n",
            "Iteration: 8566; loss: 0.6927089691162109\n",
            "Iteration: 8567; loss: 0.6890081167221069\n",
            "Iteration: 8568; loss: 0.6908671259880066\n",
            "Iteration: 8569; loss: 0.6949080228805542\n",
            "Iteration: 8570; loss: 0.6929652094841003\n",
            "Iteration: 8571; loss: 0.6886497139930725\n",
            "Iteration: 8572; loss: 0.6938272714614868\n",
            "Iteration: 8573; loss: 0.6943298578262329\n",
            "Iteration: 8574; loss: 0.6886107325553894\n",
            "Iteration: 8575; loss: 0.6948449015617371\n",
            "Iteration: 8576; loss: 0.6893107891082764\n",
            "Iteration: 8577; loss: 0.6930344104766846\n",
            "Iteration: 8578; loss: 0.6919749975204468\n",
            "Iteration: 8579; loss: 0.6902363896369934\n",
            "Iteration: 8580; loss: 0.6895411014556885\n",
            "Iteration: 8581; loss: 0.692058801651001\n",
            "Iteration: 8582; loss: 0.6953138113021851\n",
            "Iteration: 8583; loss: 0.6933612823486328\n",
            "Iteration: 8584; loss: 0.6902691125869751\n",
            "Iteration: 8585; loss: 0.694463312625885\n",
            "Iteration: 8586; loss: 0.6921157240867615\n",
            "Iteration: 8587; loss: 0.6942187547683716\n",
            "Iteration: 8588; loss: 0.6914331912994385\n",
            "Iteration: 8589; loss: 0.6909472942352295\n",
            "Iteration: 8590; loss: 0.692865252494812\n",
            "Iteration: 8591; loss: 0.6966622471809387\n",
            "Iteration: 8592; loss: 0.6966793537139893\n",
            "Iteration: 8593; loss: 0.6963845491409302\n",
            "Iteration: 8594; loss: 0.6904351115226746\n",
            "Iteration: 8595; loss: 0.690377950668335\n",
            "Iteration: 8596; loss: 0.6850357055664062\n",
            "Iteration: 8597; loss: 0.6928065419197083\n",
            "Iteration: 8598; loss: 0.6949217319488525\n",
            "Iteration: 8599; loss: 0.6895843744277954\n",
            "Iteration: 8600; loss: 0.6905895471572876\n",
            "Iteration: 8601; loss: 0.6883345246315002\n",
            "Iteration: 8602; loss: 0.694146454334259\n",
            "Iteration: 8603; loss: 0.6896594762802124\n",
            "Iteration: 8604; loss: 0.694811224937439\n",
            "Iteration: 8605; loss: 0.6952071189880371\n",
            "Iteration: 8606; loss: 0.6920485496520996\n",
            "Iteration: 8607; loss: 0.6912418007850647\n",
            "Iteration: 8608; loss: 0.6963163614273071\n",
            "Iteration: 8609; loss: 0.6904438734054565\n",
            "Iteration: 8610; loss: 0.6940438151359558\n",
            "Iteration: 8611; loss: 0.6918282508850098\n",
            "Iteration: 8612; loss: 0.6929470896720886\n",
            "Iteration: 8613; loss: 0.6933649778366089\n",
            "Iteration: 8614; loss: 0.692283570766449\n",
            "Iteration: 8615; loss: 0.6951947808265686\n",
            "Iteration: 8616; loss: 0.6937096118927002\n",
            "Iteration: 8617; loss: 0.6931182146072388\n",
            "Iteration: 8618; loss: 0.6907947659492493\n",
            "Iteration: 8619; loss: 0.6919220685958862\n",
            "Iteration: 8620; loss: 0.6914392709732056\n",
            "Iteration: 8621; loss: 0.690994143486023\n",
            "Iteration: 8622; loss: 0.694753110408783\n",
            "Iteration: 8623; loss: 0.6909854412078857\n",
            "Iteration: 8624; loss: 0.6948513984680176\n",
            "Iteration: 8625; loss: 0.6975713968276978\n",
            "Iteration: 8626; loss: 0.6887076497077942\n",
            "Iteration: 8627; loss: 0.6941672563552856\n",
            "Iteration: 8628; loss: 0.688117265701294\n",
            "Iteration: 8629; loss: 0.6959353685379028\n",
            "Iteration: 8630; loss: 0.6912903189659119\n",
            "Iteration: 8631; loss: 0.6921145915985107\n",
            "Iteration: 8632; loss: 0.6908175945281982\n",
            "Iteration: 8633; loss: 0.6925083994865417\n",
            "Iteration: 8634; loss: 0.6933901906013489\n",
            "Iteration: 8635; loss: 0.688901960849762\n",
            "Iteration: 8636; loss: 0.6952009797096252\n",
            "Iteration: 8637; loss: 0.6925288438796997\n",
            "Iteration: 8638; loss: 0.6878747344017029\n",
            "Iteration: 8639; loss: 0.6930840015411377\n",
            "Iteration: 8640; loss: 0.6909521222114563\n",
            "Iteration: 8641; loss: 0.6908321380615234\n",
            "Iteration: 8642; loss: 0.6885634660720825\n",
            "Iteration: 8643; loss: 0.6898207068443298\n",
            "Iteration: 8644; loss: 0.6926268339157104\n",
            "Iteration: 8645; loss: 0.6955480575561523\n",
            "Iteration: 8646; loss: 0.6911989450454712\n",
            "Iteration: 8647; loss: 0.6919995546340942\n",
            "Iteration: 8648; loss: 0.6928405165672302\n",
            "Iteration: 8649; loss: 0.688944399356842\n",
            "Iteration: 8650; loss: 0.6923217177391052\n",
            "Iteration: 8651; loss: 0.69137042760849\n",
            "Iteration: 8652; loss: 0.6941061615943909\n",
            "Iteration: 8653; loss: 0.6861248016357422\n",
            "Iteration: 8654; loss: 0.6931228637695312\n",
            "Iteration: 8655; loss: 0.6875239014625549\n",
            "Iteration: 8656; loss: 0.6914117336273193\n",
            "Iteration: 8657; loss: 0.6912959218025208\n",
            "Iteration: 8658; loss: 0.693350076675415\n",
            "Iteration: 8659; loss: 0.6891671419143677\n",
            "Iteration: 8660; loss: 0.6910858154296875\n",
            "Iteration: 8661; loss: 0.6936846375465393\n",
            "Iteration: 8662; loss: 0.6881476640701294\n",
            "Iteration: 8663; loss: 0.6930757761001587\n",
            "Iteration: 8664; loss: 0.6914064884185791\n",
            "Iteration: 8665; loss: 0.6885806322097778\n",
            "Iteration: 8666; loss: 0.6905806660652161\n",
            "Iteration: 8667; loss: 0.693659245967865\n",
            "Iteration: 8668; loss: 0.6896429061889648\n",
            "Iteration: 8669; loss: 0.6896541714668274\n",
            "Iteration: 8670; loss: 0.6913085579872131\n",
            "Iteration: 8671; loss: 0.6879016160964966\n",
            "Iteration: 8672; loss: 0.6917390823364258\n",
            "Iteration: 8673; loss: 0.6897312998771667\n",
            "Iteration: 8674; loss: 0.6914950609207153\n",
            "Iteration: 8675; loss: 0.6930612325668335\n",
            "Iteration: 8676; loss: 0.6927244663238525\n",
            "Iteration: 8677; loss: 0.6945015788078308\n",
            "Iteration: 8678; loss: 0.6936575174331665\n",
            "Iteration: 8679; loss: 0.6948208808898926\n",
            "Iteration: 8680; loss: 0.6920871138572693\n",
            "Iteration: 8681; loss: 0.6976547241210938\n",
            "Iteration: 8682; loss: 0.7002291679382324\n",
            "Iteration: 8683; loss: 0.6897784471511841\n",
            "Iteration: 8684; loss: 0.6914573907852173\n",
            "Iteration: 8685; loss: 0.6931784152984619\n",
            "Iteration: 8686; loss: 0.690398097038269\n",
            "Iteration: 8687; loss: 0.6950127482414246\n",
            "Iteration: 8688; loss: 0.6931571960449219\n",
            "Iteration: 8689; loss: 0.6905821561813354\n",
            "Iteration: 8690; loss: 0.6929469108581543\n",
            "Iteration: 8691; loss: 0.694505512714386\n",
            "Iteration: 8692; loss: 0.690526008605957\n",
            "Iteration: 8693; loss: 0.68916255235672\n",
            "Iteration: 8694; loss: 0.6889376640319824\n",
            "Iteration: 8695; loss: 0.6917788982391357\n",
            "Iteration: 8696; loss: 0.6921091079711914\n",
            "Iteration: 8697; loss: 0.6937704086303711\n",
            "Iteration: 8698; loss: 0.6914569139480591\n",
            "Iteration: 8699; loss: 0.6901723146438599\n",
            "Iteration: 8700; loss: 0.6915545463562012\n",
            "Iteration: 8701; loss: 0.6923259496688843\n",
            "Iteration: 8702; loss: 0.6919785141944885\n",
            "Iteration: 8703; loss: 0.6942448019981384\n",
            "Iteration: 8704; loss: 0.6968171000480652\n",
            "Iteration: 8705; loss: 0.6957536339759827\n",
            "Iteration: 8706; loss: 0.6895070672035217\n",
            "Iteration: 8707; loss: 0.6943804025650024\n",
            "Iteration: 8708; loss: 0.6909536719322205\n",
            "Iteration: 8709; loss: 0.6910423636436462\n",
            "Iteration: 8710; loss: 0.6886701583862305\n",
            "Iteration: 8711; loss: 0.6896353960037231\n",
            "Iteration: 8712; loss: 0.6951878666877747\n",
            "Iteration: 8713; loss: 0.6945202946662903\n",
            "Iteration: 8714; loss: 0.6908171772956848\n",
            "Iteration: 8715; loss: 0.6939386129379272\n",
            "Iteration: 8716; loss: 0.6928184628486633\n",
            "Iteration: 8717; loss: 0.6925475001335144\n",
            "Iteration: 8718; loss: 0.6939762830734253\n",
            "Iteration: 8719; loss: 0.6939387321472168\n",
            "Iteration: 8720; loss: 0.6946898102760315\n",
            "Iteration: 8721; loss: 0.6923012137413025\n",
            "Iteration: 8722; loss: 0.6892461776733398\n",
            "Iteration: 8723; loss: 0.6929997205734253\n",
            "Iteration: 8724; loss: 0.6967536807060242\n",
            "Iteration: 8725; loss: 0.6911482810974121\n",
            "Iteration: 8726; loss: 0.694679856300354\n",
            "Iteration: 8727; loss: 0.6899451017379761\n",
            "Iteration: 8728; loss: 0.6921606063842773\n",
            "Iteration: 8729; loss: 0.6918966770172119\n",
            "Iteration: 8730; loss: 0.6892281770706177\n",
            "Iteration: 8731; loss: 0.6891523003578186\n",
            "Iteration: 8732; loss: 0.6895803213119507\n",
            "Iteration: 8733; loss: 0.6935800909996033\n",
            "Iteration: 8734; loss: 0.6913769841194153\n",
            "Iteration: 8735; loss: 0.6908154487609863\n",
            "Iteration: 8736; loss: 0.688616931438446\n",
            "Iteration: 8737; loss: 0.6905233860015869\n",
            "Iteration: 8738; loss: 0.6911745071411133\n",
            "Iteration: 8739; loss: 0.6870502233505249\n",
            "Iteration: 8740; loss: 0.6968932747840881\n",
            "Iteration: 8741; loss: 0.6903220415115356\n",
            "Iteration: 8742; loss: 0.6947311162948608\n",
            "Iteration: 8743; loss: 0.6911715269088745\n",
            "Iteration: 8744; loss: 0.6950733661651611\n",
            "Iteration: 8745; loss: 0.6908473968505859\n",
            "Iteration: 8746; loss: 0.6925163269042969\n",
            "Iteration: 8747; loss: 0.6943217515945435\n",
            "Iteration: 8748; loss: 0.6937893033027649\n",
            "Iteration: 8749; loss: 0.6893508434295654\n",
            "Iteration: 8750; loss: 0.6910842061042786\n",
            "Iteration: 8751; loss: 0.691788375377655\n",
            "Iteration: 8752; loss: 0.692167341709137\n",
            "Iteration: 8753; loss: 0.6926018595695496\n",
            "Iteration: 8754; loss: 0.6957681179046631\n",
            "Iteration: 8755; loss: 0.6892719864845276\n",
            "Iteration: 8756; loss: 0.6955365538597107\n",
            "Iteration: 8757; loss: 0.6931687593460083\n",
            "Iteration: 8758; loss: 0.6922556757926941\n",
            "Iteration: 8759; loss: 0.6909648776054382\n",
            "Iteration: 8760; loss: 0.6934391260147095\n",
            "Iteration: 8761; loss: 0.6988057494163513\n",
            "Iteration: 8762; loss: 0.6872501969337463\n",
            "Iteration: 8763; loss: 0.6946318745613098\n",
            "Iteration: 8764; loss: 0.6904628276824951\n",
            "Iteration: 8765; loss: 0.6968030333518982\n",
            "Iteration: 8766; loss: 0.6928830742835999\n",
            "Iteration: 8767; loss: 0.6918842792510986\n",
            "Iteration: 8768; loss: 0.691776692867279\n",
            "Iteration: 8769; loss: 0.6929627656936646\n",
            "Iteration: 8770; loss: 0.6906082630157471\n",
            "Iteration: 8771; loss: 0.6900175213813782\n",
            "Iteration: 8772; loss: 0.6954625844955444\n",
            "Iteration: 8773; loss: 0.6887519955635071\n",
            "Iteration: 8774; loss: 0.6941096186637878\n",
            "Iteration: 8775; loss: 0.6895992755889893\n",
            "Iteration: 8776; loss: 0.6888909935951233\n",
            "Iteration: 8777; loss: 0.6876016855239868\n",
            "Iteration: 8778; loss: 0.6944277286529541\n",
            "Iteration: 8779; loss: 0.6917559504508972\n",
            "Iteration: 8780; loss: 0.6982772946357727\n",
            "Iteration: 8781; loss: 0.69354647397995\n",
            "Iteration: 8782; loss: 0.6925866007804871\n",
            "Iteration: 8783; loss: 0.6931931376457214\n",
            "Iteration: 8784; loss: 0.6928122043609619\n",
            "Iteration: 8785; loss: 0.6912990808486938\n",
            "Iteration: 8786; loss: 0.6930698752403259\n",
            "Iteration: 8787; loss: 0.6917242407798767\n",
            "Iteration: 8788; loss: 0.6881169080734253\n",
            "Iteration: 8789; loss: 0.691620945930481\n",
            "Iteration: 8790; loss: 0.6887957453727722\n",
            "Iteration: 8791; loss: 0.6929617524147034\n",
            "Iteration: 8792; loss: 0.6920437812805176\n",
            "Iteration: 8793; loss: 0.6886923909187317\n",
            "Iteration: 8794; loss: 0.6941803097724915\n",
            "Iteration: 8795; loss: 0.692664384841919\n",
            "Iteration: 8796; loss: 0.6947514414787292\n",
            "Iteration: 8797; loss: 0.6970013976097107\n",
            "Iteration: 8798; loss: 0.6854186058044434\n",
            "Iteration: 8799; loss: 0.6946262121200562\n",
            "Iteration: 8800; loss: 0.6872551441192627\n",
            "Iteration: 8801; loss: 0.6902424693107605\n",
            "Iteration: 8802; loss: 0.6852850317955017\n",
            "Iteration: 8803; loss: 0.6891050338745117\n",
            "Iteration: 8804; loss: 0.6934525966644287\n",
            "Iteration: 8805; loss: 0.6950346231460571\n",
            "Iteration: 8806; loss: 0.6941184401512146\n",
            "Iteration: 8807; loss: 0.6891207695007324\n",
            "Iteration: 8808; loss: 0.6928415298461914\n",
            "Iteration: 8809; loss: 0.6937512159347534\n",
            "Iteration: 8810; loss: 0.6911780834197998\n",
            "Iteration: 8811; loss: 0.6913971900939941\n",
            "Iteration: 8812; loss: 0.6884906888008118\n",
            "Iteration: 8813; loss: 0.6970018148422241\n",
            "Iteration: 8814; loss: 0.6959776282310486\n",
            "Iteration: 8815; loss: 0.686774730682373\n",
            "Iteration: 8816; loss: 0.6902137994766235\n",
            "Iteration: 8817; loss: 0.6942137479782104\n",
            "Iteration: 8818; loss: 0.6919538974761963\n",
            "Iteration: 8819; loss: 0.6863908171653748\n",
            "Iteration: 8820; loss: 0.6935257911682129\n",
            "Iteration: 8821; loss: 0.6910712122917175\n",
            "Iteration: 8822; loss: 0.687916100025177\n",
            "Iteration: 8823; loss: 0.6961151957511902\n",
            "Iteration: 8824; loss: 0.6933572888374329\n",
            "Iteration: 8825; loss: 0.691346287727356\n",
            "Iteration: 8826; loss: 0.6921647191047668\n",
            "Iteration: 8827; loss: 0.6923660039901733\n",
            "Iteration: 8828; loss: 0.6939677000045776\n",
            "Iteration: 8829; loss: 0.6938144564628601\n",
            "Iteration: 8830; loss: 0.6957947015762329\n",
            "Iteration: 8831; loss: 0.6907448768615723\n",
            "Iteration: 8832; loss: 0.6862404346466064\n",
            "Iteration: 8833; loss: 0.6919610500335693\n",
            "Iteration: 8834; loss: 0.6876933574676514\n",
            "Iteration: 8835; loss: 0.6929721236228943\n",
            "Iteration: 8836; loss: 0.6924974322319031\n",
            "Iteration: 8837; loss: 0.691631555557251\n",
            "Iteration: 8838; loss: 0.6875348091125488\n",
            "Iteration: 8839; loss: 0.6915575861930847\n",
            "Iteration: 8840; loss: 0.6937516331672668\n",
            "Iteration: 8841; loss: 0.693605899810791\n",
            "Iteration: 8842; loss: 0.6889756321907043\n",
            "Iteration: 8843; loss: 0.6911753416061401\n",
            "Iteration: 8844; loss: 0.6935223937034607\n",
            "Iteration: 8845; loss: 0.696149468421936\n",
            "Iteration: 8846; loss: 0.6940204501152039\n",
            "Iteration: 8847; loss: 0.694595992565155\n",
            "Iteration: 8848; loss: 0.690121054649353\n",
            "Iteration: 8849; loss: 0.6952425241470337\n",
            "Iteration: 8850; loss: 0.6922412514686584\n",
            "Iteration: 8851; loss: 0.6898539066314697\n",
            "Iteration: 8852; loss: 0.692763090133667\n",
            "Iteration: 8853; loss: 0.6914165616035461\n",
            "Iteration: 8854; loss: 0.6948119401931763\n",
            "Iteration: 8855; loss: 0.6955202221870422\n",
            "Iteration: 8856; loss: 0.6912124156951904\n",
            "Iteration: 8857; loss: 0.6936224699020386\n",
            "Iteration: 8858; loss: 0.6941988468170166\n",
            "Iteration: 8859; loss: 0.6850482225418091\n",
            "Iteration: 8860; loss: 0.6938906311988831\n",
            "Iteration: 8861; loss: 0.687896728515625\n",
            "Iteration: 8862; loss: 0.6877726316452026\n",
            "Iteration: 8863; loss: 0.691579282283783\n",
            "Iteration: 8864; loss: 0.6886763572692871\n",
            "Iteration: 8865; loss: 0.6950157880783081\n",
            "Iteration: 8866; loss: 0.6910907030105591\n",
            "Iteration: 8867; loss: 0.690117597579956\n",
            "Iteration: 8868; loss: 0.6978511810302734\n",
            "Iteration: 8869; loss: 0.6888980865478516\n",
            "Iteration: 8870; loss: 0.6975172758102417\n",
            "Iteration: 8871; loss: 0.6882394552230835\n",
            "Iteration: 8872; loss: 0.686597466468811\n",
            "Iteration: 8873; loss: 0.693315327167511\n",
            "Iteration: 8874; loss: 0.6955347061157227\n",
            "Iteration: 8875; loss: 0.694389283657074\n",
            "Iteration: 8876; loss: 0.6934186220169067\n",
            "Iteration: 8877; loss: 0.6864092946052551\n",
            "Iteration: 8878; loss: 0.6877411603927612\n",
            "Iteration: 8879; loss: 0.6927343606948853\n",
            "Iteration: 8880; loss: 0.6924880743026733\n",
            "Iteration: 8881; loss: 0.685716450214386\n",
            "Iteration: 8882; loss: 0.6915613412857056\n",
            "Iteration: 8883; loss: 0.6936168074607849\n",
            "Iteration: 8884; loss: 0.6931840181350708\n",
            "Iteration: 8885; loss: 0.6879796385765076\n",
            "Iteration: 8886; loss: 0.6900085210800171\n",
            "Iteration: 8887; loss: 0.6865466237068176\n",
            "Iteration: 8888; loss: 0.6933925151824951\n",
            "Iteration: 8889; loss: 0.6914792656898499\n",
            "Iteration: 8890; loss: 0.6960147023200989\n",
            "Iteration: 8891; loss: 0.6958150863647461\n",
            "Iteration: 8892; loss: 0.7023177146911621\n",
            "Iteration: 8893; loss: 0.6955536007881165\n",
            "Iteration: 8894; loss: 0.6955690383911133\n",
            "Iteration: 8895; loss: 0.6898815035820007\n",
            "Iteration: 8896; loss: 0.6882093548774719\n",
            "Iteration: 8897; loss: 0.6896728277206421\n",
            "Iteration: 8898; loss: 0.6978250741958618\n",
            "Iteration: 8899; loss: 0.6915340423583984\n",
            "Iteration: 8900; loss: 0.6944749355316162\n",
            "Iteration: 8901; loss: 0.6914443373680115\n",
            "Iteration: 8902; loss: 0.6905775666236877\n",
            "Iteration: 8903; loss: 0.6922577023506165\n",
            "Iteration: 8904; loss: 0.6874278783798218\n",
            "Iteration: 8905; loss: 0.6861806511878967\n",
            "Iteration: 8906; loss: 0.6912004947662354\n",
            "Iteration: 8907; loss: 0.6955087184906006\n",
            "Iteration: 8908; loss: 0.6909403204917908\n",
            "Iteration: 8909; loss: 0.6936838626861572\n",
            "Iteration: 8910; loss: 0.6935499310493469\n",
            "Iteration: 8911; loss: 0.6859479546546936\n",
            "Iteration: 8912; loss: 0.6942667365074158\n",
            "Iteration: 8913; loss: 0.6882500052452087\n",
            "Iteration: 8914; loss: 0.6907097697257996\n",
            "Iteration: 8915; loss: 0.697593629360199\n",
            "Iteration: 8916; loss: 0.6876676082611084\n",
            "Iteration: 8917; loss: 0.6907922625541687\n",
            "Iteration: 8918; loss: 0.6921508312225342\n",
            "Iteration: 8919; loss: 0.6871102452278137\n",
            "Iteration: 8920; loss: 0.6894097924232483\n",
            "Iteration: 8921; loss: 0.6929949522018433\n",
            "Iteration: 8922; loss: 0.6909087300300598\n",
            "Iteration: 8923; loss: 0.6961727142333984\n",
            "Iteration: 8924; loss: 0.6916549205780029\n",
            "Iteration: 8925; loss: 0.693724513053894\n",
            "Iteration: 8926; loss: 0.6969150304794312\n",
            "Iteration: 8927; loss: 0.6950326561927795\n",
            "Iteration: 8928; loss: 0.6944071054458618\n",
            "Iteration: 8929; loss: 0.694246768951416\n",
            "Iteration: 8930; loss: 0.6958833336830139\n",
            "Iteration: 8931; loss: 0.6914494037628174\n",
            "Iteration: 8932; loss: 0.6897211074829102\n",
            "Iteration: 8933; loss: 0.6938180327415466\n",
            "Iteration: 8934; loss: 0.6922640800476074\n",
            "Iteration: 8935; loss: 0.6926872730255127\n",
            "Iteration: 8936; loss: 0.6898578405380249\n",
            "Iteration: 8937; loss: 0.6907399892807007\n",
            "Iteration: 8938; loss: 0.6919069886207581\n",
            "Iteration: 8939; loss: 0.6936028003692627\n",
            "Iteration: 8940; loss: 0.6919628977775574\n",
            "Iteration: 8941; loss: 0.6949367523193359\n",
            "Iteration: 8942; loss: 0.6900835037231445\n",
            "Iteration: 8943; loss: 0.6924318671226501\n",
            "Iteration: 8944; loss: 0.6935477256774902\n",
            "Iteration: 8945; loss: 0.6973080635070801\n",
            "Iteration: 8946; loss: 0.6871417760848999\n",
            "Iteration: 8947; loss: 0.6925723552703857\n",
            "Iteration: 8948; loss: 0.6900635957717896\n",
            "Iteration: 8949; loss: 0.6896175742149353\n",
            "Iteration: 8950; loss: 0.6925621032714844\n",
            "Iteration: 8951; loss: 0.6917967796325684\n",
            "Iteration: 8952; loss: 0.6913142800331116\n",
            "Iteration: 8953; loss: 0.6944233775138855\n",
            "Iteration: 8954; loss: 0.6926787495613098\n",
            "Iteration: 8955; loss: 0.6886376142501831\n",
            "Iteration: 8956; loss: 0.6887131929397583\n",
            "Iteration: 8957; loss: 0.6931670904159546\n",
            "Iteration: 8958; loss: 0.6888678073883057\n",
            "Iteration: 8959; loss: 0.6891378164291382\n",
            "Iteration: 8960; loss: 0.6911324262619019\n",
            "Iteration: 8961; loss: 0.6887515187263489\n",
            "Iteration: 8962; loss: 0.6982055902481079\n",
            "Iteration: 8963; loss: 0.6927604675292969\n",
            "Iteration: 8964; loss: 0.6898174285888672\n",
            "Iteration: 8965; loss: 0.6952019333839417\n",
            "Iteration: 8966; loss: 0.6947518587112427\n",
            "Iteration: 8967; loss: 0.6898816823959351\n",
            "Iteration: 8968; loss: 0.6911464929580688\n",
            "Iteration: 8969; loss: 0.6900471448898315\n",
            "Iteration: 8970; loss: 0.6895802021026611\n",
            "Iteration: 8971; loss: 0.6967820525169373\n",
            "Iteration: 8972; loss: 0.6932429671287537\n",
            "Iteration: 8973; loss: 0.6920310854911804\n",
            "Iteration: 8974; loss: 0.6961600184440613\n",
            "Iteration: 8975; loss: 0.690448522567749\n",
            "Iteration: 8976; loss: 0.6853324174880981\n",
            "Iteration: 8977; loss: 0.6898208260536194\n",
            "Iteration: 8978; loss: 0.6906081438064575\n",
            "Iteration: 8979; loss: 0.6901787519454956\n",
            "Iteration: 8980; loss: 0.6931593418121338\n",
            "Iteration: 8981; loss: 0.687303364276886\n",
            "Iteration: 8982; loss: 0.683315634727478\n",
            "Iteration: 8983; loss: 0.6876450777053833\n",
            "Iteration: 8984; loss: 0.6920263767242432\n",
            "Iteration: 8985; loss: 0.6905263066291809\n",
            "Iteration: 8986; loss: 0.6862162351608276\n",
            "Iteration: 8987; loss: 0.6909688711166382\n",
            "Iteration: 8988; loss: 0.6848520636558533\n",
            "Iteration: 8989; loss: 0.6880245804786682\n",
            "Iteration: 8990; loss: 0.6981072425842285\n",
            "Iteration: 8991; loss: 0.688846230506897\n",
            "Iteration: 8992; loss: 0.6953190565109253\n",
            "Iteration: 8993; loss: 0.6944946050643921\n",
            "Iteration: 8994; loss: 0.6889156699180603\n",
            "Iteration: 8995; loss: 0.6927400827407837\n",
            "Iteration: 8996; loss: 0.6910344958305359\n",
            "Iteration: 8997; loss: 0.6909680962562561\n",
            "Iteration: 8998; loss: 0.6882401704788208\n",
            "Iteration: 8999; loss: 0.694574773311615\n",
            "Iteration: 9000; loss: 0.6966471672058105\n",
            "Iteration: 9001; loss: 0.6900335550308228\n",
            "Iteration: 9002; loss: 0.6868578791618347\n",
            "Iteration: 9003; loss: 0.6967693567276001\n",
            "Iteration: 9004; loss: 0.6927131414413452\n",
            "Iteration: 9005; loss: 0.6886246800422668\n",
            "Iteration: 9006; loss: 0.689793586730957\n",
            "Iteration: 9007; loss: 0.6924433708190918\n",
            "Iteration: 9008; loss: 0.6940054893493652\n",
            "Iteration: 9009; loss: 0.6897059679031372\n",
            "Iteration: 9010; loss: 0.694227933883667\n",
            "Iteration: 9011; loss: 0.6954391598701477\n",
            "Iteration: 9012; loss: 0.6957389116287231\n",
            "Iteration: 9013; loss: 0.6911966800689697\n",
            "Iteration: 9014; loss: 0.6949256658554077\n",
            "Iteration: 9015; loss: 0.6911109685897827\n",
            "Iteration: 9016; loss: 0.6870365142822266\n",
            "Iteration: 9017; loss: 0.6922584772109985\n",
            "Iteration: 9018; loss: 0.6947342157363892\n",
            "Iteration: 9019; loss: 0.6906793713569641\n",
            "Iteration: 9020; loss: 0.6844092607498169\n",
            "Iteration: 9021; loss: 0.6867519021034241\n",
            "Iteration: 9022; loss: 0.6923655271530151\n",
            "Iteration: 9023; loss: 0.6921297311782837\n",
            "Iteration: 9024; loss: 0.6909511685371399\n",
            "Iteration: 9025; loss: 0.6857454776763916\n",
            "Iteration: 9026; loss: 0.693112850189209\n",
            "Iteration: 9027; loss: 0.6957384943962097\n",
            "Iteration: 9028; loss: 0.6896688938140869\n",
            "Iteration: 9029; loss: 0.6910146474838257\n",
            "Iteration: 9030; loss: 0.6856364607810974\n",
            "Iteration: 9031; loss: 0.6854836344718933\n",
            "Iteration: 9032; loss: 0.6929155588150024\n",
            "Iteration: 9033; loss: 0.6932633519172668\n",
            "Iteration: 9034; loss: 0.6918166875839233\n",
            "Iteration: 9035; loss: 0.6903587579727173\n",
            "Iteration: 9036; loss: 0.6926050782203674\n",
            "Iteration: 9037; loss: 0.6920724511146545\n",
            "Iteration: 9038; loss: 0.690565824508667\n",
            "Iteration: 9039; loss: 0.6888406872749329\n",
            "Iteration: 9040; loss: 0.6920312643051147\n",
            "Iteration: 9041; loss: 0.6968520879745483\n",
            "Iteration: 9042; loss: 0.6895073652267456\n",
            "Iteration: 9043; loss: 0.6893183588981628\n",
            "Iteration: 9044; loss: 0.6883345246315002\n",
            "Iteration: 9045; loss: 0.6917124390602112\n",
            "Iteration: 9046; loss: 0.6834932565689087\n",
            "Iteration: 9047; loss: 0.6942254900932312\n",
            "Iteration: 9048; loss: 0.6908750534057617\n",
            "Iteration: 9049; loss: 0.6956226825714111\n",
            "Iteration: 9050; loss: 0.6988722681999207\n",
            "Iteration: 9051; loss: 0.6910269260406494\n",
            "Iteration: 9052; loss: 0.6845754981040955\n",
            "Iteration: 9053; loss: 0.6909070014953613\n",
            "Iteration: 9054; loss: 0.695681095123291\n",
            "Iteration: 9055; loss: 0.6986153721809387\n",
            "Iteration: 9056; loss: 0.6874605417251587\n",
            "Iteration: 9057; loss: 0.7011808156967163\n",
            "Iteration: 9058; loss: 0.6882832646369934\n",
            "Iteration: 9059; loss: 0.6902614831924438\n",
            "Iteration: 9060; loss: 0.6882539987564087\n",
            "Iteration: 9061; loss: 0.6928203701972961\n",
            "Iteration: 9062; loss: 0.689197301864624\n",
            "Iteration: 9063; loss: 0.692499041557312\n",
            "Iteration: 9064; loss: 0.6888222694396973\n",
            "Iteration: 9065; loss: 0.6887481212615967\n",
            "Iteration: 9066; loss: 0.681398868560791\n",
            "Iteration: 9067; loss: 0.6968734860420227\n",
            "Iteration: 9068; loss: 0.6919177174568176\n",
            "Iteration: 9069; loss: 0.6976807713508606\n",
            "Iteration: 9070; loss: 0.6857901811599731\n",
            "Iteration: 9071; loss: 0.6937316060066223\n",
            "Iteration: 9072; loss: 0.685492753982544\n",
            "Iteration: 9073; loss: 0.6949011087417603\n",
            "Iteration: 9074; loss: 0.690189778804779\n",
            "Iteration: 9075; loss: 0.6927237510681152\n",
            "Iteration: 9076; loss: 0.6949958801269531\n",
            "Iteration: 9077; loss: 0.6918041706085205\n",
            "Iteration: 9078; loss: 0.6880242824554443\n",
            "Iteration: 9079; loss: 0.6953217387199402\n",
            "Iteration: 9080; loss: 0.6880907416343689\n",
            "Iteration: 9081; loss: 0.6930030584335327\n",
            "Iteration: 9082; loss: 0.69770348072052\n",
            "Iteration: 9083; loss: 0.6900961995124817\n",
            "Iteration: 9084; loss: 0.6896155476570129\n",
            "Iteration: 9085; loss: 0.6930525898933411\n",
            "Iteration: 9086; loss: 0.6907075643539429\n",
            "Iteration: 9087; loss: 0.6867480278015137\n",
            "Iteration: 9088; loss: 0.6916694641113281\n",
            "Iteration: 9089; loss: 0.6882344484329224\n",
            "Iteration: 9090; loss: 0.6918896436691284\n",
            "Iteration: 9091; loss: 0.6918070912361145\n",
            "Iteration: 9092; loss: 0.6935510039329529\n",
            "Iteration: 9093; loss: 0.6900044679641724\n",
            "Iteration: 9094; loss: 0.6884700059890747\n",
            "Iteration: 9095; loss: 0.6827151775360107\n",
            "Iteration: 9096; loss: 0.6930679678916931\n",
            "Iteration: 9097; loss: 0.6898435354232788\n",
            "Iteration: 9098; loss: 0.6932633519172668\n",
            "Iteration: 9099; loss: 0.6913073062896729\n",
            "Iteration: 9100; loss: 0.6909588575363159\n",
            "Iteration: 9101; loss: 0.6928371787071228\n",
            "Iteration: 9102; loss: 0.6949902772903442\n",
            "Iteration: 9103; loss: 0.6932163834571838\n",
            "Iteration: 9104; loss: 0.6898568272590637\n",
            "Iteration: 9105; loss: 0.6919141411781311\n",
            "Iteration: 9106; loss: 0.6943313479423523\n",
            "Iteration: 9107; loss: 0.693189799785614\n",
            "Iteration: 9108; loss: 0.6948494911193848\n",
            "Iteration: 9109; loss: 0.6875165104866028\n",
            "Iteration: 9110; loss: 0.6881981492042542\n",
            "Iteration: 9111; loss: 0.6920924186706543\n",
            "Iteration: 9112; loss: 0.691296398639679\n",
            "Iteration: 9113; loss: 0.6914670467376709\n",
            "Iteration: 9114; loss: 0.687521755695343\n",
            "Iteration: 9115; loss: 0.6890362501144409\n",
            "Iteration: 9116; loss: 0.6895604729652405\n",
            "Iteration: 9117; loss: 0.6825699210166931\n",
            "Iteration: 9118; loss: 0.6890180706977844\n",
            "Iteration: 9119; loss: 0.6955991387367249\n",
            "Iteration: 9120; loss: 0.6850485801696777\n",
            "Iteration: 9121; loss: 0.6900256872177124\n",
            "Iteration: 9122; loss: 0.6927316188812256\n",
            "Iteration: 9123; loss: 0.6931464076042175\n",
            "Iteration: 9124; loss: 0.6921983957290649\n",
            "Iteration: 9125; loss: 0.6877831220626831\n",
            "Iteration: 9126; loss: 0.6884270906448364\n",
            "Iteration: 9127; loss: 0.6861717104911804\n",
            "Iteration: 9128; loss: 0.6870654821395874\n",
            "Iteration: 9129; loss: 0.6857610940933228\n",
            "Iteration: 9130; loss: 0.6838524341583252\n",
            "Iteration: 9131; loss: 0.6846057176589966\n",
            "Iteration: 9132; loss: 0.6868037581443787\n",
            "Iteration: 9133; loss: 0.6916099190711975\n",
            "Iteration: 9134; loss: 0.6967639923095703\n",
            "Iteration: 9135; loss: 0.6996362805366516\n",
            "Iteration: 9136; loss: 0.6935983300209045\n",
            "Iteration: 9137; loss: 0.6901454329490662\n",
            "Iteration: 9138; loss: 0.6994442939758301\n",
            "Iteration: 9139; loss: 0.6856805682182312\n",
            "Iteration: 9140; loss: 0.6916732788085938\n",
            "Iteration: 9141; loss: 0.6829869151115417\n",
            "Iteration: 9142; loss: 0.6864721179008484\n",
            "Iteration: 9143; loss: 0.6926120519638062\n",
            "Iteration: 9144; loss: 0.6899646520614624\n",
            "Iteration: 9145; loss: 0.6949081420898438\n",
            "Iteration: 9146; loss: 0.6894094944000244\n",
            "Iteration: 9147; loss: 0.6865342855453491\n",
            "Iteration: 9148; loss: 0.6886581182479858\n",
            "Iteration: 9149; loss: 0.6917741298675537\n",
            "Iteration: 9150; loss: 0.6898257732391357\n",
            "Iteration: 9151; loss: 0.6876306533813477\n",
            "Iteration: 9152; loss: 0.6947764754295349\n",
            "Iteration: 9153; loss: 0.6865019202232361\n",
            "Iteration: 9154; loss: 0.6938515901565552\n",
            "Iteration: 9155; loss: 0.6861484050750732\n",
            "Iteration: 9156; loss: 0.6825674772262573\n",
            "Iteration: 9157; loss: 0.6895795464515686\n",
            "Iteration: 9158; loss: 0.6972945332527161\n",
            "Iteration: 9159; loss: 0.6880471110343933\n",
            "Iteration: 9160; loss: 0.6767949461936951\n",
            "Iteration: 9161; loss: 0.6850889921188354\n",
            "Iteration: 9162; loss: 0.6926010847091675\n",
            "Iteration: 9163; loss: 0.6883918642997742\n",
            "Iteration: 9164; loss: 0.6935847401618958\n",
            "Iteration: 9165; loss: 0.6857842206954956\n",
            "Iteration: 9166; loss: 0.6836563944816589\n",
            "Iteration: 9167; loss: 0.6887845396995544\n",
            "Iteration: 9168; loss: 0.6899259090423584\n",
            "Iteration: 9169; loss: 0.6862996816635132\n",
            "Iteration: 9170; loss: 0.6930243968963623\n",
            "Iteration: 9171; loss: 0.6882908344268799\n",
            "Iteration: 9172; loss: 0.6894546151161194\n",
            "Iteration: 9173; loss: 0.6907125115394592\n",
            "Iteration: 9174; loss: 0.6910868883132935\n",
            "Iteration: 9175; loss: 0.6976047158241272\n",
            "Iteration: 9176; loss: 0.6890839338302612\n",
            "Iteration: 9177; loss: 0.6894033551216125\n",
            "Iteration: 9178; loss: 0.6907185316085815\n",
            "Iteration: 9179; loss: 0.6891223788261414\n",
            "Iteration: 9180; loss: 0.6952274441719055\n",
            "Iteration: 9181; loss: 0.6828126907348633\n",
            "Iteration: 9182; loss: 0.6915715336799622\n",
            "Iteration: 9183; loss: 0.692284345626831\n",
            "Iteration: 9184; loss: 0.6897532939910889\n",
            "Iteration: 9185; loss: 0.686440646648407\n",
            "Iteration: 9186; loss: 0.687363862991333\n",
            "Iteration: 9187; loss: 0.6803628206253052\n",
            "Iteration: 9188; loss: 0.692970871925354\n",
            "Iteration: 9189; loss: 0.6878090500831604\n",
            "Iteration: 9190; loss: 0.700346052646637\n",
            "Iteration: 9191; loss: 0.687431275844574\n",
            "Iteration: 9192; loss: 0.6992394924163818\n",
            "Iteration: 9193; loss: 0.6877180337905884\n",
            "Iteration: 9194; loss: 0.689939558506012\n",
            "Iteration: 9195; loss: 0.6929275989532471\n",
            "Iteration: 9196; loss: 0.6952200531959534\n",
            "Iteration: 9197; loss: 0.6913314461708069\n",
            "Iteration: 9198; loss: 0.6868556141853333\n",
            "Iteration: 9199; loss: 0.6930343508720398\n",
            "Iteration: 9200; loss: 0.6886913776397705\n",
            "Iteration: 9201; loss: 0.6799502968788147\n",
            "Iteration: 9202; loss: 0.6883114576339722\n",
            "Iteration: 9203; loss: 0.6940007209777832\n",
            "Iteration: 9204; loss: 0.6828421950340271\n",
            "Iteration: 9205; loss: 0.6965258717536926\n",
            "Iteration: 9206; loss: 0.6945867538452148\n",
            "Iteration: 9207; loss: 0.6945127844810486\n",
            "Iteration: 9208; loss: 0.6900601983070374\n",
            "Iteration: 9209; loss: 0.6856345534324646\n",
            "Iteration: 9210; loss: 0.6921727061271667\n",
            "Iteration: 9211; loss: 0.6932196617126465\n",
            "Iteration: 9212; loss: 0.690144956111908\n",
            "Iteration: 9213; loss: 0.6873035430908203\n",
            "Iteration: 9214; loss: 0.6912729144096375\n",
            "Iteration: 9215; loss: 0.6854819059371948\n",
            "Iteration: 9216; loss: 0.6923013925552368\n",
            "Iteration: 9217; loss: 0.6859148144721985\n",
            "Iteration: 9218; loss: 0.6881151795387268\n",
            "Iteration: 9219; loss: 0.699691891670227\n",
            "Iteration: 9220; loss: 0.6855851411819458\n",
            "Iteration: 9221; loss: 0.6873636245727539\n",
            "Iteration: 9222; loss: 0.6922867894172668\n",
            "Iteration: 9223; loss: 0.6921898126602173\n",
            "Iteration: 9224; loss: 0.6863849759101868\n",
            "Iteration: 9225; loss: 0.6892099380493164\n",
            "Iteration: 9226; loss: 0.6927523612976074\n",
            "Iteration: 9227; loss: 0.6937156319618225\n",
            "Iteration: 9228; loss: 0.6934024691581726\n",
            "Iteration: 9229; loss: 0.6909055709838867\n",
            "Iteration: 9230; loss: 0.6845681071281433\n",
            "Iteration: 9231; loss: 0.6923469305038452\n",
            "Iteration: 9232; loss: 0.6824815273284912\n",
            "Iteration: 9233; loss: 0.695614218711853\n",
            "Iteration: 9234; loss: 0.6953427791595459\n",
            "Iteration: 9235; loss: 0.6910064220428467\n",
            "Iteration: 9236; loss: 0.6928983926773071\n",
            "Iteration: 9237; loss: 0.6873822212219238\n",
            "Iteration: 9238; loss: 0.6954227089881897\n",
            "Iteration: 9239; loss: 0.6972226500511169\n",
            "Iteration: 9240; loss: 0.697352409362793\n",
            "Iteration: 9241; loss: 0.6969907283782959\n",
            "Iteration: 9242; loss: 0.6920593976974487\n",
            "Iteration: 9243; loss: 0.6902238130569458\n",
            "Iteration: 9244; loss: 0.6848543882369995\n",
            "Iteration: 9245; loss: 0.6930885314941406\n",
            "Iteration: 9246; loss: 0.6861148476600647\n",
            "Iteration: 9247; loss: 0.6776105761528015\n",
            "Iteration: 9248; loss: 0.6836565136909485\n",
            "Iteration: 9249; loss: 0.685141384601593\n",
            "Iteration: 9250; loss: 0.6911451816558838\n",
            "Iteration: 9251; loss: 0.6955052614212036\n",
            "Iteration: 9252; loss: 0.6863247156143188\n",
            "Iteration: 9253; loss: 0.6985060572624207\n",
            "Iteration: 9254; loss: 0.6883630752563477\n",
            "Iteration: 9255; loss: 0.6945850253105164\n",
            "Iteration: 9256; loss: 0.6985442042350769\n",
            "Iteration: 9257; loss: 0.6943987011909485\n",
            "Iteration: 9258; loss: 0.6869723796844482\n",
            "Iteration: 9259; loss: 0.6919905543327332\n",
            "Iteration: 9260; loss: 0.6946296691894531\n",
            "Iteration: 9261; loss: 0.6858679056167603\n",
            "Iteration: 9262; loss: 0.686424732208252\n",
            "Iteration: 9263; loss: 0.6909382939338684\n",
            "Iteration: 9264; loss: 0.6989127993583679\n",
            "Iteration: 9265; loss: 0.6944087743759155\n",
            "Iteration: 9266; loss: 0.6892272233963013\n",
            "Iteration: 9267; loss: 0.6929233074188232\n",
            "Iteration: 9268; loss: 0.6964218020439148\n",
            "Iteration: 9269; loss: 0.685077965259552\n",
            "Iteration: 9270; loss: 0.6942245960235596\n",
            "Iteration: 9271; loss: 0.6975281238555908\n",
            "Iteration: 9272; loss: 0.6862671375274658\n",
            "Iteration: 9273; loss: 0.6913865804672241\n",
            "Iteration: 9274; loss: 0.6889159679412842\n",
            "Iteration: 9275; loss: 0.6951331496238708\n",
            "Iteration: 9276; loss: 0.6928558349609375\n",
            "Iteration: 9277; loss: 0.6966237425804138\n",
            "Iteration: 9278; loss: 0.6942182183265686\n",
            "Iteration: 9279; loss: 0.6774513721466064\n",
            "Iteration: 9280; loss: 0.7008901238441467\n",
            "Iteration: 9281; loss: 0.6887582540512085\n",
            "Iteration: 9282; loss: 0.6891615390777588\n",
            "Iteration: 9283; loss: 0.6948831081390381\n",
            "Iteration: 9284; loss: 0.690707266330719\n",
            "Iteration: 9285; loss: 0.6935577392578125\n",
            "Iteration: 9286; loss: 0.6886781454086304\n",
            "Iteration: 9287; loss: 0.6880903840065002\n",
            "Iteration: 9288; loss: 0.682257890701294\n",
            "Iteration: 9289; loss: 0.6908129453659058\n",
            "Iteration: 9290; loss: 0.6856225728988647\n",
            "Iteration: 9291; loss: 0.6906523704528809\n",
            "Iteration: 9292; loss: 0.6854626536369324\n",
            "Iteration: 9293; loss: 0.6767593622207642\n",
            "Iteration: 9294; loss: 0.6817766427993774\n",
            "Iteration: 9295; loss: 0.6903833746910095\n",
            "Iteration: 9296; loss: 0.6913472414016724\n",
            "Iteration: 9297; loss: 0.6943411827087402\n",
            "Iteration: 9298; loss: 0.6884264349937439\n",
            "Iteration: 9299; loss: 0.6947866678237915\n",
            "Iteration: 9300; loss: 0.6900985240936279\n",
            "Iteration: 9301; loss: 0.6788690686225891\n",
            "Iteration: 9302; loss: 0.689565122127533\n",
            "Iteration: 9303; loss: 0.6892364621162415\n",
            "Iteration: 9304; loss: 0.6886522769927979\n",
            "Iteration: 9305; loss: 0.6895189881324768\n",
            "Iteration: 9306; loss: 0.6869674921035767\n",
            "Iteration: 9307; loss: 0.6900491118431091\n",
            "Iteration: 9308; loss: 0.695716917514801\n",
            "Iteration: 9309; loss: 0.6960992813110352\n",
            "Iteration: 9310; loss: 0.6911250948905945\n",
            "Iteration: 9311; loss: 0.6951075792312622\n",
            "Iteration: 9312; loss: 0.6845933794975281\n",
            "Iteration: 9313; loss: 0.6965065598487854\n",
            "Iteration: 9314; loss: 0.6882036328315735\n",
            "Iteration: 9315; loss: 0.6870327591896057\n",
            "Iteration: 9316; loss: 0.6853768825531006\n",
            "Iteration: 9317; loss: 0.690101683139801\n",
            "Iteration: 9318; loss: 0.6872100830078125\n",
            "Iteration: 9319; loss: 0.6946044564247131\n",
            "Iteration: 9320; loss: 0.6899400353431702\n",
            "Iteration: 9321; loss: 0.6917140483856201\n",
            "Iteration: 9322; loss: 0.6881962418556213\n",
            "Iteration: 9323; loss: 0.6961528658866882\n",
            "Iteration: 9324; loss: 0.6890969276428223\n",
            "Iteration: 9325; loss: 0.6933190822601318\n",
            "Iteration: 9326; loss: 0.6924311518669128\n",
            "Iteration: 9327; loss: 0.6826580166816711\n",
            "Iteration: 9328; loss: 0.6862969994544983\n",
            "Iteration: 9329; loss: 0.6876179575920105\n",
            "Iteration: 9330; loss: 0.692775309085846\n",
            "Iteration: 9331; loss: 0.6881354451179504\n",
            "Iteration: 9332; loss: 0.684080183506012\n",
            "Iteration: 9333; loss: 0.6908567547798157\n",
            "Iteration: 9334; loss: 0.6823892593383789\n",
            "Iteration: 9335; loss: 0.6861875653266907\n",
            "Iteration: 9336; loss: 0.6945770382881165\n",
            "Iteration: 9337; loss: 0.694084644317627\n",
            "Iteration: 9338; loss: 0.697441816329956\n",
            "Iteration: 9339; loss: 0.6887133121490479\n",
            "Iteration: 9340; loss: 0.691342294216156\n",
            "Iteration: 9341; loss: 0.6942355036735535\n",
            "Iteration: 9342; loss: 0.6819454431533813\n",
            "Iteration: 9343; loss: 0.6924625039100647\n",
            "Iteration: 9344; loss: 0.6914093494415283\n",
            "Iteration: 9345; loss: 0.6938998103141785\n",
            "Iteration: 9346; loss: 0.6892174482345581\n",
            "Iteration: 9347; loss: 0.6902492046356201\n",
            "Iteration: 9348; loss: 0.6871913075447083\n",
            "Iteration: 9349; loss: 0.6863030195236206\n",
            "Iteration: 9350; loss: 0.6908274292945862\n",
            "Iteration: 9351; loss: 0.694972574710846\n",
            "Iteration: 9352; loss: 0.6925352811813354\n",
            "Iteration: 9353; loss: 0.6923561096191406\n",
            "Iteration: 9354; loss: 0.6827674508094788\n",
            "Iteration: 9355; loss: 0.6917144060134888\n",
            "Iteration: 9356; loss: 0.6906042695045471\n",
            "Iteration: 9357; loss: 0.6939636468887329\n",
            "Iteration: 9358; loss: 0.6866191625595093\n",
            "Iteration: 9359; loss: 0.6823015809059143\n",
            "Iteration: 9360; loss: 0.6873421669006348\n",
            "Iteration: 9361; loss: 0.6910812854766846\n",
            "Iteration: 9362; loss: 0.6965513825416565\n",
            "Iteration: 9363; loss: 0.6911632418632507\n",
            "Iteration: 9364; loss: 0.6856427192687988\n",
            "Iteration: 9365; loss: 0.6868606805801392\n",
            "Iteration: 9366; loss: 0.6860551238059998\n",
            "Iteration: 9367; loss: 0.6897140145301819\n",
            "Iteration: 9368; loss: 0.6921010613441467\n",
            "Iteration: 9369; loss: 0.6859550476074219\n",
            "Iteration: 9370; loss: 0.6859986782073975\n",
            "Iteration: 9371; loss: 0.6974697113037109\n",
            "Iteration: 9372; loss: 0.6911864280700684\n",
            "Iteration: 9373; loss: 0.6920639872550964\n",
            "Iteration: 9374; loss: 0.6943762302398682\n",
            "Iteration: 9375; loss: 0.6893591284751892\n",
            "Iteration: 9376; loss: 0.6996164917945862\n",
            "Iteration: 9377; loss: 0.6920650005340576\n",
            "Iteration: 9378; loss: 0.6874163746833801\n",
            "Iteration: 9379; loss: 0.6801707744598389\n",
            "Iteration: 9380; loss: 0.6909472346305847\n",
            "Iteration: 9381; loss: 0.6888543367385864\n",
            "Iteration: 9382; loss: 0.691318929195404\n",
            "Iteration: 9383; loss: 0.6848354935646057\n",
            "Iteration: 9384; loss: 0.6875118017196655\n",
            "Iteration: 9385; loss: 0.6901496052742004\n",
            "Iteration: 9386; loss: 0.6846048831939697\n",
            "Iteration: 9387; loss: 0.6809203624725342\n",
            "Iteration: 9388; loss: 0.6866886019706726\n",
            "Iteration: 9389; loss: 0.6979865431785583\n",
            "Iteration: 9390; loss: 0.6876107454299927\n",
            "Iteration: 9391; loss: 0.6821808218955994\n",
            "Iteration: 9392; loss: 0.6955347061157227\n",
            "Iteration: 9393; loss: 0.6935256719589233\n",
            "Iteration: 9394; loss: 0.692126989364624\n",
            "Iteration: 9395; loss: 0.6842028498649597\n",
            "Iteration: 9396; loss: 0.6851476430892944\n",
            "Iteration: 9397; loss: 0.6899119019508362\n",
            "Iteration: 9398; loss: 0.6868875026702881\n",
            "Iteration: 9399; loss: 0.6852492690086365\n",
            "Iteration: 9400; loss: 0.6927037835121155\n",
            "Iteration: 9401; loss: 0.6893601417541504\n",
            "Iteration: 9402; loss: 0.6940219402313232\n",
            "Iteration: 9403; loss: 0.6871971487998962\n",
            "Iteration: 9404; loss: 0.691068172454834\n",
            "Iteration: 9405; loss: 0.6946713924407959\n",
            "Iteration: 9406; loss: 0.6909121870994568\n",
            "Iteration: 9407; loss: 0.6936948895454407\n",
            "Iteration: 9408; loss: 0.6871601939201355\n",
            "Iteration: 9409; loss: 0.6911874413490295\n",
            "Iteration: 9410; loss: 0.686768114566803\n",
            "Iteration: 9411; loss: 0.6852225065231323\n",
            "Iteration: 9412; loss: 0.6926165223121643\n",
            "Iteration: 9413; loss: 0.6973286867141724\n",
            "Iteration: 9414; loss: 0.6842930316925049\n",
            "Iteration: 9415; loss: 0.6852742433547974\n",
            "Iteration: 9416; loss: 0.6852328181266785\n",
            "Iteration: 9417; loss: 0.6893104314804077\n",
            "Iteration: 9418; loss: 0.6891117095947266\n",
            "Iteration: 9419; loss: 0.6763407588005066\n",
            "Iteration: 9420; loss: 0.6906446814537048\n",
            "Iteration: 9421; loss: 0.6912769079208374\n",
            "Iteration: 9422; loss: 0.6975606679916382\n",
            "Iteration: 9423; loss: 0.692753791809082\n",
            "Iteration: 9424; loss: 0.685634195804596\n",
            "Iteration: 9425; loss: 0.6765210628509521\n",
            "Iteration: 9426; loss: 0.683952271938324\n",
            "Iteration: 9427; loss: 0.6901170611381531\n",
            "Iteration: 9428; loss: 0.6893247961997986\n",
            "Iteration: 9429; loss: 0.6927428841590881\n",
            "Iteration: 9430; loss: 0.6967415809631348\n",
            "Iteration: 9431; loss: 0.6810893416404724\n",
            "Iteration: 9432; loss: 0.6794396638870239\n",
            "Iteration: 9433; loss: 0.6932371854782104\n",
            "Iteration: 9434; loss: 0.6894423961639404\n",
            "Iteration: 9435; loss: 0.6883059740066528\n",
            "Iteration: 9436; loss: 0.6899958252906799\n",
            "Iteration: 9437; loss: 0.7032840251922607\n",
            "Iteration: 9438; loss: 0.6872448325157166\n",
            "Iteration: 9439; loss: 0.693049430847168\n",
            "Iteration: 9440; loss: 0.6879993677139282\n",
            "Iteration: 9441; loss: 0.6914202570915222\n",
            "Iteration: 9442; loss: 0.6935614347457886\n",
            "Iteration: 9443; loss: 0.6802132725715637\n",
            "Iteration: 9444; loss: 0.6945914030075073\n",
            "Iteration: 9445; loss: 0.6899820566177368\n",
            "Iteration: 9446; loss: 0.6895231008529663\n",
            "Iteration: 9447; loss: 0.6913530826568604\n",
            "Iteration: 9448; loss: 0.6886029243469238\n",
            "Iteration: 9449; loss: 0.6890391707420349\n",
            "Iteration: 9450; loss: 0.6881365776062012\n",
            "Iteration: 9451; loss: 0.6850167512893677\n",
            "Iteration: 9452; loss: 0.6852949261665344\n",
            "Iteration: 9453; loss: 0.6928972005844116\n",
            "Iteration: 9454; loss: 0.6816352605819702\n",
            "Iteration: 9455; loss: 0.6896661520004272\n",
            "Iteration: 9456; loss: 0.6809288263320923\n",
            "Iteration: 9457; loss: 0.6874150037765503\n",
            "Iteration: 9458; loss: 0.6892299056053162\n",
            "Iteration: 9459; loss: 0.7005617022514343\n",
            "Iteration: 9460; loss: 0.6867924332618713\n",
            "Iteration: 9461; loss: 0.6967589855194092\n",
            "Iteration: 9462; loss: 0.6864396929740906\n",
            "Iteration: 9463; loss: 0.6882556676864624\n",
            "Iteration: 9464; loss: 0.6907268166542053\n",
            "Iteration: 9465; loss: 0.6919565200805664\n",
            "Iteration: 9466; loss: 0.6894833445549011\n",
            "Iteration: 9467; loss: 0.6974765658378601\n",
            "Iteration: 9468; loss: 0.6823588609695435\n",
            "Iteration: 9469; loss: 0.6924609541893005\n",
            "Iteration: 9470; loss: 0.6908437609672546\n",
            "Iteration: 9471; loss: 0.6871495842933655\n",
            "Iteration: 9472; loss: 0.6846269369125366\n",
            "Iteration: 9473; loss: 0.6858229041099548\n",
            "Iteration: 9474; loss: 0.6962940096855164\n",
            "Iteration: 9475; loss: 0.6880538463592529\n",
            "Iteration: 9476; loss: 0.6875618696212769\n",
            "Iteration: 9477; loss: 0.6925988793373108\n",
            "Iteration: 9478; loss: 0.6886430382728577\n",
            "Iteration: 9479; loss: 0.6981215476989746\n",
            "Iteration: 9480; loss: 0.6991294622421265\n",
            "Iteration: 9481; loss: 0.681142270565033\n",
            "Iteration: 9482; loss: 0.6865482926368713\n",
            "Iteration: 9483; loss: 0.6903188228607178\n",
            "Iteration: 9484; loss: 0.6869655847549438\n",
            "Iteration: 9485; loss: 0.6789782643318176\n",
            "Iteration: 9486; loss: 0.6846140623092651\n",
            "Iteration: 9487; loss: 0.6907703876495361\n",
            "Iteration: 9488; loss: 0.688312292098999\n",
            "Iteration: 9489; loss: 0.69087815284729\n",
            "Iteration: 9490; loss: 0.6949211955070496\n",
            "Iteration: 9491; loss: 0.6972808837890625\n",
            "Iteration: 9492; loss: 0.6933205127716064\n",
            "Iteration: 9493; loss: 0.6883279085159302\n",
            "Iteration: 9494; loss: 0.6914350986480713\n",
            "Iteration: 9495; loss: 0.6821058392524719\n",
            "Iteration: 9496; loss: 0.6868100166320801\n",
            "Iteration: 9497; loss: 0.6878310441970825\n",
            "Iteration: 9498; loss: 0.6824270486831665\n",
            "Iteration: 9499; loss: 0.6946655511856079\n",
            "Iteration: 9500; loss: 0.6914811730384827\n",
            "Iteration: 9501; loss: 0.6872864365577698\n",
            "Iteration: 9502; loss: 0.6831759214401245\n",
            "Iteration: 9503; loss: 0.6857643723487854\n",
            "Iteration: 9504; loss: 0.6899675130844116\n",
            "Iteration: 9505; loss: 0.6875019073486328\n",
            "Iteration: 9506; loss: 0.6938284039497375\n",
            "Iteration: 9507; loss: 0.6808125972747803\n",
            "Iteration: 9508; loss: 0.6903814077377319\n",
            "Iteration: 9509; loss: 0.6966242790222168\n",
            "Iteration: 9510; loss: 0.6795716285705566\n",
            "Iteration: 9511; loss: 0.6877564787864685\n",
            "Iteration: 9512; loss: 0.6827929019927979\n",
            "Iteration: 9513; loss: 0.6788018345832825\n",
            "Iteration: 9514; loss: 0.6889933943748474\n",
            "Iteration: 9515; loss: 0.7009345293045044\n",
            "Iteration: 9516; loss: 0.6838310956954956\n",
            "Iteration: 9517; loss: 0.6971551179885864\n",
            "Iteration: 9518; loss: 0.6935878396034241\n",
            "Iteration: 9519; loss: 0.685148298740387\n",
            "Iteration: 9520; loss: 0.691588282585144\n",
            "Iteration: 9521; loss: 0.6860897541046143\n",
            "Iteration: 9522; loss: 0.6911545991897583\n",
            "Iteration: 9523; loss: 0.6818556785583496\n",
            "Iteration: 9524; loss: 0.6862970590591431\n",
            "Iteration: 9525; loss: 0.6901512742042542\n",
            "Iteration: 9526; loss: 0.6875575184822083\n",
            "Iteration: 9527; loss: 0.6868234872817993\n",
            "Iteration: 9528; loss: 0.6876893639564514\n",
            "Iteration: 9529; loss: 0.6910310983657837\n",
            "Iteration: 9530; loss: 0.6877852082252502\n",
            "Iteration: 9531; loss: 0.6927326917648315\n",
            "Iteration: 9532; loss: 0.6851527690887451\n",
            "Iteration: 9533; loss: 0.6956497430801392\n",
            "Iteration: 9534; loss: 0.6847437620162964\n",
            "Iteration: 9535; loss: 0.6874811053276062\n",
            "Iteration: 9536; loss: 0.6865702867507935\n",
            "Iteration: 9537; loss: 0.6984351873397827\n",
            "Iteration: 9538; loss: 0.6849886178970337\n",
            "Iteration: 9539; loss: 0.6929694414138794\n",
            "Iteration: 9540; loss: 0.6946929097175598\n",
            "Iteration: 9541; loss: 0.6882873773574829\n",
            "Iteration: 9542; loss: 0.6863699555397034\n",
            "Iteration: 9543; loss: 0.6858213543891907\n",
            "Iteration: 9544; loss: 0.6877540349960327\n",
            "Iteration: 9545; loss: 0.6975922584533691\n",
            "Iteration: 9546; loss: 0.6949341893196106\n",
            "Iteration: 9547; loss: 0.6864020824432373\n",
            "Iteration: 9548; loss: 0.6846638917922974\n",
            "Iteration: 9549; loss: 0.6817578673362732\n",
            "Iteration: 9550; loss: 0.691510796546936\n",
            "Iteration: 9551; loss: 0.6848180890083313\n",
            "Iteration: 9552; loss: 0.6942452788352966\n",
            "Iteration: 9553; loss: 0.6882826089859009\n",
            "Iteration: 9554; loss: 0.6885254383087158\n",
            "Iteration: 9555; loss: 0.6860334277153015\n",
            "Iteration: 9556; loss: 0.6966460943222046\n",
            "Iteration: 9557; loss: 0.6850603222846985\n",
            "Iteration: 9558; loss: 0.6868390440940857\n",
            "Iteration: 9559; loss: 0.7028412818908691\n",
            "Iteration: 9560; loss: 0.6966450810432434\n",
            "Iteration: 9561; loss: 0.6886395215988159\n",
            "Iteration: 9562; loss: 0.6885566115379333\n",
            "Iteration: 9563; loss: 0.6870226860046387\n",
            "Iteration: 9564; loss: 0.6825617551803589\n",
            "Iteration: 9565; loss: 0.6836662888526917\n",
            "Iteration: 9566; loss: 0.6946451663970947\n",
            "Iteration: 9567; loss: 0.68936687707901\n",
            "Iteration: 9568; loss: 0.6928004026412964\n",
            "Iteration: 9569; loss: 0.6877596378326416\n",
            "Iteration: 9570; loss: 0.6884880065917969\n",
            "Iteration: 9571; loss: 0.6873769760131836\n",
            "Iteration: 9572; loss: 0.6782244443893433\n",
            "Iteration: 9573; loss: 0.6859877705574036\n",
            "Iteration: 9574; loss: 0.6938948631286621\n",
            "Iteration: 9575; loss: 0.68876713514328\n",
            "Iteration: 9576; loss: 0.6904199719429016\n",
            "Iteration: 9577; loss: 0.6962976455688477\n",
            "Iteration: 9578; loss: 0.6876673698425293\n",
            "Iteration: 9579; loss: 0.6970421671867371\n",
            "Iteration: 9580; loss: 0.6764770746231079\n",
            "Iteration: 9581; loss: 0.6930821537971497\n",
            "Iteration: 9582; loss: 0.6925694346427917\n",
            "Iteration: 9583; loss: 0.6910027265548706\n",
            "Iteration: 9584; loss: 0.6884257197380066\n",
            "Iteration: 9585; loss: 0.6848351359367371\n",
            "Iteration: 9586; loss: 0.6975045204162598\n",
            "Iteration: 9587; loss: 0.6884647011756897\n",
            "Iteration: 9588; loss: 0.6877028346061707\n",
            "Iteration: 9589; loss: 0.6792145371437073\n",
            "Iteration: 9590; loss: 0.6773383617401123\n",
            "Iteration: 9591; loss: 0.6972745060920715\n",
            "Iteration: 9592; loss: 0.6872127056121826\n",
            "Iteration: 9593; loss: 0.6881532669067383\n",
            "Iteration: 9594; loss: 0.68894362449646\n",
            "Iteration: 9595; loss: 0.6933873891830444\n",
            "Iteration: 9596; loss: 0.6912785172462463\n",
            "Iteration: 9597; loss: 0.6934274435043335\n",
            "Iteration: 9598; loss: 0.6837520599365234\n",
            "Iteration: 9599; loss: 0.6897940635681152\n",
            "Iteration: 9600; loss: 0.6885564923286438\n",
            "Iteration: 9601; loss: 0.682954728603363\n",
            "Iteration: 9602; loss: 0.688290536403656\n",
            "Iteration: 9603; loss: 0.6931211352348328\n",
            "Iteration: 9604; loss: 0.6801445484161377\n",
            "Iteration: 9605; loss: 0.6925095319747925\n",
            "Iteration: 9606; loss: 0.6873420476913452\n",
            "Iteration: 9607; loss: 0.691521406173706\n",
            "Iteration: 9608; loss: 0.6872717142105103\n",
            "Iteration: 9609; loss: 0.680752158164978\n",
            "Iteration: 9610; loss: 0.6889164447784424\n",
            "Iteration: 9611; loss: 0.6805592775344849\n",
            "Iteration: 9612; loss: 0.6864633560180664\n",
            "Iteration: 9613; loss: 0.6866112947463989\n",
            "Iteration: 9614; loss: 0.6850440502166748\n",
            "Iteration: 9615; loss: 0.6934144496917725\n",
            "Iteration: 9616; loss: 0.6913493275642395\n",
            "Iteration: 9617; loss: 0.6795175671577454\n",
            "Iteration: 9618; loss: 0.6997641324996948\n",
            "Iteration: 9619; loss: 0.6942720413208008\n",
            "Iteration: 9620; loss: 0.6861372590065002\n",
            "Iteration: 9621; loss: 0.6906841993331909\n",
            "Iteration: 9622; loss: 0.6827803254127502\n",
            "Iteration: 9623; loss: 0.6941014528274536\n",
            "Iteration: 9624; loss: 0.6797261238098145\n",
            "Iteration: 9625; loss: 0.6972770690917969\n",
            "Iteration: 9626; loss: 0.6870657205581665\n",
            "Iteration: 9627; loss: 0.6943671703338623\n",
            "Iteration: 9628; loss: 0.6911113858222961\n",
            "Iteration: 9629; loss: 0.685019850730896\n",
            "Iteration: 9630; loss: 0.6886348128318787\n",
            "Iteration: 9631; loss: 0.6964285969734192\n",
            "Iteration: 9632; loss: 0.6887056827545166\n",
            "Iteration: 9633; loss: 0.6935268640518188\n",
            "Iteration: 9634; loss: 0.7004166841506958\n",
            "Iteration: 9635; loss: 0.6775504350662231\n",
            "Iteration: 9636; loss: 0.6864906549453735\n",
            "Iteration: 9637; loss: 0.6943331360816956\n",
            "Iteration: 9638; loss: 0.6878263354301453\n",
            "Iteration: 9639; loss: 0.6885839104652405\n",
            "Iteration: 9640; loss: 0.68971186876297\n",
            "Iteration: 9641; loss: 0.6787460446357727\n",
            "Iteration: 9642; loss: 0.6887068152427673\n",
            "Iteration: 9643; loss: 0.6943249702453613\n",
            "Iteration: 9644; loss: 0.7002208828926086\n",
            "Iteration: 9645; loss: 0.691031813621521\n",
            "Iteration: 9646; loss: 0.6811635494232178\n",
            "Iteration: 9647; loss: 0.6882116794586182\n",
            "Iteration: 9648; loss: 0.680675745010376\n",
            "Iteration: 9649; loss: 0.6876472234725952\n",
            "Iteration: 9650; loss: 0.6863523125648499\n",
            "Iteration: 9651; loss: 0.6773958206176758\n",
            "Iteration: 9652; loss: 0.694927990436554\n",
            "Iteration: 9653; loss: 0.6814982295036316\n",
            "Iteration: 9654; loss: 0.6998997330665588\n",
            "Iteration: 9655; loss: 0.6892144083976746\n",
            "Iteration: 9656; loss: 0.6909738183021545\n",
            "Iteration: 9657; loss: 0.6784653663635254\n",
            "Iteration: 9658; loss: 0.6939849257469177\n",
            "Iteration: 9659; loss: 0.6847395896911621\n",
            "Iteration: 9660; loss: 0.691077470779419\n",
            "Iteration: 9661; loss: 0.6900177001953125\n",
            "Iteration: 9662; loss: 0.6917763352394104\n",
            "Iteration: 9663; loss: 0.6849217414855957\n",
            "Iteration: 9664; loss: 0.6903758645057678\n",
            "Iteration: 9665; loss: 0.6924910545349121\n",
            "Iteration: 9666; loss: 0.6819420456886292\n",
            "Iteration: 9667; loss: 0.6806321144104004\n",
            "Iteration: 9668; loss: 0.686686098575592\n",
            "Iteration: 9669; loss: 0.688835859298706\n",
            "Iteration: 9670; loss: 0.6833159327507019\n",
            "Iteration: 9671; loss: 0.6807559132575989\n",
            "Iteration: 9672; loss: 0.6892825365066528\n",
            "Iteration: 9673; loss: 0.6810574531555176\n",
            "Iteration: 9674; loss: 0.6907243728637695\n",
            "Iteration: 9675; loss: 0.6900610327720642\n",
            "Iteration: 9676; loss: 0.6931380033493042\n",
            "Iteration: 9677; loss: 0.6877827644348145\n",
            "Iteration: 9678; loss: 0.701288104057312\n",
            "Iteration: 9679; loss: 0.6998581290245056\n",
            "Iteration: 9680; loss: 0.6825246214866638\n",
            "Iteration: 9681; loss: 0.6771740913391113\n",
            "Iteration: 9682; loss: 0.6978781223297119\n",
            "Iteration: 9683; loss: 0.6920126080513\n",
            "Iteration: 9684; loss: 0.6892420649528503\n",
            "Iteration: 9685; loss: 0.6856278777122498\n",
            "Iteration: 9686; loss: 0.6857049465179443\n",
            "Iteration: 9687; loss: 0.6955236792564392\n",
            "Iteration: 9688; loss: 0.6945099830627441\n",
            "Iteration: 9689; loss: 0.6813763976097107\n",
            "Iteration: 9690; loss: 0.6948553919792175\n",
            "Iteration: 9691; loss: 0.6899771094322205\n",
            "Iteration: 9692; loss: 0.6876816153526306\n",
            "Iteration: 9693; loss: 0.6793040633201599\n",
            "Iteration: 9694; loss: 0.6813551783561707\n",
            "Iteration: 9695; loss: 0.6923878788948059\n",
            "Iteration: 9696; loss: 0.6829333305358887\n",
            "Iteration: 9697; loss: 0.6873044967651367\n",
            "Iteration: 9698; loss: 0.6838407516479492\n",
            "Iteration: 9699; loss: 0.6839054822921753\n",
            "Iteration: 9700; loss: 0.6925139427185059\n",
            "Iteration: 9701; loss: 0.683309018611908\n",
            "Iteration: 9702; loss: 0.6866413950920105\n",
            "Iteration: 9703; loss: 0.7006287574768066\n",
            "Iteration: 9704; loss: 0.6797237992286682\n",
            "Iteration: 9705; loss: 0.6955471038818359\n",
            "Iteration: 9706; loss: 0.6879603266716003\n",
            "Iteration: 9707; loss: 0.6848523020744324\n",
            "Iteration: 9708; loss: 0.6884813904762268\n",
            "Iteration: 9709; loss: 0.6849121451377869\n",
            "Iteration: 9710; loss: 0.6806017160415649\n",
            "Iteration: 9711; loss: 0.6780146360397339\n",
            "Iteration: 9712; loss: 0.6977648138999939\n",
            "Iteration: 9713; loss: 0.683312714099884\n",
            "Iteration: 9714; loss: 0.68720942735672\n",
            "Iteration: 9715; loss: 0.6953425407409668\n",
            "Iteration: 9716; loss: 0.6916393041610718\n",
            "Iteration: 9717; loss: 0.7010856866836548\n",
            "Iteration: 9718; loss: 0.6963105201721191\n",
            "Iteration: 9719; loss: 0.67876136302948\n",
            "Iteration: 9720; loss: 0.6824064254760742\n",
            "Iteration: 9721; loss: 0.6821563243865967\n",
            "Iteration: 9722; loss: 0.6849042773246765\n",
            "Iteration: 9723; loss: 0.6834619045257568\n",
            "Iteration: 9724; loss: 0.6769578456878662\n",
            "Iteration: 9725; loss: 0.6832150816917419\n",
            "Iteration: 9726; loss: 0.6889249682426453\n",
            "Iteration: 9727; loss: 0.6878536343574524\n",
            "Iteration: 9728; loss: 0.6838370561599731\n",
            "Iteration: 9729; loss: 0.6884658336639404\n",
            "Iteration: 9730; loss: 0.6875595450401306\n",
            "Iteration: 9731; loss: 0.6884223222732544\n",
            "Iteration: 9732; loss: 0.6837731003761292\n",
            "Iteration: 9733; loss: 0.6864503622055054\n",
            "Iteration: 9734; loss: 0.6850676536560059\n",
            "Iteration: 9735; loss: 0.6883558630943298\n",
            "Iteration: 9736; loss: 0.6842906475067139\n",
            "Iteration: 9737; loss: 0.6860816478729248\n",
            "Iteration: 9738; loss: 0.6997925043106079\n",
            "Iteration: 9739; loss: 0.6860410571098328\n",
            "Iteration: 9740; loss: 0.6766095161437988\n",
            "Iteration: 9741; loss: 0.7001156806945801\n",
            "Iteration: 9742; loss: 0.6877224445343018\n",
            "Iteration: 9743; loss: 0.6860888004302979\n",
            "Iteration: 9744; loss: 0.6819080710411072\n",
            "Iteration: 9745; loss: 0.6957371830940247\n",
            "Iteration: 9746; loss: 0.6861409544944763\n",
            "Iteration: 9747; loss: 0.696440577507019\n",
            "Iteration: 9748; loss: 0.6843689680099487\n",
            "Iteration: 9749; loss: 0.6664867401123047\n",
            "Iteration: 9750; loss: 0.6825625896453857\n",
            "Iteration: 9751; loss: 0.6965382695198059\n",
            "Iteration: 9752; loss: 0.6956584453582764\n",
            "Iteration: 9753; loss: 0.68267422914505\n",
            "Iteration: 9754; loss: 0.6777694225311279\n",
            "Iteration: 9755; loss: 0.6762008666992188\n",
            "Iteration: 9756; loss: 0.690454363822937\n",
            "Iteration: 9757; loss: 0.6868312954902649\n",
            "Iteration: 9758; loss: 0.6825422644615173\n",
            "Iteration: 9759; loss: 0.6813284754753113\n",
            "Iteration: 9760; loss: 0.685001015663147\n",
            "Iteration: 9761; loss: 0.687800407409668\n",
            "Iteration: 9762; loss: 0.691515326499939\n",
            "Iteration: 9763; loss: 0.6787399649620056\n",
            "Iteration: 9764; loss: 0.701995849609375\n",
            "Iteration: 9765; loss: 0.6941282153129578\n",
            "Iteration: 9766; loss: 0.6823889017105103\n",
            "Iteration: 9767; loss: 0.6971073746681213\n",
            "Iteration: 9768; loss: 0.6742860078811646\n",
            "Iteration: 9769; loss: 0.695009171962738\n",
            "Iteration: 9770; loss: 0.6900767087936401\n",
            "Iteration: 9771; loss: 0.6926801204681396\n",
            "Iteration: 9772; loss: 0.6835753917694092\n",
            "Iteration: 9773; loss: 0.6874526739120483\n",
            "Iteration: 9774; loss: 0.7023448348045349\n",
            "Iteration: 9775; loss: 0.6754945516586304\n",
            "Iteration: 9776; loss: 0.6869564652442932\n",
            "Iteration: 9777; loss: 0.6814515590667725\n",
            "Iteration: 9778; loss: 0.6882073879241943\n",
            "Iteration: 9779; loss: 0.6809514760971069\n",
            "Iteration: 9780; loss: 0.676990807056427\n",
            "Iteration: 9781; loss: 0.6983681321144104\n",
            "Iteration: 9782; loss: 0.6840621829032898\n",
            "Iteration: 9783; loss: 0.6948571801185608\n",
            "Iteration: 9784; loss: 0.6903091669082642\n",
            "Iteration: 9785; loss: 0.6884139180183411\n",
            "Iteration: 9786; loss: 0.6875671148300171\n",
            "Iteration: 9787; loss: 0.6884410381317139\n",
            "Iteration: 9788; loss: 0.6832974553108215\n",
            "Iteration: 9789; loss: 0.6859256029129028\n",
            "Iteration: 9790; loss: 0.6875348091125488\n",
            "Iteration: 9791; loss: 0.6920908689498901\n",
            "Iteration: 9792; loss: 0.6837348937988281\n",
            "Iteration: 9793; loss: 0.6917624473571777\n",
            "Iteration: 9794; loss: 0.6809701919555664\n",
            "Iteration: 9795; loss: 0.6866490244865417\n",
            "Iteration: 9796; loss: 0.689268946647644\n",
            "Iteration: 9797; loss: 0.6916450262069702\n",
            "Iteration: 9798; loss: 0.6864163279533386\n",
            "Iteration: 9799; loss: 0.6900817155838013\n",
            "Iteration: 9800; loss: 0.6759474277496338\n",
            "Iteration: 9801; loss: 0.6833398342132568\n",
            "Iteration: 9802; loss: 0.6888676881790161\n",
            "Iteration: 9803; loss: 0.6870898008346558\n",
            "Iteration: 9804; loss: 0.6846501231193542\n",
            "Iteration: 9805; loss: 0.6904549598693848\n",
            "Iteration: 9806; loss: 0.6825729608535767\n",
            "Iteration: 9807; loss: 0.6788839101791382\n",
            "Iteration: 9808; loss: 0.6786439418792725\n",
            "Iteration: 9809; loss: 0.6968521475791931\n",
            "Iteration: 9810; loss: 0.6956947445869446\n",
            "Iteration: 9811; loss: 0.696866512298584\n",
            "Iteration: 9812; loss: 0.6879248023033142\n",
            "Iteration: 9813; loss: 0.685992956161499\n",
            "Iteration: 9814; loss: 0.6875083446502686\n",
            "Iteration: 9815; loss: 0.6817060708999634\n",
            "Iteration: 9816; loss: 0.6965930461883545\n",
            "Iteration: 9817; loss: 0.6793214678764343\n",
            "Iteration: 9818; loss: 0.705658495426178\n",
            "Iteration: 9819; loss: 0.6927917003631592\n",
            "Iteration: 9820; loss: 0.6891107559204102\n",
            "Iteration: 9821; loss: 0.688113272190094\n",
            "Iteration: 9822; loss: 0.6840747594833374\n",
            "Iteration: 9823; loss: 0.6961965560913086\n",
            "Iteration: 9824; loss: 0.6835851669311523\n",
            "Iteration: 9825; loss: 0.6802880764007568\n",
            "Iteration: 9826; loss: 0.6933057904243469\n",
            "Iteration: 9827; loss: 0.6823756098747253\n",
            "Iteration: 9828; loss: 0.6873239278793335\n",
            "Iteration: 9829; loss: 0.6960829496383667\n",
            "Iteration: 9830; loss: 0.6881524920463562\n",
            "Iteration: 9831; loss: 0.6913585662841797\n",
            "Iteration: 9832; loss: 0.6932172775268555\n",
            "Iteration: 9833; loss: 0.6822723150253296\n",
            "Iteration: 9834; loss: 0.6762213706970215\n",
            "Iteration: 9835; loss: 0.688833475112915\n",
            "Iteration: 9836; loss: 0.6914967894554138\n",
            "Iteration: 9837; loss: 0.6785516738891602\n",
            "Iteration: 9838; loss: 0.6882042288780212\n",
            "Iteration: 9839; loss: 0.6805601716041565\n",
            "Iteration: 9840; loss: 0.6918030381202698\n",
            "Iteration: 9841; loss: 0.6942991018295288\n",
            "Iteration: 9842; loss: 0.6830889582633972\n",
            "Iteration: 9843; loss: 0.6891013383865356\n",
            "Iteration: 9844; loss: 0.6912142038345337\n",
            "Iteration: 9845; loss: 0.6931783556938171\n",
            "Iteration: 9846; loss: 0.689923882484436\n",
            "Iteration: 9847; loss: 0.6809830665588379\n",
            "Iteration: 9848; loss: 0.6932036876678467\n",
            "Iteration: 9849; loss: 0.6937402486801147\n",
            "Iteration: 9850; loss: 0.6770586371421814\n",
            "Iteration: 9851; loss: 0.6974359750747681\n",
            "Iteration: 9852; loss: 0.6826092004776001\n",
            "Iteration: 9853; loss: 0.6840494275093079\n",
            "Iteration: 9854; loss: 0.6810064315795898\n",
            "Iteration: 9855; loss: 0.674257755279541\n",
            "Iteration: 9856; loss: 0.6774487495422363\n",
            "Iteration: 9857; loss: 0.6863079071044922\n",
            "Iteration: 9858; loss: 0.6816837191581726\n",
            "Iteration: 9859; loss: 0.6917346715927124\n",
            "Iteration: 9860; loss: 0.6805445551872253\n",
            "Iteration: 9861; loss: 0.6862873435020447\n",
            "Iteration: 9862; loss: 0.6778009533882141\n",
            "Iteration: 9863; loss: 0.6847923398017883\n",
            "Iteration: 9864; loss: 0.6909809112548828\n",
            "Iteration: 9865; loss: 0.6936004757881165\n",
            "Iteration: 9866; loss: 0.6829430460929871\n",
            "Iteration: 9867; loss: 0.6901100277900696\n",
            "Iteration: 9868; loss: 0.6855735182762146\n",
            "Iteration: 9869; loss: 0.6919655799865723\n",
            "Iteration: 9870; loss: 0.6890133619308472\n",
            "Iteration: 9871; loss: 0.6734082102775574\n",
            "Iteration: 9872; loss: 0.6983387470245361\n",
            "Iteration: 9873; loss: 0.6929192543029785\n",
            "Iteration: 9874; loss: 0.6917981505393982\n",
            "Iteration: 9875; loss: 0.6814628839492798\n",
            "Iteration: 9876; loss: 0.6925725340843201\n",
            "Iteration: 9877; loss: 0.6942322254180908\n",
            "Iteration: 9878; loss: 0.6804873943328857\n",
            "Iteration: 9879; loss: 0.6914317607879639\n",
            "Iteration: 9880; loss: 0.6846434473991394\n",
            "Iteration: 9881; loss: 0.6969617605209351\n",
            "Iteration: 9882; loss: 0.7024474143981934\n",
            "Iteration: 9883; loss: 0.6928789615631104\n",
            "Iteration: 9884; loss: 0.6789657473564148\n",
            "Iteration: 9885; loss: 0.681787371635437\n",
            "Iteration: 9886; loss: 0.6911106109619141\n",
            "Iteration: 9887; loss: 0.6981609463691711\n",
            "Iteration: 9888; loss: 0.6859635710716248\n",
            "Iteration: 9889; loss: 0.6687489748001099\n",
            "Iteration: 9890; loss: 0.6792189478874207\n",
            "Iteration: 9891; loss: 0.679231584072113\n",
            "Iteration: 9892; loss: 0.6809277534484863\n",
            "Iteration: 9893; loss: 0.6923521757125854\n",
            "Iteration: 9894; loss: 0.681822657585144\n",
            "Iteration: 9895; loss: 0.6886889934539795\n",
            "Iteration: 9896; loss: 0.6938982605934143\n",
            "Iteration: 9897; loss: 0.6972777247428894\n",
            "Iteration: 9898; loss: 0.6856222748756409\n",
            "Iteration: 9899; loss: 0.6912587285041809\n",
            "Iteration: 9900; loss: 0.6866922378540039\n",
            "Iteration: 9901; loss: 0.6867355108261108\n",
            "Iteration: 9902; loss: 0.6750968098640442\n",
            "Iteration: 9903; loss: 0.6908823251724243\n",
            "Iteration: 9904; loss: 0.682019829750061\n",
            "Iteration: 9905; loss: 0.674364447593689\n",
            "Iteration: 9906; loss: 0.6875767111778259\n",
            "Iteration: 9907; loss: 0.6905364394187927\n",
            "Iteration: 9908; loss: 0.7050992846488953\n",
            "Iteration: 9909; loss: 0.6786841154098511\n",
            "Iteration: 9910; loss: 0.6869977116584778\n",
            "Iteration: 9911; loss: 0.6800434589385986\n",
            "Iteration: 9912; loss: 0.6850443482398987\n",
            "Iteration: 9913; loss: 0.6994192600250244\n",
            "Iteration: 9914; loss: 0.6983269453048706\n",
            "Iteration: 9915; loss: 0.674228847026825\n",
            "Iteration: 9916; loss: 0.6757453083992004\n",
            "Iteration: 9917; loss: 0.6804309487342834\n",
            "Iteration: 9918; loss: 0.6894992589950562\n",
            "Iteration: 9919; loss: 0.6958843469619751\n",
            "Iteration: 9920; loss: 0.6774924993515015\n",
            "Iteration: 9921; loss: 0.6738607883453369\n",
            "Iteration: 9922; loss: 0.6844111680984497\n",
            "Iteration: 9923; loss: 0.70298171043396\n",
            "Iteration: 9924; loss: 0.6981632113456726\n",
            "Iteration: 9925; loss: 0.6855012774467468\n",
            "Iteration: 9926; loss: 0.6916707754135132\n",
            "Iteration: 9927; loss: 0.673701822757721\n",
            "Iteration: 9928; loss: 0.6794402599334717\n",
            "Iteration: 9929; loss: 0.688127875328064\n",
            "Iteration: 9930; loss: 0.6778304576873779\n",
            "Iteration: 9931; loss: 0.695746898651123\n",
            "Iteration: 9932; loss: 0.6923118233680725\n",
            "Iteration: 9933; loss: 0.6856188774108887\n",
            "Iteration: 9934; loss: 0.6838567852973938\n",
            "Iteration: 9935; loss: 0.6990838050842285\n",
            "Iteration: 9936; loss: 0.6762306690216064\n",
            "Iteration: 9937; loss: 0.6893535256385803\n",
            "Iteration: 9938; loss: 0.684759795665741\n",
            "Iteration: 9939; loss: 0.685843825340271\n",
            "Iteration: 9940; loss: 0.6674712896347046\n",
            "Iteration: 9941; loss: 0.6926196813583374\n",
            "Iteration: 9942; loss: 0.683628499507904\n",
            "Iteration: 9943; loss: 0.6833707094192505\n",
            "Iteration: 9944; loss: 0.684109091758728\n",
            "Iteration: 9945; loss: 0.6711650490760803\n",
            "Iteration: 9946; loss: 0.6805615425109863\n",
            "Iteration: 9947; loss: 0.685823380947113\n",
            "Iteration: 9948; loss: 0.6757178902626038\n",
            "Iteration: 9949; loss: 0.6887718439102173\n",
            "Iteration: 9950; loss: 0.6941392421722412\n",
            "Iteration: 9951; loss: 0.6862634420394897\n",
            "Iteration: 9952; loss: 0.688674807548523\n",
            "Iteration: 9953; loss: 0.6856945157051086\n",
            "Iteration: 9954; loss: 0.6932272911071777\n",
            "Iteration: 9955; loss: 0.6782743334770203\n",
            "Iteration: 9956; loss: 0.685600996017456\n",
            "Iteration: 9957; loss: 0.6804296970367432\n",
            "Iteration: 9958; loss: 0.6679462194442749\n",
            "Iteration: 9959; loss: 0.6788191795349121\n",
            "Iteration: 9960; loss: 0.6768678426742554\n",
            "Iteration: 9961; loss: 0.6877226829528809\n",
            "Iteration: 9962; loss: 0.6905108690261841\n",
            "Iteration: 9963; loss: 0.6946801543235779\n",
            "Iteration: 9964; loss: 0.6903911232948303\n",
            "Iteration: 9965; loss: 0.6838029623031616\n",
            "Iteration: 9966; loss: 0.6789970397949219\n",
            "Iteration: 9967; loss: 0.6834661960601807\n",
            "Iteration: 9968; loss: 0.6946922540664673\n",
            "Iteration: 9969; loss: 0.6855612397193909\n",
            "Iteration: 9970; loss: 0.6671593189239502\n",
            "Iteration: 9971; loss: 0.6988479495048523\n",
            "Iteration: 9972; loss: 0.6947749257087708\n",
            "Iteration: 9973; loss: 0.6933517456054688\n",
            "Iteration: 9974; loss: 0.680027425289154\n",
            "Iteration: 9975; loss: 0.6981086134910583\n",
            "Iteration: 9976; loss: 0.6924156546592712\n",
            "Iteration: 9977; loss: 0.6768405437469482\n",
            "Iteration: 9978; loss: 0.6864882111549377\n",
            "Iteration: 9979; loss: 0.6839415431022644\n",
            "Iteration: 9980; loss: 0.6673450469970703\n",
            "Iteration: 9981; loss: 0.667949914932251\n",
            "Iteration: 9982; loss: 0.6719388961791992\n",
            "Iteration: 9983; loss: 0.6968057751655579\n",
            "Iteration: 9984; loss: 0.6683189272880554\n",
            "Iteration: 9985; loss: 0.6753378510475159\n",
            "Iteration: 9986; loss: 0.681247889995575\n",
            "Iteration: 9987; loss: 0.6857373714447021\n",
            "Iteration: 9988; loss: 0.6852090954780579\n",
            "Iteration: 9989; loss: 0.68306565284729\n",
            "Iteration: 9990; loss: 0.6773175597190857\n",
            "Iteration: 9991; loss: 0.6752930879592896\n",
            "Iteration: 9992; loss: 0.6824300289154053\n",
            "Iteration: 9993; loss: 0.6970599293708801\n",
            "Iteration: 9994; loss: 0.6753389835357666\n",
            "Iteration: 9995; loss: 0.6805998086929321\n",
            "Iteration: 9996; loss: 0.6992908120155334\n",
            "Iteration: 9997; loss: 0.666998028755188\n",
            "Iteration: 9998; loss: 0.6835134029388428\n",
            "Iteration: 9999; loss: 0.6939358115196228\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f9c03454be0>]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "57Gx-GFTyG8g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}