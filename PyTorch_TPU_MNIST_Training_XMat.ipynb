{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WzFpOKDmURO"
      },
      "source": [
        "## PyTorch/TPU MNIST Demo\n",
        "\n",
        "This colab example corresponds to the implementation under [test_train_mp_mnist.py](https://github.com/pytorch/xla/blob/master/test/test_train_mp_mnist.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOp9jBEumdvC"
      },
      "source": [
        "<h3>  &nbsp;&nbsp;Use Colab Cloud TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
        "\n",
        "* On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
        "* The cell below makes sure you have access to a TPU on Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx4YVNHametU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YofXQrnxmf5r"
      },
      "source": [
        "### [RUNME] Install Colab TPU compatible PyTorch/TPU wheels and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OApBOAe1fpH_",
        "cellView": "code"
      },
      "outputs": [],
      "source": [
        "#@title Default title text\n",
        "!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp39-cp39-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfSCdVlA8jFg"
      },
      "source": [
        "### If you're using GPU with this colab notebook, run the below commented code to install GPU compatible PyTorch wheel and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1Vfg-rH8bF4"
      },
      "outputs": [],
      "source": [
        "#!pip install cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/cuda/117/torch_xla-2.0-cp38-cp38-linux_x86_64.whl --force-reinstall "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPrij_iPfqTV"
      },
      "source": [
        "### Only run the below commented cell if you would like a nightly release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJZrkoejQhxK"
      },
      "outputs": [],
      "source": [
        "# VERSION = \"1.13\"  #@param [\"1.13\", \"nightly\", \"20220315\"]  # or YYYYMMDD format\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION\n",
        "# import os \n",
        "# os.environ['LD_LIBRARY_PATH']='/usr/local/lib'\n",
        "# !echo $LD_LIBRARY_PATH\n",
        "\n",
        "# !sudo ln -s /usr/local/lib/libmkl_intel_lp64.so /usr/local/lib/libmkl_intel_lp64.so.1\n",
        "# !sudo ln -s /usr/local/lib/libmkl_intel_thread.so /usr/local/lib/libmkl_intel_thread.so.1\n",
        "# !sudo ln -s /usr/local/lib/libmkl_core.so /usr/local/lib/libmkl_core.so.1\n",
        "\n",
        "# !ldconfig\n",
        "# !ldd /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch.so"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PSGD Code\n",
        "\"\"\"Created in May, 2018\n",
        "Pytorch functions for preconditioned SGD\n",
        "@author: XILIN LI, lixilinx@gmail.com\n",
        "\n",
        "Updated in Dec, 2020: \n",
        "Wrapped Kronecker product preconditioner for easy use: the code will select the proper Kronecker product  \n",
        "preconditioner based on the formats of input left and right preconditioners.\n",
        "Add torch.jit.script decorator by default\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "@torch.jit.script\n",
        "def update_precond_dense(Q, dxs, dgs, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, List[Tensor], List[Tensor], float, float) -> Tensor\n",
        "    \"\"\"\n",
        "    update dense preconditioner P = Q^T*Q\n",
        "    Q: Cholesky factor of preconditioner with positive diagonal entries \n",
        "    dxs: list of perturbations of parameters\n",
        "    dgs: list of perturbations of gradients\n",
        "    step: update step size normalized to range [0, 1] \n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    dx = torch.cat([torch.reshape(x, [-1, 1]) for x in dxs])\n",
        "    dg = torch.cat([torch.reshape(g, [-1, 1]) for g in dgs])\n",
        "    \n",
        "    a = Q.mm(dg)\n",
        "    #b = torch.triangular_solve(dx, Q, upper=True, transpose=True)[0]\n",
        "    b = torch.linalg.solve_triangular(Q.t(), dx, upper=False)\n",
        "\n",
        "    grad = torch.triu(a.mm(a.t()) - b.mm(b.t()))\n",
        "    step0 = step/(grad.abs().max() + _tiny)        \n",
        "        \n",
        "    return Q - step0*grad.mm(Q)\n",
        "\n",
        "@torch.jit.script\n",
        "def precond_grad_dense(Q, grads):\n",
        "    # type: (Tensor, List[Tensor]) -> List[Tensor]\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using dense preconditioner\n",
        "    Q: Cholesky factor of preconditioner\n",
        "    grads: list of gradients\n",
        "    \"\"\"\n",
        "    grad = [torch.reshape(g, [-1, 1]) for g in grads]\n",
        "    lens = [g.shape[0] for g in grad]\n",
        "    grad = torch.cat(grad)\n",
        "    grad = Q.t().mm(Q.mm(grad))\n",
        "    \n",
        "    pre_grads = []\n",
        "    idx = 0\n",
        "    for i in range(len(grads)):\n",
        "        pre_grads.append(torch.reshape(grad[idx : idx + lens[i]], grads[i].shape))\n",
        "        idx = idx + lens[i]\n",
        "        \n",
        "    return pre_grads\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "def update_precond_kron(Ql, Qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    \"\"\"\n",
        "    Update Kronecker product preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql)\n",
        "    Either Ql or Qr can be sparse, and the code can choose the right update rule.\n",
        "    dX: perturbation of (matrix) parameter\n",
        "    dG: perturbation of (matrix) gradient\n",
        "    step: update step size\n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    m, n = Ql.shape\n",
        "    p, q = Qr.shape\n",
        "    if m==n: # left is dense\n",
        "        if p==q: #(dense, dense) format\n",
        "            return _update_precond_dense_dense(Ql, Qr, dX, dG, step, _tiny)\n",
        "        elif p==2: # (dense, normalization) format\n",
        "            return _update_precond_norm_dense(Qr, Ql, dX.t(), dG.t(), step, _tiny)[::-1]\n",
        "        elif p==1: # (dense, scaling) format\n",
        "            return _update_precond_dense_scale(Ql, Qr, dX, dG, step, _tiny)\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    elif m==2: # left is normalization\n",
        "        if p==q: # (normalization, dense) format\n",
        "            return _update_precond_norm_dense(Ql, Qr, dX, dG, step, _tiny)\n",
        "        elif p==1: # (normalization, scaling) format\n",
        "            return _update_precond_norm_scale(Ql, Qr, dX, dG, step, _tiny)\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    elif m==1: # left is scaling\n",
        "        if p==q: # (scaling, dense) format\n",
        "            return _update_precond_dense_scale(Qr, Ql, dX.t(), dG.t(), step, _tiny)[::-1]\n",
        "        elif p==2: # (scaling, normalization) format\n",
        "            return _update_precond_norm_scale(Qr, Ql, dX.t(), dG.t(), step, _tiny)[::-1]\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    else:\n",
        "        raise Exception('Unknown Kronecker product preconditioner')\n",
        " \n",
        "       \n",
        "def precond_grad_kron(Ql, Qr, Grad):\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using Kronecker product preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql)\n",
        "    Either Ql or Qr can be sparse, and the code can choose the right way to precondition the gradient\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    m, n = Ql.shape\n",
        "    p, q = Qr.shape\n",
        "    if m==n: # left is dense\n",
        "        if p==q: #(dense, dense) format\n",
        "            return _precond_grad_dense_dense(Ql, Qr, Grad)\n",
        "        elif p==2: # (dense, normalization) format\n",
        "            return _precond_grad_norm_dense(Qr, Ql, Grad.t()).t()\n",
        "        elif p==1: # (dense, scaling) format\n",
        "            return _precond_grad_dense_scale(Ql, Qr, Grad)\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    elif m==2: # left is normalization\n",
        "        if p==q: # (normalization, dense) format\n",
        "            return _precond_grad_norm_dense(Ql, Qr, Grad)\n",
        "        elif p==1: # (normalization, scaling) format\n",
        "            return _precond_grad_norm_scale(Ql, Qr, Grad)\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    elif m==1: # left is scaling\n",
        "        if p==q: # (scaling, dense) format\n",
        "            return _precond_grad_dense_scale(Qr, Ql, Grad.t()).t()\n",
        "        elif p==2: # (scaling, normalization) format\n",
        "            return _precond_grad_norm_scale(Qr, Ql, Grad.t()).t()\n",
        "        else:\n",
        "            raise Exception('Unknown Kronecker product preconditioner')\n",
        "    else:\n",
        "        raise Exception('Unknown Kronecker product preconditioner')\n",
        "        \n",
        "\n",
        "###############################################################################\n",
        "@torch.jit.script\n",
        "def _update_precond_dense_dense(Ql, Qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    update Kronecker product preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql)\n",
        "    Ql: (left side) Cholesky factor of preconditioner with positive diagonal entries\n",
        "    Qr: (right side) Cholesky factor of preconditioner with positive diagonal entries\n",
        "    dX: perturbation of (matrix) parameter\n",
        "    dG: perturbation of (matrix) gradient\n",
        "    step: update step size normalized to range [0, 1] \n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    max_l = torch.max(torch.diag(Ql))\n",
        "    max_r = torch.max(torch.diag(Qr))\n",
        "    \n",
        "    rho = torch.sqrt(max_l/max_r)\n",
        "    Ql /= rho\n",
        "    Qr *= rho\n",
        "    \n",
        "    #A = Ql.mm( dG.mm( Qr.t() ) )\n",
        "    #Bt = torch.triangular_solve((torch.triangular_solve(dX.t(), Qr, upper=True, transpose=True))[0].t(), \n",
        "    #                 Ql, upper=True, transpose=True)[0]\n",
        "    A = torch.linalg.multi_dot([Ql, dG, Qr.t()])\n",
        "    Bt = torch.linalg.solve_triangular(Ql.t(), torch.linalg.solve_triangular(Qr, dX, upper=True, left=False), upper=False)\n",
        "    \n",
        "    grad1 = torch.triu(A.mm(A.t()) - Bt.mm(Bt.t()))\n",
        "    grad2 = torch.triu(A.t().mm(A) - Bt.t().mm(Bt))\n",
        "    \n",
        "    step1 = step/(torch.max(torch.abs(grad1)) + _tiny)\n",
        "    step2 = step/(torch.max(torch.abs(grad2)) + _tiny)\n",
        "        \n",
        "    return Ql - step1*grad1.mm(Ql), Qr - step2*grad2.mm(Qr)\n",
        "    \n",
        "@torch.jit.script\n",
        "def _precond_grad_dense_dense(Ql, Qr, Grad):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using Kronecker product preconditioner\n",
        "    Ql: (left side) Cholesky factor of preconditioner\n",
        "    Qr: (right side) Cholesky factor of preconditioner\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    #return torch.chain_matmul(Ql.t(), Ql, Grad, Qr.t(), Qr)\n",
        "    return torch.linalg.multi_dot([Ql.t(), Ql, Grad, Qr.t(), Qr])\n",
        "    \n",
        "\n",
        "###############################################################################\n",
        "# (normalization, dense) format Kronecker product preconditioner\n",
        "@torch.jit.script\n",
        "def _update_precond_norm_dense(ql, Qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    update (normalization, dense) Kronecker product preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql), where\n",
        "    dX and dG have shape (M, N)\n",
        "    ql has shape (2, M)\n",
        "    Qr has shape (N, N)\n",
        "    ql[0] is the diagonal part of Ql\n",
        "    ql[1,0:-1] is the last column of Ql, excluding the last entry\n",
        "    dX is perturbation of (matrix) parameter\n",
        "    dG is perturbation of (matrix) gradient\n",
        "    step: update step size normalized to range [0, 1] \n",
        "    _tiny: an offset to avoid division by zero  \n",
        "    \"\"\"\n",
        "    # make sure that Ql and Qr have similar dynamic range\n",
        "    max_l = torch.max(ql[0])\n",
        "    max_r = torch.max(torch.diag(Qr))  \n",
        "    rho = torch.sqrt(max_l/max_r)\n",
        "    ql /= rho\n",
        "    Qr *= rho\n",
        "    \n",
        "    # refer to https://arxiv.org/abs/1512.04202 for details\n",
        "    A = ql[0:1].t()*dG + ql[1:].t().mm( dG[-1:] ) # Ql*dG \n",
        "    A = A.mm(Qr.t())\n",
        "    \n",
        "    Bt = dX/ql[0:1].t()\n",
        "    Bt[-1:] -= (ql[1:]/(ql[0:1]*ql[0,-1])).mm(dX)\n",
        "    #Bt = torch.triangular_solve(Bt.t(), Qr, upper=True, transpose=True)[0].t()\n",
        "    Bt = torch.linalg.solve_triangular(Qr, Bt, upper=True, left=False)\n",
        "    \n",
        "    grad1_diag = torch.sum(A*A, dim=1) - torch.sum(Bt*Bt, dim=1)\n",
        "    grad1_bias = A[:-1].mm(A[-1:].t()) - Bt[:-1].mm(Bt[-1:].t()) \n",
        "    grad1_bias = torch.cat([torch.squeeze(grad1_bias), grad1_bias.new_zeros(1)])  \n",
        "\n",
        "    step1 = step/(torch.max(torch.max(torch.abs(grad1_diag)), \n",
        "                            torch.max(torch.abs(grad1_bias))) + _tiny)\n",
        "    new_ql0 = ql[0] - step1*grad1_diag*ql[0]\n",
        "    new_ql1 = ql[1] - step1*(grad1_diag*ql[1] + ql[0,-1]*grad1_bias)\n",
        "    \n",
        "    grad2 = torch.triu(A.t().mm(A) - Bt.t().mm(Bt))\n",
        "    step2 = step/(torch.max(torch.abs(grad2)) + _tiny)\n",
        "    \n",
        "    return torch.stack((new_ql0, new_ql1)), Qr - step2*grad2.mm(Qr)\n",
        "\n",
        "@torch.jit.script\n",
        "def _precond_grad_norm_dense(ql, Qr, Grad):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using (normalization, dense) Kronecker product preconditioner \n",
        "    Suppose Grad has shape (M, N)\n",
        "    ql[0] is the diagonal part of Ql\n",
        "    ql[1, 0:-1] is the last column of Ql, excluding the last entry\n",
        "    Qr: shape (N, N), Cholesky factor of right preconditioner\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    preG = ql[0:1].t()*Grad + ql[1:].t().mm(Grad[-1:]) # Ql*Grad \n",
        "    #preG = torch.chain_matmul(preG, Qr.t(), Qr)\n",
        "    preG = torch.linalg.multi_dot([preG, Qr.t(), Qr])\n",
        "    add_last_row = ql[1:].mm(preG) # use it to modify the last row\n",
        "    preG *= ql[0:1].t()\n",
        "    preG[-1:] += add_last_row\n",
        "    \n",
        "    return preG\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# (normalization, scaling) Kronecker product preconditioner \n",
        "# the left one is a normalization preconditioner; the right one is a scaling preconditioner\n",
        "@torch.jit.script\n",
        "def _update_precond_norm_scale(ql, qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    update (normalization, scaling) preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql), where\n",
        "    dX and dG have shape (M, N)\n",
        "    ql has shape (2, M)\n",
        "    qr has shape (1, N)\n",
        "    ql[0] is the diagonal part of Ql\n",
        "    ql[1, 0:-1] is the last column of Ql, excluding the last entry\n",
        "    qr is the diagonal part of Qr\n",
        "    dX is perturbation of (matrix) parameter\n",
        "    dG is perturbation of (matrix) gradient\n",
        "    step: update step size\n",
        "    _tiny: an offset to avoid division by zero  \n",
        "    \"\"\"\n",
        "    # make sure that Ql and Qr have similar dynamic range\n",
        "    max_l = torch.max(ql[0])\n",
        "    max_r = torch.max(qr) # qr always is positive\n",
        "    rho = torch.sqrt(max_l/max_r)\n",
        "    ql /= rho\n",
        "    qr *= rho\n",
        "    \n",
        "    # refer to https://arxiv.org/abs/1512.04202 for details\n",
        "    A = ql[0:1].t()*dG + ql[1:].t().mm( dG[-1:] ) # Ql*dG \n",
        "    A *= qr # Ql*dG*Qr \n",
        "    \n",
        "    Bt = dX/ql[0:1].t()\n",
        "    Bt[-1:] -= (ql[1:]/(ql[0:1]*ql[0,-1])).mm(dX)\n",
        "    Bt /= qr # Ql^(-T)*dX*Qr^(-1) \n",
        "    \n",
        "    grad1_diag = torch.sum(A*A, dim=1) - torch.sum(Bt*Bt, dim=1)\n",
        "    grad1_bias = A[:-1].mm(A[-1:].t()) - Bt[:-1].mm(Bt[-1:].t()) \n",
        "    grad1_bias = torch.cat([torch.squeeze(grad1_bias), grad1_bias.new_zeros(1)])  \n",
        "\n",
        "    step1 = step/(torch.max(torch.max(torch.abs(grad1_diag)), \n",
        "                            torch.max(torch.abs(grad1_bias))) + _tiny)\n",
        "    new_ql0 = ql[0] - step1*grad1_diag*ql[0]\n",
        "    new_ql1 = ql[1] - step1*(grad1_diag*ql[1] + ql[0,-1]*grad1_bias)\n",
        "    \n",
        "    grad2 = torch.sum(A*A, dim=0, keepdim=True) - torch.sum(Bt*Bt, dim=0, keepdim=True)\n",
        "    step2 = step/(torch.max(torch.abs(grad2)) + _tiny)\n",
        "    \n",
        "    return torch.stack((new_ql0, new_ql1)), qr - step2*grad2*qr\n",
        "\n",
        "@torch.jit.script\n",
        "def _precond_grad_norm_scale(ql, qr, Grad):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using (normalization, scaling) Kronecker product preconditioner\n",
        "    Suppose Grad has shape (M, N)\n",
        "    ql has shape (2, M) \n",
        "    qr has shape (1, N) \n",
        "    ql[0] is the diagonal part of Ql\n",
        "    ql[1, 0:-1] is the last column of Ql, excluding the last entry\n",
        "    qr is the diagonal part of Qr\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    preG = ql[0:1].t()*Grad + ql[1:].t().mm(Grad[-1:]) # Ql*Grad \n",
        "    preG *= (qr*qr) # Ql*Grad*Qr^T*Qr\n",
        "    add_last_row = ql[1:].mm(preG) # use it to modify the last row\n",
        "    preG *= ql[0:1].t()\n",
        "    preG[-1:] += add_last_row\n",
        "    \n",
        "    return preG\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "@torch.jit.script\n",
        "def _update_precond_dense_scale(Ql, qr, dX, dG, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> Tuple[Tensor, Tensor]\n",
        "    \"\"\"\n",
        "    update (dense, scaling) preconditioner P = kron_prod(Qr^T*Qr, Ql^T*Ql), where\n",
        "    dX and dG have shape (M, N)\n",
        "    Ql has shape (M, M)\n",
        "    qr has shape (1, N)\n",
        "    qr is the diagonal part of Qr\n",
        "    dX is perturbation of (matrix) parameter\n",
        "    dG is perturbation of (matrix) gradient\n",
        "    step: update step size\n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    max_l = torch.max(torch.diag(Ql))\n",
        "    max_r = torch.max(qr)\n",
        "    \n",
        "    rho = torch.sqrt(max_l/max_r)\n",
        "    Ql /= rho\n",
        "    qr *= rho\n",
        "    \n",
        "    A = Ql.mm( dG*qr )\n",
        "    #Bt = torch.triangular_solve(dX/qr, Ql, upper=True, transpose=True)[0]\n",
        "    Bt = torch.linalg.solve_triangular(Ql.t(), dX/qr, upper=False)\n",
        "    \n",
        "    grad1 = torch.triu(A.mm(A.t()) - Bt.mm(Bt.t()))\n",
        "    grad2 = torch.sum(A*A, dim=0, keepdim=True) - torch.sum(Bt*Bt, dim=0, keepdim=True)\n",
        "    \n",
        "    step1 = step/(torch.max(torch.abs(grad1)) + _tiny)\n",
        "    step2 = step/(torch.max(torch.abs(grad2)) + _tiny)\n",
        "        \n",
        "    return Ql - step1*grad1.mm(Ql), qr - step2*grad2*qr\n",
        "    \n",
        "@torch.jit.script\n",
        "def _precond_grad_dense_scale(Ql, qr, Grad):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    return preconditioned gradient using (dense, scaling) Kronecker product preconditioner\n",
        "    Suppose Grad has shape (M, N)\n",
        "    Ql: shape (M, M), (left side) Cholesky factor of preconditioner\n",
        "    qr: shape (1, N), defines a diagonal matrix for output feature scaling\n",
        "    Grad: (matrix) gradient\n",
        "    \"\"\"\n",
        "    #return torch.chain_matmul(Ql.t(), Ql, Grad*(qr*qr))\n",
        "    return torch.linalg.multi_dot([Ql.t(), Ql, Grad*(qr*qr)])\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################   \n",
        "@torch.jit.script                     \n",
        "def update_precond_splu(L12, l3, U12, u3, dxs, dgs, step=0.01, _tiny=1.2e-38):\n",
        "    # type: (Tensor,Tensor,Tensor,Tensor, List[Tensor],List[Tensor], float,float) -> Tuple[Tensor,Tensor,Tensor,Tensor]\n",
        "    \"\"\"\n",
        "    update sparse LU preconditioner P = Q^T*Q, where \n",
        "    Q = L*U,\n",
        "    L12 = [L1; L2]\n",
        "    U12 = [U1, U2]\n",
        "    L = [L1, 0; L2, diag(l3)]\n",
        "    U = [U1, U2; 0, diag(u3)]\n",
        "    l3 and u3 are column vectors\n",
        "    dxs: a list of random perturbation on parameters\n",
        "    dgs: a list of resultant perturbation on gradients\n",
        "    step: update step size normalized to range [0, 1] \n",
        "    _tiny: an offset to avoid division by zero \n",
        "    \"\"\"\n",
        "    # make sure that L and U have similar dynamic range\n",
        "    max_l = torch.max(torch.max(torch.diag(L12)), torch.max(l3))\n",
        "    max_u = torch.max(torch.max(torch.diag(U12)), torch.max(u3))\n",
        "    rho = torch.sqrt(max_l/max_u)\n",
        "    L12 /= rho\n",
        "    l3 /= rho\n",
        "    U12 *= rho\n",
        "    u3 *= rho\n",
        "    \n",
        "    # extract the blocks\n",
        "    r = U12.shape[0]\n",
        "    L1 = L12[:r]\n",
        "    L2 = L12[r:]\n",
        "    U1 = U12[:, :r]\n",
        "    U2 = U12[:, r:]\n",
        "    \n",
        "    dx = torch.cat([torch.reshape(x, [-1, 1]) for x in dxs]) # a tall column vector\n",
        "    dg = torch.cat([torch.reshape(g, [-1, 1]) for g in dgs]) # a tall column vector\n",
        "    \n",
        "    # U*dg\n",
        "    Ug1 = U1.mm(dg[:r]) + U2.mm(dg[r:])\n",
        "    Ug2 = u3*dg[r:]\n",
        "    # Q*dg\n",
        "    Qg1 = L1.mm(Ug1)\n",
        "    Qg2 = L2.mm(Ug1) + l3*Ug2\n",
        "    # inv(U^T)*dx\n",
        "    #iUtx1 = torch.triangular_solve(dx[:r], U1, upper=True, transpose=True)[0]\n",
        "    iUtx1 = torch.linalg.solve_triangular(U1.t(), dx[:r], upper=False)\n",
        "    iUtx2 = (dx[r:] - U2.t().mm(iUtx1))/u3\n",
        "    # inv(Q^T)*dx\n",
        "    iQtx2 = iUtx2/l3\n",
        "    #iQtx1 = torch.triangular_solve(iUtx1 - L2.t().mm(iQtx2), L1, upper=False, transpose=True)[0]\n",
        "    iQtx1 = torch.linalg.solve_triangular(L1.t(), iUtx1 - L2.t().mm(iQtx2), upper=True)\n",
        "    # L^T*Q*dg\n",
        "    LtQg1 = L1.t().mm(Qg1) + L2.t().mm(Qg2)\n",
        "    LtQg2 = l3*Qg2\n",
        "    # P*dg\n",
        "    Pg1 = U1.t().mm(LtQg1)\n",
        "    Pg2 = U2.t().mm(LtQg1) + u3*LtQg2\n",
        "    # inv(L)*inv(Q^T)*dx\n",
        "    #iLiQtx1 = torch.triangular_solve(iQtx1, L1, upper=False)[0]\n",
        "    iLiQtx1 = torch.linalg.solve_triangular(L1, iQtx1, upper=False)\n",
        "    iLiQtx2 = (iQtx2 - L2.mm(iLiQtx1))/l3\n",
        "    # inv(P)*dx\n",
        "    iPx2 = iLiQtx2/u3\n",
        "    #iPx1 = torch.triangular_solve(iLiQtx1 - U2.mm(iPx2), U1, upper=True)[0]\n",
        "    iPx1 = torch.linalg.solve_triangular(U1, iLiQtx1 - U2.mm(iPx2), upper=True)\n",
        "    \n",
        "    # update L\n",
        "    grad1 = Qg1.mm(Qg1.t()) - iQtx1.mm(iQtx1.t())\n",
        "    grad1 = torch.tril(grad1)\n",
        "    grad2 = Qg2.mm(Qg1.t()) - iQtx2.mm(iQtx1.t())\n",
        "    grad3 = Qg2*Qg2 - iQtx2*iQtx2\n",
        "    max_abs_grad = torch.max(torch.abs(grad1))\n",
        "    max_abs_grad = torch.max(max_abs_grad, torch.max(torch.abs(grad2)))\n",
        "    max_abs_grad = torch.max(max_abs_grad, torch.max(torch.abs(grad3)))\n",
        "    step0 = step/(max_abs_grad + _tiny)\n",
        "    newL1 = L1 - step0*grad1.mm(L1)\n",
        "    newL2 = L2 - step0*grad2.mm(L1) - step0*grad3*L2\n",
        "    newl3 = l3 - step0*grad3*l3\n",
        "\n",
        "    # update U\n",
        "    grad1 = Pg1.mm(dg[:r].t()) - dx[:r].mm(iPx1.t())\n",
        "    grad1 = torch.triu(grad1)\n",
        "    grad2 = Pg1.mm(dg[r:].t()) - dx[:r].mm(iPx2.t())\n",
        "    grad3 = Pg2*dg[r:] - dx[r:]*iPx2\n",
        "    max_abs_grad = torch.max(torch.abs(grad1))\n",
        "    max_abs_grad = torch.max(max_abs_grad, torch.max(torch.abs(grad2)))\n",
        "    max_abs_grad = torch.max(max_abs_grad, torch.max(torch.abs(grad3)))\n",
        "    step0 = step/(max_abs_grad + _tiny)\n",
        "    newU1 = U1 - U1.mm(step0*grad1)\n",
        "    newU2 = U2 - U1.mm(step0*grad2) - step0*grad3.t()*U2\n",
        "    newu3 = u3 - step0*grad3*u3\n",
        "\n",
        "    return torch.cat([newL1, newL2], dim=0), newl3, torch.cat([newU1, newU2], dim=1), newu3\n",
        "\n",
        "@torch.jit.script\n",
        "def precond_grad_splu(L12, l3, U12, u3, grads):\n",
        "    # type: (Tensor,Tensor,Tensor,Tensor, List[Tensor]) -> List[Tensor]\n",
        "    \"\"\"\n",
        "    return preconditioned gradient with sparse LU preconditioner\n",
        "    where P = Q^T*Q, \n",
        "    Q = L*U,\n",
        "    L12 = [L1; L2]\n",
        "    U12 = [U1, U2]\n",
        "    L = [L1, 0; L2, diag(l3)]\n",
        "    U = [U1, U2; 0, diag(u3)]\n",
        "    l3 and u3 are column vectors\n",
        "    grads: a list of gradients to be preconditioned\n",
        "    \"\"\"\n",
        "    grad = [torch.reshape(g, [-1, 1]) for g in grads] # a list of column vector\n",
        "    lens = [g.shape[0] for g in grad] # length of each column vector\n",
        "    grad = torch.cat(grad)  # a tall column vector\n",
        "    \n",
        "    r = U12.shape[0]\n",
        "    L1 = L12[:r]\n",
        "    L2 = L12[r:]\n",
        "    U1 = U12[:, :r]\n",
        "    U2 = U12[:, r:]    \n",
        "    \n",
        "    # U*g\n",
        "    Ug1 = U1.mm(grad[:r]) + U2.mm(grad[r:])\n",
        "    Ug2 = u3*grad[r:]\n",
        "    # Q*g\n",
        "    Qg1 = L1.mm(Ug1)\n",
        "    Qg2 = L2.mm(Ug1) + l3*Ug2\n",
        "    # L^T*Q*g\n",
        "    LtQg1 = L1.t().mm(Qg1) + L2.t().mm(Qg2)\n",
        "    LtQg2 = l3*Qg2\n",
        "    # P*g\n",
        "    pre_grad = torch.cat([U1.t().mm(LtQg1),\n",
        "                          U2.t().mm(LtQg1) + u3*LtQg2])\n",
        "    \n",
        "    pre_grads = [] # restore pre_grad to its original shapes\n",
        "    idx = 0\n",
        "    for i in range(len(grads)):\n",
        "        pre_grads.append(torch.reshape(pre_grad[idx : idx + lens[i]], grads[i].shape))\n",
        "        idx = idx + lens[i]\n",
        "    \n",
        "    return pre_grads\n",
        "\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "#\n",
        "# The low-rank approximation (UVd) preconditioner is defined by\n",
        "#\n",
        "#   Q = (I + U*V')*diag(d)\n",
        "#\n",
        "# which, after reparameterization, is equivalent to form\n",
        "#\n",
        "#   diag(d) + U*V'\n",
        "# \n",
        "# It relates to the LM-BFGS and conjugate gradient methods. \n",
        "#\n",
        "# The JIT decorator can be enabled if helps. \n",
        "# \n",
        "\n",
        "#@torch.jit.script\n",
        "def IpUVtmatvec(U, V, x):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    Returns (I + U*V')*x. All variables are either matrices or column vectors. \n",
        "    \"\"\"\n",
        "    return x + U.mm(V.t().mm(x))\n",
        "\n",
        "# def IpUVtsolve(U, V, x):\n",
        "#     \"\"\"\n",
        "#     Returns inv(I + U*V')*x. All variables are either matrices or column vectors.\n",
        "#     \"\"\"\n",
        "#     VtU = V.t().mm(U)\n",
        "#     I = torch.eye(VtU.size(dim=0), dtype=VtU.dtype, device=VtU.device)\n",
        "#     return x - U.mm(torch.linalg.solve(I + VtU, V.t().mm(x))) # torch.solve is slow\n",
        "\n",
        "# def norm_UVt(U, V):\n",
        "#     \"\"\"\n",
        "#     Returns ||U*V'||_fro = sqrt(tr(U'*U*V'*V)) = sqrt(sum((U'*U)*(V'*V))) \n",
        "#     \"\"\"\n",
        "#     return torch.sqrt(torch.abs(torch.sum( (U.t().mm(U))*(V.t().mm(V)) )))\n",
        "\n",
        "#@torch.jit.script\n",
        "def update_precond_UVd_math_(U, V, d, v, h, step, tiny):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, Tensor, float, float) -> None\n",
        "    \"\"\"\n",
        "    Update preconditioner Q = (I + U*V')*diag(d) with (vector, Hessian-vector product) = (v, h).\n",
        "    State variables U, V and d are updated inplace. \n",
        "                               \n",
        "    U, V, d, v, and h are either matrices or column vectors.  \n",
        "    \"\"\"\n",
        "    # balance the numerical dynamic ranges of U and V; optional \n",
        "    if torch.rand([]) < 0.01:\n",
        "        normU = torch.linalg.vector_norm(U)\n",
        "        normV = torch.linalg.vector_norm(V)\n",
        "        rho = torch.sqrt(normU/normV)\n",
        "        U.div_(rho)\n",
        "        V.mul_(rho)\n",
        "\n",
        "    Qh = IpUVtmatvec(U, V, d*h)\n",
        "    Ph = d*IpUVtmatvec(V, U, Qh)\n",
        "    \n",
        "    # invQtv = IpUVtsolve(V, U, v/d)\n",
        "    # invPv = IpUVtsolve(U, V, invQtv)/d\n",
        "    VtU = V.t().mm(U)\n",
        "    I = torch.eye(VtU.size(dim=0), dtype=VtU.dtype, device=VtU.device)\n",
        "    IpVtU = I + VtU\n",
        "    invQtv = v/d\n",
        "    # torch's linalg.solve is slow for small matrix\n",
        "    invQtv = invQtv - V.mm(torch.linalg.solve(IpVtU.t(), U.t().mm(invQtv)))  \n",
        "    invPv  = invQtv - U.mm(torch.linalg.solve(IpVtU,     V.t().mm(invQtv)))\n",
        "    invPv = invPv/d\n",
        "\n",
        "    nablaD = Ph*h - v*invPv\n",
        "    mu = step/(torch.max(torch.abs(nablaD)) + tiny)\n",
        "    #d = d - mu*d*nablaD\n",
        "    d.sub_(mu*d*nablaD)\n",
        "    \n",
        "    # update either U or V, not both at the same time\n",
        "    a, b = Qh, invQtv\n",
        "    if torch.rand([]) < 0.5:\n",
        "        # nablaU = Qh.mm(Qh.t().mm(V)) - invQtv.mm(invQtv.t().mm(V))\n",
        "        # mu = step/(norm_UVt(nablaU, V) + _tiny)\n",
        "        # U = U - mu*(nablaU + nablaU.mm(V.t().mm(U)))\n",
        "        atV = a.t().mm(V)\n",
        "        atVVt = atV.mm(V.t())\n",
        "        btV = b.t().mm(V)\n",
        "        btVVt = btV.mm(V.t())\n",
        "        norm = torch.sqrt(torch.abs( (a.t().mm(a))*(atVVt.mm(atVVt.t())) # abs to avoid sqrt(-0.0) \n",
        "                                    +(b.t().mm(b))*(btVVt.mm(btVVt.t())) \n",
        "                                  -2*(a.t().mm(b))*(atVVt.mm(btVVt.t())) ))\n",
        "        mu = step/(norm + tiny)\n",
        "        # U = U - mu*( a.mm(atV.mm(IpVtU)) \n",
        "        #             -b.mm(btV.mm(IpVtU)) )\n",
        "        U.sub_(mu*( a.mm(atV.mm(IpVtU)) \n",
        "                   -b.mm(btV.mm(IpVtU)) ))\n",
        "    else:\n",
        "        # nablaV = Qh.mm(Qh.t().mm(U)) - invQtv.mm(invQtv.t().mm(U))\n",
        "        # mu = step/(norm_UVt(U, nablaV) + _tiny)\n",
        "        # V = V - mu*(nablaV + V.mm(U.t().mm(nablaV)))\n",
        "        atU = a.t().mm(U)\n",
        "        btU = b.t().mm(U)\n",
        "        UUta = U.mm(atU.t())\n",
        "        UUtb = U.mm(btU.t())\n",
        "        norm = torch.sqrt(torch.abs( (UUta.t().mm(UUta))*(a.t().mm(a)) # abs to avoid sqrt(-0.0)\n",
        "                                    +(UUtb.t().mm(UUtb))*(b.t().mm(b))\n",
        "                                  -2*(UUta.t().mm(UUtb))*(a.t().mm(b)) ))\n",
        "        mu = step/(norm + tiny)\n",
        "        # V = V - mu*( (a + V.mm(atU.t())).mm(atU) \n",
        "        #             -(b + V.mm(btU.t())).mm(btU) )\n",
        "        V.sub_(mu*( (a + V.mm(atU.t())).mm(atU) \n",
        "                   -(b + V.mm(btU.t())).mm(btU) ))\n",
        "\n",
        "    # return [U, V, d]\n",
        "\n",
        "#@torch.jit.script\n",
        "def precond_grad_UVd_math(U, V, d, g):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    Preconditioning gradient g with Q = (I + U*V')*diag(d).\n",
        "                                         \n",
        "    All variables here are either matrices or column vectors. \n",
        "    \"\"\"\n",
        "    g = IpUVtmatvec(U, V, d*g)\n",
        "    g = d*IpUVtmatvec(V, U, g)\n",
        "    return g\n",
        "\n",
        "\n",
        "class UVd:\n",
        "    \"\"\"\n",
        "    Implements the low-rank approximation (UVd) preconditioner, Q = (I + U*V')*diag(d), as a class.\n",
        "\n",
        "    Args for initialization:\n",
        "        params_with_grad: a list of parameters or variables requiring gradients;\n",
        "        rank_of_approximation: rank of approximation, i.e., rank of U or V;\n",
        "        preconditioner_init_scale: initial scale of Q, or roughly, Q = preconditioner_init_scale*eye();\n",
        "        lr_params: normalized learning rate for parameters in range [0, 1];\n",
        "        lr_preconditioner: normalized learning rate for preconditioner in range [0, 1];\n",
        "        momentum: momentum factor in range [0,1);\n",
        "        grad_clip_max_norm: maximum allowable gradient norm after clipping, None for no clipping;\n",
        "        preconditioner_update_probability: probability on updating Q, 1 for updating at every step, and 0 for never;\n",
        "        exact_hessian_vector_product: True for exact Hessian-vector product via 2nd derivative,\n",
        "                                    and False for approximate one via finite-difference formulae.\n",
        "\n",
        "    Notes:\n",
        "        Note 1: The Hessian-vector product can be approximated using the finite-difference formulae by setting \n",
        "        exact_hessian_vector_product = False when the 2nd derivatives is not available.\n",
        "        In this case, make sure that the closure produces the same outputs given the same inputs, \n",
        "        except for numerical errors due to non-deterministic behaviors.\n",
        "        Random numbers, if any, used inside the closure should be generated starting from the same state, where the rng state can be\n",
        "        read and set by, e.g., `torch.cuda.get_rng_state' and `torch.cuda.set_rng_state', respectively.\n",
        "        \n",
        "        Note 2: Momentum here is the moving average of gradient so that its setting is decoupled from the learning rate.\n",
        "        This is necessary as the learning rate in PSGD is normalized. \n",
        "\n",
        "        Note 3: `torch.linalg.solve' is called twice in function `update_precond_UVd_math_'.\n",
        "        Certain solver could be orders of magnitude faster than others, especially for small matrices (see the pdf file).\n",
        "        Considering replace it with faster ones if the default solver is too slow.\n",
        "\n",
        "        Note 4: Currently, no support of sparse and mixed-precision gradients. \n",
        "        Half precision is supported except that torch.linalg.solve (v1.12) requires casting float16 to float32.    \n",
        "        \n",
        "        Note 5: lr_params, lr_preconditioner, momentum, grad_clip_max_norm, preconditioner_update_probability, and \n",
        "        exact_hessian_vector_product (bool) all can be reset on the fly. \n",
        "    \"\"\"\n",
        "    def __init__(self,  params_with_grad, rank_of_approximation:int=10, preconditioner_init_scale=1.0,\n",
        "                        lr_params=0.01, lr_preconditioner=0.01, momentum=0.0,\n",
        "                        grad_clip_max_norm=None, preconditioner_update_probability=1.0,\n",
        "                        exact_hessian_vector_product:bool=True):\n",
        "        # mutable members\n",
        "        self.lr_params = lr_params\n",
        "        self.lr_preconditioner = lr_preconditioner\n",
        "        self.momentum = momentum if (0<momentum<1) else 0.0\n",
        "        self.grad_clip_max_norm = grad_clip_max_norm\n",
        "        self.preconditioner_update_probability = preconditioner_update_probability\n",
        "        self.exact_hessian_vector_product = exact_hessian_vector_product\n",
        "        # protected members\n",
        "        params_with_grad = [params_with_grad,] if isinstance(params_with_grad, torch.Tensor) else params_with_grad\n",
        "        self._params_with_grad = [param for param in params_with_grad if param.requires_grad] # double check requires_grad flag\n",
        "        dtype, device = self._params_with_grad[0].dtype, self._params_with_grad[0].device\n",
        "        self._tiny = torch.finfo(dtype).tiny\n",
        "        self._delta_param_scale = torch.finfo(dtype).eps**0.5\n",
        "        self._param_sizes = [torch.numel(param) for param in self._params_with_grad]\n",
        "        self._param_cumsizes = torch.cumsum(torch.tensor(self._param_sizes), 0)\n",
        "        num_params = self._param_cumsizes[-1]\n",
        "        self._U = torch.randn(num_params, rank_of_approximation, dtype=dtype, device=device) / (num_params*rank_of_approximation)**0.5\n",
        "        self._V = torch.randn(num_params, rank_of_approximation, dtype=dtype, device=device) / (num_params*rank_of_approximation)**0.5\n",
        "        self._d = torch.ones( num_params, 1, dtype=dtype, device=device) * preconditioner_init_scale\n",
        "        self._m = None # momentum buffer \n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure):\n",
        "        \"\"\"\n",
        "        Performs a single step of PSGD with low-rank approximation (UVd) preconditioner, i.e., \n",
        "        updating the trainable parameters once, and returning what closure returns.\n",
        "\n",
        "        Args:\n",
        "            closure (callable): a closure that evaluates the function of self._params_with_grad,\n",
        "                                and returns the loss, or an iterable with the first one being loss.\n",
        "                                Random numbers, if any, used inside the closure should be generated starting \n",
        "                                from the same rng state if self.exact_hessian_vector_product = False; otherwise doesn't matter. \n",
        "        \"\"\"\n",
        "        if torch.rand([]) < self.preconditioner_update_probability:\n",
        "            # evaluates gradients, Hessian-vector product, and updates the preconditioner\n",
        "            if self.exact_hessian_vector_product:\n",
        "                # exact Hessian-vector product\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad, create_graph=True)\n",
        "                    vs = [torch.randn_like(param) for param in self._params_with_grad]\n",
        "                    Hvs = torch.autograd.grad(grads, self._params_with_grad, vs)\n",
        "            else:\n",
        "                # approximate Hessian-vector product via finite-difference formulae. Use it with cautions.\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "                vs = [self._delta_param_scale * torch.randn_like(param) for param in self._params_with_grad]\n",
        "                [param.add_(v) for (param, v) in zip(self._params_with_grad, vs)]\n",
        "                with torch.enable_grad():\n",
        "                    perturbed_returns = closure()\n",
        "                    perturbed_loss = perturbed_returns if isinstance(perturbed_returns, torch.Tensor) else perturbed_returns[0]\n",
        "                    perturbed_grads = torch.autograd.grad(perturbed_loss, self._params_with_grad)\n",
        "                Hvs = [perturbed_g - g for (perturbed_g, g) in zip(perturbed_grads, grads)]\n",
        "            # update preconditioner\n",
        "            v = torch.cat([torch.flatten(v) for v in vs])\n",
        "            h = torch.cat([torch.flatten(h) for h in Hvs])\n",
        "            if self.exact_hessian_vector_product:\n",
        "                update_precond_UVd_math_(self._U, self._V, self._d,\n",
        "                                         v[:,None], h[:,None], step=self.lr_preconditioner, tiny=self._tiny)\n",
        "            else: # compensate the levels of v and h; helpful to reduce numerical errors in half-precision training\n",
        "                update_precond_UVd_math_(self._U, self._V, self._d,\n",
        "                                         v[:,None]/self._delta_param_scale, h[:,None]/self._delta_param_scale, step=self.lr_preconditioner, tiny=self._tiny)\n",
        "        else:\n",
        "            # only evaluates the gradients\n",
        "            with torch.enable_grad():\n",
        "                closure_returns = closure()\n",
        "                loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "            vs = None # no vs and Hvs\n",
        "\n",
        "        # preconditioned gradients; momentum is optional\n",
        "        grad = torch.cat([torch.flatten(g) for g in grads])\n",
        "        if self.momentum > 0:\n",
        "            if self._m is None:\n",
        "                self._m = (1 - self.momentum)*grad\n",
        "            else:\n",
        "                self._m.mul_(self.momentum).add_((1 - self.momentum)*grad)\n",
        "            pre_grad = precond_grad_UVd_math(self._U, self._V, self._d, self._m[:, None])\n",
        "        else:\n",
        "            self._m = None # clean the buffer when momentum is set to zero \n",
        "            pre_grad = precond_grad_UVd_math(self._U, self._V, self._d, grad[:, None])\n",
        "            \n",
        "        # gradient clipping is optional\n",
        "        if self.grad_clip_max_norm is None:\n",
        "            lr = self.lr_params\n",
        "        else:\n",
        "            grad_norm = torch.linalg.vector_norm(pre_grad) + self._tiny\n",
        "            lr = self.lr_params * min(self.grad_clip_max_norm/grad_norm, 1.0)\n",
        "            \n",
        "        # update the parameters\n",
        "        if self.exact_hessian_vector_product or (vs is None):\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param))\n",
        "             for (param, i, j) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes)]\n",
        "        else: # in this case, do not forget to remove the perturbation on parameters\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param) + v)\n",
        "             for (param, i, j, v) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes, vs)]\n",
        "        # return whatever closure returns\n",
        "        return closure_returns\n",
        "\n",
        "################## end of UVd preconditioner #################################\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "# An Xmat (X-matrix) preconditioner is defined by\n",
        "#\n",
        "#   Q = diag(a) + adiag(b)\n",
        "#\n",
        "# where adiag means anti-diagonal.\n",
        "# It's slightly more complicated than a diagonal preconditioner, but performs better.\n",
        "#\n",
        "\n",
        "#@torch.jit.script\n",
        "def update_precond_Xmat_math_(a, b, v, h, step, tiny):\n",
        "    # type: (Tensor, Tensor, Tensor, Tensor, float, float) -> None\n",
        "    \"\"\"\n",
        "    Update preconditioner Q = diag(a) + adiag(b) with (vector, Hessian-vector product) = (v, h).\n",
        "    State variables a and b are updated inplace.\n",
        "    \"\"\"\n",
        "    Qh = a*h + b*torch.flip(h, [0])\n",
        "    aflip, bflip = torch.flip(a, [0]), torch.flip(b, [0])\n",
        "    invQtv = (aflip*v - bflip*torch.flip(v, [0]))/(a*aflip - b*bflip)\n",
        "    nablaA = Qh*Qh - invQtv*invQtv\n",
        "    nablaB = Qh*torch.flip(Qh, [0]) - invQtv*torch.flip(invQtv, [0])\n",
        "    q, r = divmod(len(nablaB), 2)\n",
        "    if r == 1:\n",
        "        nablaB[q] = 0\n",
        "\n",
        "    mu = step/(torch.maximum(torch.max(torch.abs(nablaA)), torch.max(torch.abs(nablaB))) + tiny)\n",
        "    a.sub_(mu*(nablaA*a + nablaB*bflip))\n",
        "    b.sub_(mu*(nablaA*b + nablaB*aflip))\n",
        "\n",
        "#@torch.jit.script\n",
        "def precond_grad_Xmat_math(a, b, g):\n",
        "    # type: (Tensor, Tensor, Tensor) -> Tensor\n",
        "    \"\"\"\n",
        "    Preconditioning gradient g with Q = diag(a) + adiag(b).\n",
        "    \"\"\"\n",
        "    ab = a * b\n",
        "    return (a*a + torch.flip(b*b, [0]))*g + (ab + torch.flip(ab, [0]))*torch.flip(g, [0])\n",
        "\n",
        "from torch.optim.optimizer import Optimizer\n",
        "class XMat(Optimizer):\n",
        "    \"\"\"\n",
        "    Implements the Xmat preconditioner, Q = diag(a) + adiag(b), as a class.\n",
        "    Args for initialization:\n",
        "        params_with_grad: a list of parameters or variables requiring gradients;\n",
        "        preconditioner_init_scale: initial scale of Q, i.e., Q = preconditioner_init_scale*eye();\n",
        "        lr_params: normalized learning rate for parameters in range [0, 1];\n",
        "        lr_preconditioner: normalized learning rate for preconditioner in range [0, 1];\n",
        "        momentum: momentum factor in range [0,1);\n",
        "        grad_clip_max_norm: maximum allowable gradient norm after clipping, None for no clipping;\n",
        "        preconditioner_update_probability: probability on updating Q, 1 for updating at every step, and 0 for never, i.e., SGD;\n",
        "        exact_hessian_vector_product: True for exact Hessian-vector product via 2nd derivative,\n",
        "                                    and False for approximate one via finite-difference formulae.\n",
        "    Notes:\n",
        "        Note 1: The Hessian-vector product can be approximated using the finite-difference formulae by setting\n",
        "        exact_hessian_vector_product = False when the 2nd derivatives is not available.\n",
        "        In this case, make sure that the closure produces the same outputs given the same inputs,\n",
        "        except for numerical errors due to non-deterministic behaviors.\n",
        "        Random numbers, if any, used inside the closure should be generated starting from the same state, where the rng state can be\n",
        "        read and set by, e.g., `torch.cuda.get_rng_state' and `torch.cuda.set_rng_state', respectively.\n",
        "        \n",
        "        Note 2: Momentum here is the moving average of gradient so that its setting is decoupled from the learning rate.\n",
        "        This is necessary as the learning rate in PSGD is normalized.\n",
        "\n",
        "        Note 3: Currently, no support of sparse and mixed-precision gradients.\n",
        "\n",
        "        Note 4: lr_params, lr_preconditioner, momentum, grad_clip_max_norm, preconditioner_update_probability, and\n",
        "        exact_hessian_vector_product (bool) all can be reset on the fly.\n",
        "    \"\"\"\n",
        "    def __init__(self, params_with_grad, preconditioner_init_scale=1.0,\n",
        "                 lr_params=0.01, lr_preconditioner=0.01, momentum=0.0, \n",
        "                 grad_clip_max_norm=None, preconditioner_update_probability=1.0,\n",
        "                 exact_hessian_vector_product: bool = True):\n",
        "        # mutable members\n",
        "        self.lr_params = lr_params\n",
        "        self.lr_preconditioner = lr_preconditioner\n",
        "        self.momentum = momentum if (0<momentum<1) else 0.0\n",
        "        self.grad_clip_max_norm = grad_clip_max_norm\n",
        "        self.preconditioner_update_probability = preconditioner_update_probability\n",
        "        self.exact_hessian_vector_product = exact_hessian_vector_product\n",
        "        # protected members\n",
        "        params_with_grad = [params_with_grad, ] if isinstance(params_with_grad, torch.Tensor) else params_with_grad\n",
        "        self._params_with_grad = [param for param in params_with_grad if param.requires_grad]  # double check requires_grad flag\n",
        "        dtype, device = self._params_with_grad[0].dtype, self._params_with_grad[0].device\n",
        "        self._tiny = torch.finfo(dtype).tiny\n",
        "        self._delta_param_scale = torch.finfo(dtype).eps ** 0.5\n",
        "        self._param_sizes = [torch.numel(param) for param in self._params_with_grad]\n",
        "        self._param_cumsizes = torch.cumsum(torch.tensor(self._param_sizes), 0)\n",
        "        num_params = self._param_cumsizes[-1]\n",
        "        self._a = torch.ones(num_params, dtype=dtype, device=device)*preconditioner_init_scale\n",
        "        self._b = torch.zeros(num_params, dtype=dtype, device=device)\n",
        "        self._m = None # buffer for momentum \n",
        "        defaults = dict(lr=lr_params)\n",
        "        super(XMat, self).__init__(self._params_with_grad, defaults)        \n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure):\n",
        "        \"\"\"\n",
        "        Performs a single step of PSGD with Xmat preconditioner, i.e.,\n",
        "        updating the trainable parameters once, and returning what closure returns.\n",
        "        Args:\n",
        "            closure (callable): a closure that evaluates the function of self._params_with_grad,\n",
        "                                and returns the loss, or an iterable with the first one being loss.\n",
        "                                Random numbers, if any, used inside the closure should be generated starting\n",
        "                                from the same rng state if self.exact_hessian_vector_product = False; otherwise doesn't matter.\n",
        "        \"\"\"\n",
        "        if torch.rand([]) < self.preconditioner_update_probability:\n",
        "            # evaluates gradients, Hessian-vector product, and updates the preconditioner\n",
        "            if self.exact_hessian_vector_product:\n",
        "                # exact Hessian-vector product\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad, create_graph=True)\n",
        "                    vs = [torch.randn_like(param) for param in self._params_with_grad]\n",
        "                    Hvs = torch.autograd.grad(grads, self._params_with_grad, vs)\n",
        "            else:\n",
        "                # approximate Hessian-vector product via finite-difference formulae. Use it with cautions.\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "                vs = [self._delta_param_scale * torch.randn_like(param) for param in self._params_with_grad]\n",
        "                [param.add_(v) for (param, v) in zip(self._params_with_grad, vs)]\n",
        "                with torch.enable_grad():\n",
        "                    perturbed_returns = closure()\n",
        "                    perturbed_loss = perturbed_returns if isinstance(perturbed_returns, torch.Tensor) else perturbed_returns[0]\n",
        "                    perturbed_grads = torch.autograd.grad(perturbed_loss, self._params_with_grad)\n",
        "                Hvs = [perturbed_g - g for (perturbed_g, g) in zip(perturbed_grads, grads)]\n",
        "            # update preconditioner\n",
        "            v = torch.cat([torch.flatten(v) for v in vs])\n",
        "            h = torch.cat([torch.flatten(h) for h in Hvs])\n",
        "            if self.exact_hessian_vector_product:\n",
        "                update_precond_Xmat_math_(self._a, self._b,\n",
        "                                         v, h, step=self.lr_preconditioner, tiny=self._tiny)\n",
        "            else:  # compensate the levels of v and h; helpful to reduce numerical errors in half-precision training\n",
        "                update_precond_Xmat_math_(self._a, self._b,\n",
        "                                         v/self._delta_param_scale, h/self._delta_param_scale,\n",
        "                                         step=self.lr_preconditioner, tiny=self._tiny)\n",
        "        else:\n",
        "            # only evaluates the gradients\n",
        "            with torch.enable_grad():\n",
        "                closure_returns = closure()\n",
        "                loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "            vs = None  # no vs and Hvs\n",
        "\n",
        "        # preconditioned gradients; momentum is optional        \n",
        "        grad = torch.cat([torch.flatten(g) for g in grads])\n",
        "        if self.momentum > 0:\n",
        "            if self._m is None:\n",
        "                self._m = (1 - self.momentum)*grad\n",
        "            else:\n",
        "                self._m.mul_(self.momentum).add_((1 - self.momentum)*grad)\n",
        "            pre_grad = precond_grad_Xmat_math(self._a, self._b, self._m)\n",
        "        else:\n",
        "            self._m = None # clean the buffer when momentum is set to zero again \n",
        "            pre_grad = precond_grad_Xmat_math(self._a, self._b, grad)\n",
        "        \n",
        "        # gradient clipping is optional\n",
        "        if self.grad_clip_max_norm is None:\n",
        "            lr = self.lr_params\n",
        "        else:\n",
        "            grad_norm = torch.linalg.vector_norm(pre_grad) + self._tiny\n",
        "            lr = self.lr_params * min(self.grad_clip_max_norm / grad_norm, 1.0)\n",
        "\n",
        "        # update the parameters\n",
        "        if self.exact_hessian_vector_product or (vs is None):\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param))\n",
        "             for (param, i, j) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes)]\n",
        "        else:  # in this case, do not forget to remove the perturbation on parameters\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param) + v)\n",
        "             for (param, i, j, v) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes, vs)]\n",
        "        # return whatever closure returns\n",
        "        return closure_returns\n",
        "\n",
        "################## end of Xmat preconditioner #################################\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# The classic Newton–Raphson type preconditioner.\n",
        "# Clearly, it is applicable only to small scale problems \n",
        "#\n",
        "\n",
        "# @torch.jit.script\n",
        "def update_precond_newton_math_(Q, v, h, step, tiny):\n",
        "    # type: (Tensor, Tensor, Tensor, float, float) -> None\n",
        "    \"\"\"\n",
        "    Update the classic Newton–Raphson type preconditioner P = Q'*Q with (v, h).\n",
        "    \"\"\"\n",
        "    a = Q.mm(h)\n",
        "    b = torch.linalg.solve_triangular(Q.t(), v, upper=False)\n",
        "    grad = torch.triu(a.mm(a.t()) - b.mm(b.t()))\n",
        "    mu = step/(grad.abs().max() + tiny)      \n",
        "    Q.sub_(mu*grad.mm(Q))\n",
        "\n",
        "class Newton:\n",
        "    \"\"\"\n",
        "    Implements the classic Newton–Raphson type preconditioner for SGD as a class.\n",
        "    Args for initialization:\n",
        "        params_with_grad: a list of parameters or variables requiring gradients;\n",
        "        preconditioner_init_scale: initial scale of Q, i.e., Q = preconditioner_init_scale*eye();\n",
        "        lr_params: normalized learning rate for parameters in range [0, 1];\n",
        "        lr_preconditioner: normalized learning rate for preconditioner in range [0, 1];\n",
        "        momentum: momentum factor in range [0,1);\n",
        "        grad_clip_max_norm: maximum allowable gradient norm after clipping, None for no clipping;\n",
        "        preconditioner_update_probability: probability on updating Q, 1 for updating at every step, and 0 for never, i.e., SGD;\n",
        "        exact_hessian_vector_product: True for exact Hessian-vector product via 2nd derivative,\n",
        "                                    and False for approximate one via finite-difference formulae.\n",
        "    Notes:\n",
        "        Note 1: The Hessian-vector product can be approximated using the finite-difference formulae by setting\n",
        "        exact_hessian_vector_product = False when the 2nd derivatives is not available.\n",
        "        In this case, make sure that the closure produces the same outputs given the same inputs,\n",
        "        except for numerical errors due to non-deterministic behaviors.\n",
        "        Random numbers, if any, used inside the closure should be generated starting from the same state, where the rng state can be\n",
        "        read and set by, e.g., `torch.cuda.get_rng_state' and `torch.cuda.set_rng_state', respectively.\n",
        "        \n",
        "        Note 2: Momentum here is the moving average of gradient so that its setting is decoupled from the learning rate.\n",
        "        This is necessary as the learning rate in PSGD is normalized.\n",
        "        Note 3: Currently, no support of sparse and mixed-precision gradients.\n",
        "        Note 4: lr_params, lr_preconditioner, momentum, grad_clip_max_norm, preconditioner_update_probability, and\n",
        "        exact_hessian_vector_product (bool) all can be reset on the fly.\n",
        "    \"\"\"\n",
        "    def __init__(self, params_with_grad, preconditioner_init_scale=1.0,\n",
        "                 lr_params=0.01, lr_preconditioner=0.01, momentum=0.0, \n",
        "                 grad_clip_max_norm=None, preconditioner_update_probability=1.0,\n",
        "                 exact_hessian_vector_product: bool = True):\n",
        "        # mutable members\n",
        "        self.lr_params = lr_params\n",
        "        self.lr_preconditioner = lr_preconditioner\n",
        "        self.momentum = momentum if (0<momentum<1) else 0.0\n",
        "        self.grad_clip_max_norm = grad_clip_max_norm\n",
        "        self.preconditioner_update_probability = preconditioner_update_probability\n",
        "        self.exact_hessian_vector_product = exact_hessian_vector_product\n",
        "        # protected members\n",
        "        params_with_grad = [params_with_grad, ] if isinstance(params_with_grad, torch.Tensor) else params_with_grad\n",
        "        self._params_with_grad = [param for param in params_with_grad if param.requires_grad]  # double check requires_grad flag\n",
        "        dtype, device = self._params_with_grad[0].dtype, self._params_with_grad[0].device\n",
        "        self._tiny = torch.finfo(dtype).tiny\n",
        "        self._delta_param_scale = torch.finfo(dtype).eps ** 0.5\n",
        "        self._param_sizes = [torch.numel(param) for param in self._params_with_grad]\n",
        "        self._param_cumsizes = torch.cumsum(torch.tensor(self._param_sizes), 0)\n",
        "        num_params = self._param_cumsizes[-1]\n",
        "        self._Q = torch.eye(num_params, dtype=dtype, device=device)*preconditioner_init_scale\n",
        "        self._m = None # buffer for momentum \n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure):\n",
        "        \"\"\"\n",
        "        Performs a single step of PSGD with Newton–Raphson preconditioner, i.e.,\n",
        "        updating the trainable parameters once, and returning what closure returns.\n",
        "        Args:\n",
        "            closure (callable): a closure that evaluates the function of self._params_with_grad,\n",
        "                                and returns the loss, or an iterable with the first one being loss.\n",
        "                                Random numbers, if any, used inside the closure should be generated starting\n",
        "                                from the same rng state if self.exact_hessian_vector_product = False; otherwise doesn't matter.\n",
        "        \"\"\"\n",
        "        if torch.rand([]) < self.preconditioner_update_probability:\n",
        "            # evaluates gradients, Hessian-vector product, and updates the preconditioner\n",
        "            if self.exact_hessian_vector_product:\n",
        "                # exact Hessian-vector product\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad, create_graph=True)\n",
        "                    vs = [torch.randn_like(param) for param in self._params_with_grad]\n",
        "                    Hvs = torch.autograd.grad(grads, self._params_with_grad, vs)\n",
        "            else:\n",
        "                # approximate Hessian-vector product via finite-difference formulae. Use it with cautions.\n",
        "                with torch.enable_grad():\n",
        "                    closure_returns = closure()\n",
        "                    loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                    grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "                vs = [self._delta_param_scale * torch.randn_like(param) for param in self._params_with_grad]\n",
        "                [param.add_(v) for (param, v) in zip(self._params_with_grad, vs)]\n",
        "                with torch.enable_grad():\n",
        "                    perturbed_returns = closure()\n",
        "                    perturbed_loss = perturbed_returns if isinstance(perturbed_returns, torch.Tensor) else perturbed_returns[0]\n",
        "                    perturbed_grads = torch.autograd.grad(perturbed_loss, self._params_with_grad)\n",
        "                Hvs = [perturbed_g - g for (perturbed_g, g) in zip(perturbed_grads, grads)]\n",
        "            # update preconditioner\n",
        "            v = torch.cat([torch.flatten(v) for v in vs])\n",
        "            h = torch.cat([torch.flatten(h) for h in Hvs])\n",
        "            if self.exact_hessian_vector_product:\n",
        "                update_precond_newton_math_(self._Q,\n",
        "                                            v[:,None], h[:,None], step=self.lr_preconditioner, tiny=self._tiny)\n",
        "            else:  # compensate the levels of v and h; helpful to reduce numerical errors in half-precision training\n",
        "                update_precond_newton_math_(self._Q,\n",
        "                                            v[:,None]/self._delta_param_scale, h[:,None]/self._delta_param_scale,\n",
        "                                            step=self.lr_preconditioner, tiny=self._tiny)\n",
        "        else:\n",
        "            # only evaluates the gradients\n",
        "            with torch.enable_grad():\n",
        "                closure_returns = closure()\n",
        "                loss = closure_returns if isinstance(closure_returns, torch.Tensor) else closure_returns[0]\n",
        "                grads = torch.autograd.grad(loss, self._params_with_grad)\n",
        "            vs = None  # no vs and Hvs\n",
        "\n",
        "        # preconditioned gradients; momentum is optional        \n",
        "        grad = torch.cat([torch.flatten(g) for g in grads])\n",
        "        if self.momentum > 0:\n",
        "            if self._m is None:\n",
        "                self._m = (1 - self.momentum)*grad\n",
        "            else:\n",
        "                self._m.mul_(self.momentum).add_((1 - self.momentum)*grad)\n",
        "            pre_grad = self._Q.t() @ (self._Q @ self._m)\n",
        "        else:\n",
        "            self._m = None # clean the buffer when momentum is set to zero again \n",
        "            pre_grad = self._Q.t() @ (self._Q @ grad)\n",
        "        \n",
        "        # gradient clipping is optional\n",
        "        if self.grad_clip_max_norm is None:\n",
        "            lr = self.lr_params\n",
        "        else:\n",
        "            grad_norm = torch.linalg.vector_norm(pre_grad) + self._tiny\n",
        "            lr = self.lr_params * min(self.grad_clip_max_norm / grad_norm, 1.0)\n",
        "\n",
        "        # update the parameters\n",
        "        if self.exact_hessian_vector_product or (vs is None):\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param))\n",
        "             for (param, i, j) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes)]\n",
        "        else:  # in this case, do not forget to remove the perturbation on parameters\n",
        "            [param.subtract_(lr * pre_grad[j - i:j].view_as(param) + v)\n",
        "             for (param, i, j, v) in zip(self._params_with_grad, self._param_sizes, self._param_cumsizes, vs)]\n",
        "        # return whatever closure returns\n",
        "        return closure_returns\n",
        "\n",
        "################## end of Newton–Raphson preconditioner #################################"
      ],
      "metadata": {
        "id": "rMjTZHp-FJbY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHpYeQrOLcmZ"
      },
      "source": [
        "### Define Parameters and Helpers, and start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNLegt4jLkRD"
      },
      "outputs": [],
      "source": [
        "# Result Visualization Helper\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "M, N = 4, 6\n",
        "RESULT_IMG_PATH = '/tmp/test_result.png'\n",
        "\n",
        "def plot_results(images, labels, preds):\n",
        "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
        "  inv_norm = transforms.Normalize((-0.1307/0.3081,), (1/0.3081,))\n",
        "\n",
        "  num_images = images.shape[0]\n",
        "  fig, axes = plt.subplots(M, N, figsize=(11, 9))\n",
        "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
        "\n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.axis('off')\n",
        "    if i >= num_images:\n",
        "      continue\n",
        "    img, label, prediction = images[i], labels[i], preds[i]\n",
        "    img = inv_norm(img)\n",
        "    img = img.squeeze() # [1,Y,X] -> [Y,X]\n",
        "    label, prediction = label.item(), prediction.item()\n",
        "    if label == prediction:\n",
        "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
        "    else:\n",
        "      ax.set_title(\n",
        "          'X {}/{}'.format(label, prediction), color='red')\n",
        "    ax.imshow(img)\n",
        "  plt.savefig(RESULT_IMG_PATH, transparent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNh-oEmHmorI"
      },
      "outputs": [],
      "source": [
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "FLAGS['datadir'] = \"/tmp/mnist\"\n",
        "FLAGS['batch_size'] = 128\n",
        "FLAGS['num_workers'] = 2\n",
        "FLAGS['learning_rate'] = 0.1\n",
        "FLAGS['momentum'] = 0.5\n",
        "FLAGS['num_epochs'] = 10\n",
        "FLAGS['num_cores'] = 8\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTmxZL5ymp8P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "SERIAL_EXEC = xmp.MpSerialExecutor()\n",
        "\n",
        "class MNIST(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MNIST, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.bn1 = nn.BatchNorm2d(10)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.bn2 = nn.BatchNorm2d(20)\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = self.bn1(x)\n",
        "    x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "    x = self.bn2(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Only instantiate model weights once in memory.\n",
        "WRAPPED_MODEL = xmp.MpModelWrapper(MNIST())\n",
        "\n",
        "def train_mnist():\n",
        "  torch.manual_seed(1)\n",
        "  \n",
        "  def get_dataset():\n",
        "    norm = transforms.Normalize((0.1307,), (0.3081,))\n",
        "    train_dataset = datasets.MNIST(\n",
        "        FLAGS['datadir'],\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.ToTensor(), norm]))\n",
        "    test_dataset = datasets.MNIST(\n",
        "        FLAGS['datadir'],\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transforms.Compose(\n",
        "            [transforms.ToTensor(), norm]))\n",
        "    \n",
        "    return train_dataset, test_dataset\n",
        "  \n",
        "  # Using the serial executor avoids multiple processes to\n",
        "  # download the same data.\n",
        "  train_dataset, test_dataset = SERIAL_EXEC.run(get_dataset)\n",
        "\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=FLAGS['batch_size'],\n",
        "      shuffle=False,\n",
        "      num_workers=FLAGS['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  # Scale learning rate to world size\n",
        "  lr = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "\n",
        "  # Get loss function, optimizer, and model\n",
        "  device = xm.xla_device()\n",
        "  model = WRAPPED_MODEL.to(device)\n",
        "  # optimizer = optim.SGD(model.parameters(), lr=lr, momentum=FLAGS['momentum'])\n",
        "  optimizer = XMat(model.parameters(),lr_params=lr,momentum=0.9,preconditioner_update_probability=0.1)\n",
        "  loss_fn = nn.NLLLoss()\n",
        "\n",
        "  def train_loop_fn(loader):\n",
        "    tracker = xm.RateTracker()\n",
        "    model.train()\n",
        "    for x, (data, target) in enumerate(loader):\n",
        "      output = model(data)\n",
        "      loss = loss_fn(output, target)\n",
        "      def closure():\n",
        "          return loss \n",
        "      # if using single TPU one can use mark_step after standatd optimizer.step(closure)\n",
        "      #xm.mark_step()\n",
        "      xm.optimizer_step(optimizer, optimizer_args={'closure':closure})\n",
        "      tracker.add(FLAGS['batch_size'])\n",
        "      if x % FLAGS['log_steps'] == 0:\n",
        "        print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "            xm.get_ordinal(), x, loss.item(), tracker.rate(),\n",
        "            tracker.global_rate(), time.asctime()), flush=True)\n",
        "\n",
        "  def test_loop_fn(loader):\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "    model.eval()\n",
        "    data, pred, target = None, None, None\n",
        "    for data, target in loader:\n",
        "      output = model(data)\n",
        "      pred = output.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      total_samples += data.size()[0]\n",
        "\n",
        "    accuracy = 100.0 * correct / total_samples\n",
        "    print('[xla:{}] Accuracy={:.2f}%'.format(\n",
        "        xm.get_ordinal(), accuracy), flush=True)\n",
        "    return accuracy, data, pred, target\n",
        "\n",
        "  # Train and eval loops\n",
        "  accuracy = 0.0\n",
        "  data, pred, target = None, None, None\n",
        "  for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "    train_loop_fn(para_loader.per_device_loader(device))\n",
        "    xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "\n",
        "    para_loader = pl.ParallelLoader(test_loader, [device])\n",
        "    accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n",
        "    if FLAGS['metrics_debug']:\n",
        "      xm.master_print(met.metrics_report(), flush=True)\n",
        "\n",
        "  return accuracy, data, pred, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afwo4H7kSd8P",
        "outputId": "3711fb7a-213e-49a2-83ed-8ecdbe17ce19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[xla:0](0) Loss=2.33622 Rate=360.25 GlobalRate=360.23 Time=Tue Mar 21 22:05:38 2023\n",
            "[xla:2](0) Loss=2.36655 Rate=170.40 GlobalRate=170.39 Time=Tue Mar 21 22:05:40 2023\n",
            "[xla:3](0) Loss=2.35576 Rate=77.56 GlobalRate=77.56 Time=Tue Mar 21 22:05:41 2023\n",
            "[xla:4](0) Loss=2.38184 Rate=64.69 GlobalRate=64.69 Time=Tue Mar 21 22:05:42 2023\n",
            "[xla:0](20) Loss=0.36951 Rate=519.97 GlobalRate=605.15 Time=Tue Mar 21 22:05:42 2023\n",
            "[xla:1](0) Loss=2.36546 Rate=44.22 GlobalRate=44.22 Time=Tue Mar 21 22:05:43 2023\n",
            "[xla:7](0) Loss=2.33757 Rate=36.86 GlobalRate=36.86 Time=Tue Mar 21 22:05:44 2023\n",
            "[xla:6](0) Loss=2.39576 Rate=29.96 GlobalRate=29.96 Time=Tue Mar 21 22:05:46 2023\n",
            "[xla:2](20) Loss=0.38797 Rate=241.12 GlobalRate=279.08 Time=Tue Mar 21 22:05:49 2023\n",
            "[xla:5](0) Loss=2.35329 Rate=27.54 GlobalRate=27.54 Time=Tue Mar 21 22:05:50 2023\n",
            "[xla:3](20) Loss=0.40883 Rate=162.62 GlobalRate=201.76 Time=Tue Mar 21 22:05:53 2023\n",
            "[xla:4](20) Loss=0.19045 Rate=148.03 GlobalRate=184.71 Time=Tue Mar 21 22:05:54 2023\n",
            "[xla:0](40) Loss=0.27515 Rate=328.07 GlobalRate=304.52 Time=Tue Mar 21 22:05:55 2023\n",
            "[xla:1](20) Loss=0.20308 Rate=134.13 GlobalRate=167.11 Time=Tue Mar 21 22:05:56 2023\n",
            "[xla:7](20) Loss=0.29599 Rate=132.59 GlobalRate=162.84 Time=Tue Mar 21 22:05:57 2023\n",
            "[xla:6](20) Loss=0.17262 Rate=138.84 GlobalRate=164.09 Time=Tue Mar 21 22:05:59 2023\n",
            "[xla:2](40) Loss=0.19393 Rate=232.29 GlobalRate=250.63 Time=Tue Mar 21 22:06:00 2023\n",
            "[xla:5](20) Loss=0.33367 Rate=154.72 GlobalRate=175.27 Time=Tue Mar 21 22:06:01 2023\n",
            "Finished training epoch 1\n",
            "[xla:3](40) Loss=0.21777 Rate=227.97 GlobalRate=230.68 Time=Tue Mar 21 22:06:02 2023\n",
            "[xla:4](40) Loss=0.08850 Rate=225.24 GlobalRate=220.46 Time=Tue Mar 21 22:06:04 2023\n",
            "[xla:1](40) Loss=0.14228 Rate=240.80 GlobalRate=216.03 Time=Tue Mar 21 22:06:05 2023\n",
            "[xla:7](40) Loss=0.14473 Rate=252.79 GlobalRate=216.89 Time=Tue Mar 21 22:06:05 2023\n",
            "[xla:6](40) Loss=0.18559 Rate=252.49 GlobalRate=217.04 Time=Tue Mar 21 22:06:06 2023\n",
            "[xla:5](40) Loss=0.14162 Rate=246.82 GlobalRate=221.98 Time=Tue Mar 21 22:06:09 2023\n",
            "[xla:0] Accuracy=95.95%\n",
            "[xla:0](0) Loss=0.18850 Rate=43.19 GlobalRate=43.19 Time=Tue Mar 21 22:06:44 2023\n",
            "[xla:2] Accuracy=96.64%\n",
            "[xla:3] Accuracy=93.13%\n",
            "[xla:4] Accuracy=95.04%\n",
            "[xla:2](0) Loss=0.10780 Rate=61.48 GlobalRate=61.48 Time=Tue Mar 21 22:06:49 2023\n",
            "[xla:3](0) Loss=0.24743 Rate=60.59 GlobalRate=60.59 Time=Tue Mar 21 22:06:50 2023\n",
            "[xla:1] Accuracy=94.62%\n",
            "[xla:7] Accuracy=95.84%\n",
            "[xla:4](0) Loss=0.15213 Rate=56.46 GlobalRate=56.46 Time=Tue Mar 21 22:06:51 2023\n",
            "[xla:6] Accuracy=95.15%\n",
            "[xla:0](20) Loss=0.13302 Rate=225.15 GlobalRate=259.64 Time=Tue Mar 21 22:06:52 2023\n",
            "[xla:1](0) Loss=0.21882 Rate=62.83 GlobalRate=62.82 Time=Tue Mar 21 22:06:53 2023\n",
            "[xla:7](0) Loss=0.21765 Rate=46.04 GlobalRate=46.04 Time=Tue Mar 21 22:06:53 2023\n",
            "[xla:5] Accuracy=96.33%\n",
            "[xla:6](0) Loss=0.17167 Rate=32.39 GlobalRate=32.39 Time=Tue Mar 21 22:06:56 2023\n",
            "[xla:2](20) Loss=0.12297 Rate=153.15 GlobalRate=191.59 Time=Tue Mar 21 22:07:00 2023\n",
            "[xla:5](0) Loss=0.14098 Rate=26.22 GlobalRate=26.22 Time=Tue Mar 21 22:07:01 2023\n",
            "[xla:3](20) Loss=0.14321 Rate=147.93 GlobalRate=185.00 Time=Tue Mar 21 22:07:02 2023\n",
            "[xla:4](20) Loss=0.09682 Rate=131.42 GlobalRate=164.10 Time=Tue Mar 21 22:07:05 2023\n",
            "[xla:0](40) Loss=0.16615 Rate=201.29 GlobalRate=217.20 Time=Tue Mar 21 22:07:06 2023\n",
            "[xla:1](20) Loss=0.05411 Rate=134.70 GlobalRate=167.42 Time=Tue Mar 21 22:07:07 2023\n",
            "[xla:7](20) Loss=0.08718 Rate=130.34 GlobalRate=162.87 Time=Tue Mar 21 22:07:07 2023\n",
            "[xla:6](20) Loss=0.02695 Rate=138.96 GlobalRate=166.52 Time=Tue Mar 21 22:07:08 2023\n",
            "[xla:2](40) Loss=0.03967 Rate=217.87 GlobalRate=220.15 Time=Tue Mar 21 22:07:10 2023\n",
            "[xla:5](20) Loss=0.18459 Rate=168.21 GlobalRate=183.86 Time=Tue Mar 21 22:07:10 2023\n",
            "Finished training epoch 2\n",
            "[xla:3](40) Loss=0.13654 Rate=228.49 GlobalRate=222.36 Time=Tue Mar 21 22:07:11 2023\n",
            "[xla:4](40) Loss=0.03001 Rate=248.31 GlobalRate=216.61 Time=Tue Mar 21 22:07:13 2023\n",
            "[xla:1](40) Loss=0.06658 Rate=251.89 GlobalRate=220.38 Time=Tue Mar 21 22:07:14 2023\n",
            "[xla:7](40) Loss=0.03073 Rate=251.44 GlobalRate=216.76 Time=Tue Mar 21 22:07:15 2023\n",
            "[xla:6](40) Loss=0.04376 Rate=246.72 GlobalRate=217.06 Time=Tue Mar 21 22:07:16 2023\n",
            "[xla:5](40) Loss=0.03663 Rate=234.31 GlobalRate=220.35 Time=Tue Mar 21 22:07:20 2023\n",
            "[xla:0] Accuracy=97.80%\n",
            "[xla:0](0) Loss=0.08700 Rate=50.21 GlobalRate=50.21 Time=Tue Mar 21 22:07:52 2023\n",
            "[xla:2] Accuracy=97.83%\n",
            "[xla:3] Accuracy=96.95%\n",
            "[xla:4] Accuracy=96.88%\n",
            "[xla:2](0) Loss=0.06367 Rate=56.78 GlobalRate=56.78 Time=Tue Mar 21 22:07:56 2023\n",
            "[xla:3](0) Loss=0.05326 Rate=57.95 GlobalRate=57.95 Time=Tue Mar 21 22:07:58 2023\n",
            "[xla:7] Accuracy=96.45%\n",
            "[xla:1] Accuracy=97.33%\n",
            "[xla:4](0) Loss=0.06497 Rate=46.38 GlobalRate=46.38 Time=Tue Mar 21 22:07:59 2023\n",
            "[xla:6] Accuracy=97.05%\n",
            "[xla:0](20) Loss=0.11849 Rate=231.57 GlobalRate=273.95 Time=Tue Mar 21 22:07:59 2023\n",
            "[xla:7](0) Loss=0.12760 Rate=54.93 GlobalRate=54.93 Time=Tue Mar 21 22:08:00 2023\n",
            "[xla:1](0) Loss=0.04367 Rate=41.00 GlobalRate=41.00 Time=Tue Mar 21 22:08:02 2023\n",
            "[xla:5] Accuracy=97.52%[xla:6](0) Loss=0.12088 Rate=41.61 GlobalRate=41.61 Time=Tue Mar 21 22:08:02 2023\n",
            "\n",
            "[xla:2](20) Loss=0.04601 Rate=182.56 GlobalRate=226.57 Time=Tue Mar 21 22:08:06 2023\n",
            "[xla:5](0) Loss=0.05954 Rate=34.74 GlobalRate=34.74 Time=Tue Mar 21 22:08:06 2023\n",
            "[xla:3](20) Loss=0.13148 Rate=161.44 GlobalRate=201.83 Time=Tue Mar 21 22:08:09 2023\n",
            "[xla:4](20) Loss=0.06540 Rate=161.35 GlobalRate=198.87 Time=Tue Mar 21 22:08:09 2023\n",
            "[xla:0](40) Loss=0.09040 Rate=215.37 GlobalRate=235.06 Time=Tue Mar 21 22:08:12 2023\n",
            "[xla:7](20) Loss=0.03470 Rate=142.31 GlobalRate=178.08 Time=Tue Mar 21 22:08:13 2023\n",
            "[xla:1](20) Loss=0.01737 Rate=139.65 GlobalRate=172.48 Time=Tue Mar 21 22:08:14 2023\n",
            "[xla:6](20) Loss=0.03595 Rate=145.59 GlobalRate=179.34 Time=Tue Mar 21 22:08:14 2023\n",
            "[xla:2](40) Loss=0.02014 Rate=221.31 GlobalRate=236.17 Time=Tue Mar 21 22:08:16 2023\n",
            "[xla:5](20) Loss=0.07198 Rate=161.11 GlobalRate=190.39 Time=Tue Mar 21 22:08:17 2023\n",
            "Finished training epoch 3\n",
            "[xla:3](40) Loss=0.05570 Rate=231.30 GlobalRate=232.93 Time=Tue Mar 21 22:08:18 2023\n",
            "[xla:4](40) Loss=0.01860 Rate=236.75 GlobalRate=233.91 Time=Tue Mar 21 22:08:18 2023\n",
            "[xla:7](40) Loss=0.02674 Rate=263.94 GlobalRate=233.10 Time=Tue Mar 21 22:08:20 2023\n",
            "[xla:1](40) Loss=0.03008 Rate=258.80 GlobalRate=226.67 Time=Tue Mar 21 22:08:22 2023\n",
            "[xla:6](40) Loss=0.02904 Rate=256.24 GlobalRate=230.73 Time=Tue Mar 21 22:08:22 2023\n",
            "[xla:5](40) Loss=0.01557 Rate=241.53 GlobalRate=230.25 Time=Tue Mar 21 22:08:25 2023\n",
            "[xla:0] Accuracy=97.90%\n",
            "[xla:0](0) Loss=0.07209 Rate=64.40 GlobalRate=64.40 Time=Tue Mar 21 22:09:04 2023\n",
            "[xla:2] Accuracy=97.85%\n",
            "[xla:2](0) Loss=0.05928 Rate=55.63 GlobalRate=55.62 Time=Tue Mar 21 22:09:07 2023\n",
            "[xla:4] Accuracy=97.34%\n",
            "[xla:3] Accuracy=97.07%\n",
            "[xla:7] Accuracy=97.48%\n",
            "[xla:4](0) Loss=0.04034 Rate=51.65 GlobalRate=51.65 Time=Tue Mar 21 22:09:10 2023\n",
            "[xla:3](0) Loss=0.01812 Rate=54.48 GlobalRate=54.48 Time=Tue Mar 21 22:09:10 2023\n",
            "[xla:1] Accuracy=97.99%\n",
            "[xla:6] Accuracy=97.57%\n",
            "[xla:0](20) Loss=0.06191 Rate=220.40 GlobalRate=272.09 Time=Tue Mar 21 22:09:12 2023\n",
            "[xla:7](0) Loss=0.09014 Rate=34.26 GlobalRate=34.26 Time=Tue Mar 21 22:09:14 2023\n",
            "[xla:5] Accuracy=97.49%\n",
            "[xla:1](0) Loss=0.02456 Rate=32.49 GlobalRate=32.49 Time=Tue Mar 21 22:09:15 2023\n",
            "[xla:6](0) Loss=0.04913 Rate=31.73 GlobalRate=31.73 Time=Tue Mar 21 22:09:16 2023\n",
            "[xla:2](20) Loss=0.04067 Rate=168.61 GlobalRate=210.07 Time=Tue Mar 21 22:09:18 2023\n",
            "[xla:5](0) Loss=0.05233 Rate=33.90 GlobalRate=33.90 Time=Tue Mar 21 22:09:20 2023\n",
            "[xla:4](20) Loss=0.03176 Rate=148.53 GlobalRate=185.50 Time=Tue Mar 21 22:09:22 2023\n",
            "[xla:3](20) Loss=0.10608 Rate=144.70 GlobalRate=181.05 Time=Tue Mar 21 22:09:23 2023\n",
            "[xla:0](40) Loss=0.06689 Rate=215.23 GlobalRate=238.90 Time=Tue Mar 21 22:09:24 2023\n",
            "[xla:7](20) Loss=0.03182 Rate=151.41 GlobalRate=180.52 Time=Tue Mar 21 22:09:25 2023\n",
            "[xla:1](20) Loss=0.00348 Rate=155.76 GlobalRate=182.88 Time=Tue Mar 21 22:09:26 2023[xla:6](20) Loss=0.01131 Rate=158.71 GlobalRate=184.70 Time=Tue Mar 21 22:09:26 2023\n",
            "\n",
            "[xla:2](40) Loss=0.00976 Rate=214.99 GlobalRate=226.15 Time=Tue Mar 21 22:09:28 2023\n",
            "[xla:5](20) Loss=0.02958 Rate=185.75 GlobalRate=211.72 Time=Tue Mar 21 22:09:28 2023\n",
            "Finished training epoch 4\n",
            "[xla:4](40) Loss=0.03273 Rate=253.14 GlobalRate=234.09 Time=Tue Mar 21 22:09:30 2023\n",
            "[xla:3](40) Loss=0.07277 Rate=251.83 GlobalRate=230.52 Time=Tue Mar 21 22:09:31 2023\n",
            "[xla:7](40) Loss=0.03693 Rate=249.77 GlobalRate=228.09 Time=Tue Mar 21 22:09:33 2023\n",
            "[xla:6](40) Loss=0.02676 Rate=240.75 GlobalRate=226.03 Time=Tue Mar 21 22:09:35 2023\n",
            "[xla:1](40) Loss=0.01250 Rate=235.66 GlobalRate=222.77 Time=Tue Mar 21 22:09:35 2023\n",
            "[xla:5](40) Loss=0.00496 Rate=232.01 GlobalRate=233.92 Time=Tue Mar 21 22:09:38 2023\n",
            "[xla:0] Accuracy=98.03%\n",
            "[xla:0](0) Loss=0.02842 Rate=54.06 GlobalRate=54.06 Time=Tue Mar 21 22:10:12 2023\n",
            "[xla:2] Accuracy=98.06%\n",
            "[xla:4] Accuracy=97.46%\n",
            "[xla:2](0) Loss=0.02857 Rate=53.25 GlobalRate=53.25 Time=Tue Mar 21 22:10:16 2023\n",
            "[xla:3] Accuracy=97.29%\n",
            "[xla:4](0) Loss=0.00612 Rate=47.93 GlobalRate=47.92 Time=Tue Mar 21 22:10:19 2023[xla:7] Accuracy=97.52%\n",
            "\n",
            "[xla:3](0) Loss=0.01115 Rate=46.93 GlobalRate=46.93 Time=Tue Mar 21 22:10:19 2023\n",
            "[xla:6] Accuracy=97.38%\n",
            "[xla:1] Accuracy=97.89%\n",
            "[xla:0](20) Loss=0.01925 Rate=191.95 GlobalRate=236.09 Time=Tue Mar 21 22:10:21 2023\n",
            "[xla:7](0) Loss=0.04916 Rate=37.35 GlobalRate=37.35 Time=Tue Mar 21 22:10:22 2023\n",
            "[xla:6](0) Loss=0.05503 Rate=33.86 GlobalRate=33.86 Time=Tue Mar 21 22:10:23 2023\n",
            "[xla:5] Accuracy=97.60%\n",
            "[xla:1](0) Loss=0.01686 Rate=32.18 GlobalRate=32.18 Time=Tue Mar 21 22:10:26 2023\n",
            "[xla:2](20) Loss=0.03497 Rate=167.68 GlobalRate=208.42 Time=Tue Mar 21 22:10:27 2023\n",
            "[xla:5](0) Loss=0.01175 Rate=30.01 GlobalRate=30.01 Time=Tue Mar 21 22:10:28 2023\n",
            "[xla:4](20) Loss=0.01793 Rate=138.76 GlobalRate=173.25 Time=Tue Mar 21 22:10:31 2023\n",
            "[xla:3](20) Loss=0.08693 Rate=134.89 GlobalRate=168.47 Time=Tue Mar 21 22:10:32 2023\n",
            "[xla:0](40) Loss=0.06259 Rate=197.14 GlobalRate=217.33 Time=Tue Mar 21 22:10:34 2023\n",
            "[xla:7](20) Loss=0.01855 Rate=138.24 GlobalRate=169.23 Time=Tue Mar 21 22:10:35 2023\n",
            "[xla:6](20) Loss=0.01825 Rate=141.92 GlobalRate=170.72 Time=Tue Mar 21 22:10:35 2023\n",
            "[xla:1](20) Loss=0.00893 Rate=151.99 GlobalRate=178.98 Time=Tue Mar 21 22:10:37 2023\n",
            "[xla:2](40) Loss=0.01231 Rate=208.71 GlobalRate=221.05 Time=Tue Mar 21 22:10:37 2023\n",
            "[xla:5](20) Loss=0.02507 Rate=168.25 GlobalRate=190.69 Time=Tue Mar 21 22:10:38 2023\n",
            "[xla:4](40) Loss=0.00263 Rate=239.72 GlobalRate=220.01 Time=Tue Mar 21 22:10:40 2023\n",
            "Finished training epoch 5\n",
            "[xla:3](40) Loss=0.03834 Rate=240.84 GlobalRate=217.09 Time=Tue Mar 21 22:10:41 2023\n",
            "[xla:7](40) Loss=0.02768 Rate=246.25 GlobalRate=219.33 Time=Tue Mar 21 22:10:43 2023\n",
            "[xla:6](40) Loss=0.01049 Rate=238.08 GlobalRate=216.71 Time=Tue Mar 21 22:10:44 2023\n",
            "[xla:1](40) Loss=0.00548 Rate=233.77 GlobalRate=219.59 Time=Tue Mar 21 22:10:45 2023\n",
            "[xla:5](40) Loss=0.00476 Rate=218.52 GlobalRate=216.38 Time=Tue Mar 21 22:10:48 2023\n",
            "[xla:0] Accuracy=98.11%\n",
            "[xla:0](0) Loss=0.03750 Rate=50.32 GlobalRate=50.32 Time=Tue Mar 21 22:11:21 2023\n",
            "[xla:2] Accuracy=97.13%\n",
            "[xla:2](0) Loss=0.02862 Rate=64.55 GlobalRate=64.55 Time=Tue Mar 21 22:11:24 2023\n",
            "[xla:4] Accuracy=97.88%\n",
            "[xla:3] Accuracy=97.32%\n",
            "[xla:4](0) Loss=0.00991 Rate=52.77 GlobalRate=52.77 Time=Tue Mar 21 22:11:27 2023\n",
            "[xla:3](0) Loss=0.01191 Rate=46.68 GlobalRate=46.68 Time=Tue Mar 21 22:11:27 2023\n",
            "[xla:6] Accuracy=97.40%\n",
            "[xla:7] Accuracy=97.64%\n",
            "[xla:0](20) Loss=0.01501 Rate=208.38 GlobalRate=251.15 Time=Tue Mar 21 22:11:29 2023\n",
            "[xla:1] Accuracy=98.16%\n",
            "[xla:6](0) Loss=0.05222 Rate=33.92 GlobalRate=33.92 Time=Tue Mar 21 22:11:32 2023\n",
            "[xla:5] Accuracy=97.40%\n",
            "[xla:7](0) Loss=0.01621 Rate=33.59 GlobalRate=33.59 Time=Tue Mar 21 22:11:32 2023\n",
            "[xla:1](0) Loss=0.00675 Rate=35.39 GlobalRate=35.39 Time=Tue Mar 21 22:11:33 2023\n",
            "[xla:2](20) Loss=0.01816 Rate=177.64 GlobalRate=222.15 Time=Tue Mar 21 22:11:34 2023\n",
            "[xla:5](0) Loss=0.00972 Rate=30.12 GlobalRate=30.12 Time=Tue Mar 21 22:11:36 2023\n",
            "[xla:4](20) Loss=0.00903 Rate=151.90 GlobalRate=189.71 Time=Tue Mar 21 22:11:38 2023\n",
            "[xla:3](20) Loss=0.08010 Rate=143.14 GlobalRate=178.21 Time=Tue Mar 21 22:11:40 2023\n",
            "[xla:0](40) Loss=0.02207 Rate=216.04 GlobalRate=235.56 Time=Tue Mar 21 22:11:41 2023\n",
            "[xla:6](20) Loss=0.01248 Rate=154.50 GlobalRate=183.21 Time=Tue Mar 21 22:11:43 2023\n",
            "[xla:1](20) Loss=0.01204 Rate=168.76 GlobalRate=198.35 Time=Tue Mar 21 22:11:43 2023\n",
            "[xla:7](20) Loss=0.00779 Rate=155.82 GlobalRate=184.14 Time=Tue Mar 21 22:11:43 2023[xla:2](40) Loss=0.00253 Rate=232.96 GlobalRate=243.11 Time=Tue Mar 21 22:11:43 2023\n",
            "\n",
            "[xla:5](20) Loss=0.03686 Rate=178.41 GlobalRate=199.36 Time=Tue Mar 21 22:11:45 2023\n",
            "Finished training epoch 6\n",
            "[xla:4](40) Loss=0.00277 Rate=244.96 GlobalRate=233.16 Time=Tue Mar 21 22:11:47 2023\n",
            "[xla:3](40) Loss=0.01157 Rate=240.15 GlobalRate=223.50 Time=Tue Mar 21 22:11:48 2023\n",
            "[xla:6](40) Loss=0.00877 Rate=237.21 GlobalRate=224.00 Time=Tue Mar 21 22:11:51 2023\n",
            "[xla:1](40) Loss=0.01079 Rate=232.21 GlobalRate=229.40 Time=Tue Mar 21 22:11:52 2023\n",
            "[xla:7](40) Loss=0.01102 Rate=224.75 GlobalRate=218.17 Time=Tue Mar 21 22:11:53 2023\n",
            "[xla:5](40) Loss=0.00438 Rate=208.51 GlobalRate=212.62 Time=Tue Mar 21 22:11:57 2023\n",
            "[xla:0] Accuracy=98.40%\n",
            "[xla:0](0) Loss=0.03031 Rate=56.65 GlobalRate=56.64 Time=Tue Mar 21 22:12:28 2023\n",
            "[xla:2] Accuracy=97.52%\n",
            "[xla:2](0) Loss=0.04642 Rate=54.60 GlobalRate=54.60 Time=Tue Mar 21 22:12:30 2023\n",
            "[xla:4] Accuracy=98.21%\n",
            "[xla:3] Accuracy=97.52%\n",
            "[xla:4](0) Loss=0.00290 Rate=60.69 GlobalRate=60.69 Time=Tue Mar 21 22:12:33 2023\n",
            "[xla:3](0) Loss=0.00214 Rate=61.75 GlobalRate=61.75 Time=Tue Mar 21 22:12:34 2023\n",
            "[xla:0](20) Loss=0.00943 Rate=232.81 GlobalRate=280.91 Time=Tue Mar 21 22:12:35 2023\n",
            "[xla:6] Accuracy=97.94%\n",
            "[xla:7] Accuracy=97.92%\n",
            "[xla:1] Accuracy=98.18%\n",
            "[xla:5] Accuracy=97.69%\n",
            "[xla:2](20) Loss=0.00921 Rate=201.24 GlobalRate=246.47 Time=Tue Mar 21 22:12:39 2023\n",
            "[xla:6](0) Loss=0.02555 Rate=36.71 GlobalRate=36.71 Time=Tue Mar 21 22:12:39 2023\n",
            "[xla:7](0) Loss=0.02138 Rate=35.49 GlobalRate=35.49 Time=Tue Mar 21 22:12:41 2023\n",
            "[xla:1](0) Loss=0.00708 Rate=35.91 GlobalRate=35.91 Time=Tue Mar 21 22:12:41 2023\n",
            "[xla:5](0) Loss=0.05943 Rate=33.76 GlobalRate=33.76 Time=Tue Mar 21 22:12:43 2023\n",
            "[xla:4](20) Loss=0.01013 Rate=166.21 GlobalRate=207.87 Time=Tue Mar 21 22:12:44 2023\n",
            "[xla:3](20) Loss=0.08120 Rate=150.95 GlobalRate=188.77 Time=Tue Mar 21 22:12:47 2023\n",
            "[xla:0](40) Loss=0.00481 Rate=220.16 GlobalRate=242.30 Time=Tue Mar 21 22:12:47 2023\n",
            "[xla:2](40) Loss=0.01732 Rate=224.76 GlobalRate=243.49 Time=Tue Mar 21 22:12:50 2023\n",
            "[xla:6](20) Loss=0.00557 Rate=155.49 GlobalRate=186.73 Time=Tue Mar 21 22:12:50 2023\n",
            "[xla:7](20) Loss=0.05074 Rate=163.38 GlobalRate=193.34 Time=Tue Mar 21 22:12:51 2023\n",
            "[xla:1](20) Loss=0.00102 Rate=159.52 GlobalRate=190.01 Time=Tue Mar 21 22:12:51 2023\n",
            "Finished training epoch 7\n",
            "[xla:5](20) Loss=0.00985 Rate=173.21 GlobalRate=200.46 Time=Tue Mar 21 22:12:53 2023\n",
            "[xla:4](40) Loss=0.00138 Rate=233.32 GlobalRate=237.06 Time=Tue Mar 21 22:12:53 2023\n",
            "[xla:3](40) Loss=0.00386 Rate=261.11 GlobalRate=239.73 Time=Tue Mar 21 22:12:54 2023\n",
            "[xla:6](40) Loss=0.00614 Rate=233.01 GlobalRate=224.39 Time=Tue Mar 21 22:12:59 2023\n",
            "[xla:7](40) Loss=0.00439 Rate=228.02 GlobalRate=224.80 Time=Tue Mar 21 22:13:00 2023\n",
            "[xla:1](40) Loss=0.00650 Rate=226.49 GlobalRate=222.49 Time=Tue Mar 21 22:13:01 2023\n",
            "[xla:5](40) Loss=0.00394 Rate=234.10 GlobalRate=230.89 Time=Tue Mar 21 22:13:02 2023\n",
            "[xla:0] Accuracy=98.35%\n",
            "[xla:0](0) Loss=0.01925 Rate=48.34 GlobalRate=48.34 Time=Tue Mar 21 22:13:33 2023\n",
            "[xla:2] Accuracy=97.84%\n",
            "[xla:2](0) Loss=0.01460 Rate=61.19 GlobalRate=61.18 Time=Tue Mar 21 22:13:35 2023\n",
            "[xla:4] Accuracy=98.11%\n",
            "[xla:3] Accuracy=98.04%\n",
            "[xla:4](0) Loss=0.00113 Rate=63.34 GlobalRate=63.34 Time=Tue Mar 21 22:13:39 2023\n",
            "[xla:3](0) Loss=0.00097 Rate=54.88 GlobalRate=54.87 Time=Tue Mar 21 22:13:40 2023\n",
            "[xla:0](20) Loss=0.00551 Rate=227.84 GlobalRate=268.41 Time=Tue Mar 21 22:13:40 2023\n",
            "[xla:6] Accuracy=97.91%\n",
            "[xla:7] Accuracy=97.92%\n",
            "[xla:2](20) Loss=0.00232 Rate=253.83 GlobalRate=305.84 Time=Tue Mar 21 22:13:42 2023\n",
            "[xla:1] Accuracy=98.24%\n",
            "[xla:6](0) Loss=0.01625 Rate=44.88 GlobalRate=44.88 Time=Tue Mar 21 22:13:44 2023\n",
            "[xla:5] Accuracy=97.65%\n",
            "[xla:7](0) Loss=0.01750 Rate=42.87 GlobalRate=42.87 Time=Tue Mar 21 22:13:45 2023\n",
            "[xla:1](0) Loss=0.00513 Rate=33.44 GlobalRate=33.44 Time=Tue Mar 21 22:13:46 2023\n",
            "[xla:5](0) Loss=0.01159 Rate=33.93 GlobalRate=33.92 Time=Tue Mar 21 22:13:48 2023\n",
            "[xla:4](20) Loss=0.00176 Rate=186.02 GlobalRate=232.12 Time=Tue Mar 21 22:13:49 2023\n",
            "[xla:3](20) Loss=0.02186 Rate=173.74 GlobalRate=215.87 Time=Tue Mar 21 22:13:50 2023\n",
            "[xla:0](40) Loss=0.00163 Rate=248.92 GlobalRate=265.73 Time=Tue Mar 21 22:13:50 2023\n",
            "[xla:2](40) Loss=0.00136 Rate=225.31 GlobalRate=247.56 Time=Tue Mar 21 22:13:54 2023\n",
            "[xla:6](20) Loss=0.00634 Rate=148.47 GlobalRate=183.85 Time=Tue Mar 21 22:13:56 2023\n",
            "[xla:7](20) Loss=0.01028 Rate=149.93 GlobalRate=184.69 Time=Tue Mar 21 22:13:57 2023\n",
            "[xla:1](20) Loss=0.00112 Rate=152.20 GlobalRate=180.50 Time=Tue Mar 21 22:13:57 2023\n",
            "Finished training epoch 8\n",
            "[xla:4](40) Loss=0.00975 Rate=230.90 GlobalRate=245.29 Time=Tue Mar 21 22:13:58 2023\n",
            "[xla:5](20) Loss=0.00381 Rate=165.96 GlobalRate=194.05 Time=Tue Mar 21 22:13:58 2023\n",
            "[xla:3](40) Loss=0.00848 Rate=228.36 GlobalRate=237.25 Time=Tue Mar 21 22:14:00 2023\n",
            "[xla:6](40) Loss=0.00527 Rate=272.24 GlobalRate=240.33 Time=Tue Mar 21 22:14:03 2023\n",
            "[xla:7](40) Loss=0.00300 Rate=261.24 GlobalRate=236.55 Time=Tue Mar 21 22:14:04 2023\n",
            "[xla:1](40) Loss=0.00230 Rate=257.75 GlobalRate=231.25 Time=Tue Mar 21 22:14:05 2023\n",
            "[xla:5](40) Loss=0.00231 Rate=254.27 GlobalRate=238.25 Time=Tue Mar 21 22:14:07 2023\n",
            "[xla:0] Accuracy=98.38%\n",
            "[xla:2] Accuracy=98.11%\n",
            "[xla:0](0) Loss=0.00121 Rate=40.39 GlobalRate=40.39 Time=Tue Mar 21 22:14:36 2023\n",
            "[xla:2](0) Loss=0.00297 Rate=37.52 GlobalRate=37.52 Time=Tue Mar 21 22:14:38 2023\n",
            "[xla:4] Accuracy=98.17%\n",
            "[xla:3] Accuracy=97.99%\n",
            "[xla:4](0) Loss=0.01881 Rate=59.95 GlobalRate=59.95 Time=Tue Mar 21 22:14:43 2023\n",
            "[xla:0](20) Loss=0.00377 Rate=209.04 GlobalRate=241.45 Time=Tue Mar 21 22:14:44 2023\n",
            "[xla:3](0) Loss=0.00238 Rate=54.86 GlobalRate=54.85 Time=Tue Mar 21 22:14:44 2023\n",
            "[xla:6] Accuracy=97.84%\n",
            "[xla:2](20) Loss=0.00170 Rate=229.96 GlobalRate=254.61 Time=Tue Mar 21 22:14:45 2023\n",
            "[xla:7] Accuracy=97.93%\n",
            "[xla:1] Accuracy=98.39%\n",
            "[xla:6](0) Loss=0.00295 Rate=56.03 GlobalRate=56.03 Time=Tue Mar 21 22:14:47 2023\n",
            "[xla:5] Accuracy=97.56%"
          ]
        }
      ],
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "  global FLAGS\n",
        "  FLAGS = flags\n",
        "  torch.set_default_tensor_type('torch.FloatTensor')\n",
        "  accuracy, data, pred, target = train_mnist()\n",
        "  if rank == 0:\n",
        "    # Retrieve tensors that are on TPU core 0 and plot.\n",
        "    plot_results(data.cpu(), pred.cpu(), target.cpu())\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
        "          start_method='fork')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MznTE72_mthI"
      },
      "source": [
        "## Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9VAwyUnI7Sb"
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "img = cv2.imread(RESULT_IMG_PATH, cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}